{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization notebook\n",
    "\n",
    "The goal of this notebook is to provide visualization of data using Kepler.gl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global variables\n",
    "\n",
    "# Setting up the Coordinate Reference Systems up front in the necessary format.\n",
    "crs_degree = {'init': 'epsg:4326'} # CGS_WGS_1984 (what the GPS uses)\n",
    "\n",
    "# --- Paths\n",
    "\n",
    "# Root path of Fremont Dropbox\n",
    "import os\n",
    "import sys\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from fremontdropbox import get_dropbox_location\n",
    "# Root path of the Dropbox business account\n",
    "dbx = get_dropbox_location()\n",
    "\n",
    "# Temporary! Location of the folder where the restructuring is currently happening\n",
    "data_path = dbx + '/Private Structured data collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf(path):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        path: path of the file to read as a geodataframe\n",
    "        \n",
    "    return:\n",
    "        a GeoDataFrame (with Geopandas) corresponding to the file path\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame.from_file(path)\n",
    "    gdf = gdf.to_crs('epsg:4326')\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aimsun map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aimsun_path_regular = dbx + '/Private Structured data collection/Aimsun/Inputs/'\n",
    "aimsun_path_complex = dbx + '/Private Structured data collection/Aimsun/Inputs_complex/'\n",
    "\n",
    "aimsun_path = aimsun_path_complex\n",
    "\n",
    "#detectors = to_gdf(aimsun_path +'detectors.shp')\n",
    "# meterings = to_gdf(aimsun_path +'meterings.shp')\n",
    "# centroids = to_gdf(aimsun_path +'centroids.shp')\n",
    "# centroid_connections = to_gdf(aimsun_path +'centroid_connections.shp')\n",
    "# nodes = to_gdf(aimsun_path +'nodes.shp')\n",
    "# polygons = to_gdf(aimsun_path +'polygons.shp')\n",
    "sections = to_gdf(aimsun_path +'sections.shp')\n",
    "# sectionsGeo = to_gdf(aimsun_path +'sectionsGeo.shp')\n",
    "# turnings = to_gdf(aimsun_path +'turnings.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fremont_map = KeplerGl(height=600)\n",
    "# fremont_map.add_data(data = detectors, name=\"Detectors\")\n",
    "# fremont_map.add_data(data = meterings, name=\"meterings\")\n",
    "fremont_map.add_data(data = nodes, name=\"nodes\")\n",
    "# fremont_map.add_data(data = polygons, name=\"polygons\")\n",
    "fremont_map.add_data(data = sections, name=\"sections\")\n",
    "# fremont_map.add_data(data = sectionsGeo, name=\"sectionsGeo\")\n",
    "# fremont_map.add_data(data = turnings, name=\"turnings\")\n",
    "# fremont_map.add_data(data = centroids, name=\"centroids\")\n",
    "# fremont_map.add_data(data = centroid_connections, name=\"centroid_connections\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sections['rd_type'].count())\n",
    "print(sections['rd_type'].unique())\n",
    "print(\"175: \" + str(sections[sections['rd_type'] == 175.0]['id'].count()))\n",
    "print(\"177: \" + str(sections[sections['rd_type'] == 177.0]['id'].count()))\n",
    "print(\"179: \" + str(sections[sections['rd_type'] == 179.0]['id'].count()))\n",
    "print(\"180: \" + str(sections[sections['rd_type'] == 180.0]['id'].count()))\n",
    "print(\"182: \" + str(sections[sections['rd_type'] == 182.0]['id'].count()))\n",
    "print(\"184: \" + str(sections[sections['rd_type'] == 184.0]['id'].count()))\n",
    "print(\"185: \" + str(sections[sections['rd_type'] == 185.0]['id'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes['nodetype'].unique())\n",
    "\n",
    "print(\"0: \" + str(nodes[nodes['nodetype'] == 0.0]['id'].count()))\n",
    "print(\"1: \" + str(nodes[nodes['nodetype'] == 1.0]['id'].count()))\n",
    "print(\"2: \" + str(nodes[nodes['nodetype'] == 2.0]['id'].count()))\n",
    "print(\"3: \" + str(nodes[nodes['nodetype'] == 3.0]['id'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic signals and stop signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf_csv(path):\n",
    "# https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-wkt-format\n",
    "    df = pd.read_csv(path)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, crs='epsg:4326', geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_infra_path = data_path + \"/Manual-made dataset (do not touch)/Network/Infrastructure/\"\n",
    "\n",
    "stop_signs = to_gdf_csv(network_infra_path + \"Stop signs location/Stop_Signs.csv\")\n",
    "traffic_lights = to_gdf_csv(network_infra_path + \"Traffic lights location/Traffic_Lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = stop_signs, name=\"Stop signs\")\n",
    "fremont_map.add_data(data = traffic_lights, name=\"Traffic lights\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_signs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "path_taz = data_path + \"/Data processing/Auxiliary files/Demand/OD demand/TAZ\"\n",
    "internal_taz = to_gdf(path_taz + \"/Internal_TAZ.shp\")\n",
    "external_taz = to_gdf(path_taz + \"/External_TAZ.shp\")\n",
    "\n",
    "# Get gravity centers for all TAZs (internal and external)\n",
    "centroid_gravity = {}\n",
    "for i in range(len(internal_taz['geometry'])):\n",
    "    centroid_gravity[internal_taz['CentroidID'][i]] = internal_taz['geometry'][i].centroid\n",
    "for i in range(len(external_taz['geometry'])):\n",
    "    centroid_gravity[external_taz['CentroidID'][i]] = external_taz['geometry'][i].centroid\n",
    "\n",
    "od_demand_path = data_path + \"/Data processing/Temporary exports to be copied to processed data/Demand/OD demand/\" \n",
    "internal_od = pd.read_csv(od_demand_path + \"Internal OD grouped by timestamp.csv\")\n",
    "external_od = pd.read_csv(od_demand_path + \"External OD grouped by timestamp.csv\")\n",
    "\n",
    "# Remove centroids with no demand\n",
    "external_od = external_od[external_od[\"counts\"] != 0]\n",
    "internal_od = internal_od[internal_od[\"counts\"] != 0]\n",
    "internal_od = internal_od[internal_od[\"CentroidID_O\"] != \"int_16\"]\n",
    "internal_od = internal_od[internal_od[\"CentroidID_D\"] != \"int_16\"]\n",
    "internal_od = internal_od[internal_od[\"CentroidID_O\"] != \"int_62\"]\n",
    "internal_od = internal_od[internal_od[\"CentroidID_D\"] != \"int_62\"]\n",
    "# Fix analysis timestep at 6 PM\n",
    "internal_od_6_pm = internal_od[internal_od[\"dt_15\"]==\"18:00\"]\n",
    "external_od_6_pm = external_od[external_od[\"dt_15\"]==\"18:0\"]\n",
    "\n",
    "internal_od_6_pm = internal_od_6_pm.reset_index()\n",
    "external_od_6_pm = external_od_6_pm.reset_index()\n",
    "\n",
    "external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(external_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = external_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = external_od_6_pm['CentroidID_D'][i]\n",
    "    demand = external_od_6_pm['counts'][i]\n",
    "    external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "internal_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "internal_external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(internal_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = internal_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = internal_od_6_pm['CentroidID_D'][i]\n",
    "    demand = internal_od_6_pm['counts'][i]\n",
    "    if \"ext\" not in origin_id and \"ext\" not in dest_id:\n",
    "        internal_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "    else:\n",
    "        internal_external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Internal):\", internal_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (External):\", external_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Total):\", internal_od_6_pm.counts.sum() + external_od_6_pm.counts.sum())\n",
    "\n",
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = external_demand, name=\"External/External Demand\")\n",
    "fremont_map.add_data(data = internal_demand, name=\"Internal/Internal Demand\")\n",
    "fremont_map.add_data(data = internal_external_demand, name=\"Internal/External Demand\")\n",
    "fremont_map.add_data(data = internal_taz, name=\"Internal TAZs\")\n",
    "fremont_map.add_data(data = external_taz, name=\"External TAZs\")\n",
    "fremont_map\n",
    "\n",
    "'''\n",
    "fremont_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/6pm_demand_map.html\")\n",
    "frm_config = fremont_map.config\n",
    "frm_map = KeplerGl(height=600, config=frm_config)\n",
    "frm_map.add_data(data = external_demand, name=\"External/External Demand\")\n",
    "frm_map.add_data(data = internal_demand, name=\"Internal/Internal Demand\")\n",
    "frm_map.add_data(data = internal_external_demand, name=\"Internal/External Demand\")\n",
    "frm_map.add_data(data = internal_taz, name=\"Internal TAZs\")\n",
    "frm_map.add_data(data = external_taz, name=\"External TAZs\")\n",
    "frm_map\n",
    "frm_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/6pm_demand_count_widths_map.html\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
