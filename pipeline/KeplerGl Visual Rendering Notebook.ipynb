{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeplerGl Visual Rendering\n",
    "\n",
    "The goal of this notebook is to visually render various generated/acquired datasets using Uber's open-source KeplerGL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- explain how the data has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Modules and Auxilary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell contains the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generates the important paths used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edson/Dropbox\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from fremontdropbox import get_dropbox_location\n",
    "dbx = get_dropbox_location()\n",
    "print(dbx)\n",
    "data_path = dbx + '/Private Structured data collection'\n",
    "project_del_path = data_path + \"/Manual-made dataset (do not touch)/Network/Map/Project Delimitation/Project_delimitation.shp\"\n",
    "map_path = data_path + \"/Data processing/Kepler maps/Network/Map/\"\n",
    "inputs_path = data_path + '//Aimsun/Inputs/'\n",
    "outputs_path = data_path + '/Aimsun/Outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts files into GeoDataframe objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf(path):\n",
    "    gdf = gpd.GeoDataFrame.from_file(path)\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    return gdf\n",
    "\n",
    "def to_gdf_from_shp(filename):\n",
    "    return to_gdf(inputs_path + filename)\n",
    "\n",
    "def to_gdf_from_csv(path):\n",
    "# https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-wkt-format\n",
    "    df = pd.read_csv(path)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, crs='epsg:4326', geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints diagnostic information for each table field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_write(csv_file, tablename, table, table_map, field, fieldname, first=False, unit='', key=None):\n",
    "    field_type = np.sort(field.unique())\n",
    "\n",
    "    if first:\n",
    "        csv_file.write(\"--------{}s-------\\n\".format(tablename))\n",
    "        csv_file.write(\"Total number of {}s: {}\\n\".format(tablename, str(table['id'].count())))\n",
    "        \n",
    "    csv_file.write(\"{} {}, number of sections\\n\".format(fieldname, unit))\n",
    "\n",
    "    if key:\n",
    "        csv_file.write(key + '\\n')\n",
    "\n",
    "    for i in field_type:\n",
    "        csv_file.write(\"{}, {}\\n\".format(str(i), str(table[table[fieldname] == i]['id'].count())))\n",
    "        table_map.add_data(data=table[table[fieldname] == i], name=\"{} {}\".format(fieldname, str(i)))\n",
    "\n",
    "    csv_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(config_file='visualization_config.txt'):\n",
    "    configs = open(config_file, 'r')\n",
    "    config_dict = ast.literal_eval(configs.read())\n",
    "    configs.close()\n",
    "    return config_dict\n",
    "    \n",
    "def write_configs(config_dict, key, config, config_file='visualization_config.txt'):\n",
    "    config_dict[key] = config\n",
    "    print(config_dict, file=open(config_file, 'w'))\n",
    "\n",
    "dict_config = get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_rows(df, col):\n",
    "    return df[col.isnull()]\n",
    "\n",
    "def get_not_null_rows(df, col):\n",
    "    return df[col.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell toggles printing debugging information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_kepler = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project delimitation\n",
    "\n",
    "Project delimitation has been created in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/delimitation.html!\n"
     ]
    }
   ],
   "source": [
    "project_del = to_gdf(project_del_path)\n",
    "\n",
    "if print_kepler:\n",
    "    del_map = KeplerGl(height=600)\n",
    "    del_map.add_data(data = project_del[project_del.Type == \"Delimitation\"], name=\"Project delimitation\")\n",
    "    del_map.add_data(data = project_del[project_del.Type == \"Box\"], name=\"Project box delimitation\")\n",
    "    del_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/delimitation.html\")\n",
    "    del_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network data\n",
    "\n",
    "## To do: \n",
    "1. Get the different screenshot of the section characteristics (Theo)\n",
    "2. Put the data in a CSV file (Theo)\n",
    "3. Understand what is the meaning of node types and section function class\n",
    "4. If possible render data about (Daniel M.):\n",
    "    - traffic signal plans\n",
    "    - master control plans\n",
    "    - changes in network between 2013, 2017 and 2019\n",
    "5. produce similar results with OSM directly (Ayush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Map\n",
    "#### Aimsun map\n",
    "\n",
    "Aimsun map has been created in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new demand\n",
    "aimsun_complete = False\n",
    "\n",
    "detectors = to_gdf_from_shp('detectors.shp')\n",
    "meterings = to_gdf_from_shp('meterings.shp')\n",
    "nodes = to_gdf_from_shp('nodes.shp')\n",
    "sections = to_gdf_from_shp('sections.shp')\n",
    "sectionsGeo = to_gdf_from_shp('sectionsGeo.shp')\n",
    "turnings = to_gdf_from_shp('turnings.shp')\n",
    "if aimsun_complete:\n",
    "    centroids = to_gdf_from_shp('centroids.shp')\n",
    "    centroid_connections = to_gdf_from_shp('centroid_connections.shp')\n",
    "#     polygons = to_gdf_from_shp('polygons.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell is useful to better understand and render the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Done) To do Edson: change print_diagnostic_information to get the information inside a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/nodes_to_check.html!\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/Sections/types.html!\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/Sections/speed.html!\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/Sections/capacity.html!\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/Sections/funct_type.html!\n"
     ]
    }
   ],
   "source": [
    "# fremont_map = KeplerGl(height=600)\n",
    "if print_kepler:\n",
    "    node_map = KeplerGl(height=600)\n",
    "    section_map = KeplerGl(height=600)\n",
    "    speed_map = KeplerGl(height=600)\n",
    "    capacity_map = KeplerGl(height=600)    \n",
    "    func_type_map = KeplerGl(height=600)\n",
    "    \n",
    "section_key = \"175 = Motorway, 177 = Primary, 179 = Residential, 180 = Secondary, 182.0 = Tertiary, 184.0 = Trunk, 185.0 = Unclassified\"\n",
    "speed_unit = \"(km/h)\"\n",
    "capacity_unit = \"(veh/h)\"\n",
    "\n",
    "# write diagnostic information to csv\n",
    "diagnostic_filepath = data_path + \"/Data processing/Kepler maps/Network/Map/diagnostic_info.csv\"\n",
    "diagnostic_csv = open(diagnostic_filepath, 'w')\n",
    "diagnostic_write(diagnostic_csv, \"Node\", nodes, node_map, nodes.nodetype, \"nodetype\", first = True)    \n",
    "\n",
    "diagnostic_write(diagnostic_csv, \"Section\", sections, section_map, sections.rd_type, \"rd_type\", first = True, key = section_key)\n",
    "diagnostic_write(diagnostic_csv, \"Section\", sections, speed_map, sections.speed, \"speed\", unit = speed_unit)\n",
    "diagnostic_write(diagnostic_csv, \"Section\", sections, capacity_map, sections.capacity, \"capacity\", unit = capacity_unit)\n",
    "diagnostic_write(diagnostic_csv, \"Section\", sections, func_type_map, sections.func_class, \"func_class\")\n",
    "diagnostic_csv.close()\n",
    "\n",
    "# fremont_map\n",
    "if print_kepler:\n",
    "    node_map.save_to_html(file_name = map_path + \"nodes_to_check.html\")\n",
    "    section_map.save_to_html(file_name = map_path + \"Sections/types.html\")\n",
    "    speed_map.save_to_html(file_name = map_path + \"Sections/speed.html\")\n",
    "    capacity_map.save_to_html(file_name = map_path + \"Sections/capacity.html\")\n",
    "    func_type_map.save_to_html(file_name = map_path + \"Sections/funct_type.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/aimsun_data.html!\n"
     ]
    }
   ],
   "source": [
    "if print_kepler:\n",
    "    aimsun_map = KeplerGl(height=600)\n",
    "    aimsun_map.add_data(data = detectors, name=\"Detectors\")\n",
    "    aimsun_map.add_data(data = meterings, name=\"meterings\")\n",
    "    aimsun_map.add_data(data = nodes, name=\"nodes\")\n",
    "    aimsun_map.add_data(data = sections, name=\"sections\")\n",
    "    aimsun_map.add_data(data = sectionsGeo, name=\"sectionsGeo\")\n",
    "    aimsun_map.add_data(data = turnings, name=\"turnings\")\n",
    "    if aimsun_complete:\n",
    "        aimsun_map.add_data(data = centroids, name=\"centroids\")\n",
    "        aimsun_map.add_data(data = centroid_connections, name=\"centroid_connections\")\n",
    "#         fremont_map.add_data(data = polygons, name=\"polygons\")\n",
    "    aimsun_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/aimsun_data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aimsun centroids and centroids connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler and aimsun_complete:\n",
    "    centroid_map = KeplerGl(height=600, config=dict_config['centroid_config'])\n",
    "    centroid_map.add_data(data = centroids[centroids['name'].str.contains('ext')], name=\"External centroids\")\n",
    "    centroid_map.add_data(data = centroids[centroids['name'].str.contains('int')], name=\"Internal centroids\")\n",
    "    centroid_map.add_data(data = centroid_connections, name=\"Centroid connections\")\n",
    "    centroid_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'from'], name=\"Outgoing centroid connections\")\n",
    "    centroid_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'to'], name=\"Incoming centroid connections\")\n",
    "    centroid_map.add_data(data = sections, name=\"sections\")\n",
    "    centroid_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Map/centroids.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aimsun road section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler:\n",
    "    road_section_map = KeplerGl(height=600, config=dict_config['road_type_config'])\n",
    "\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 175.0], name=\"Motorway\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 177.0], name=\"Primary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 179.0], name=\"Residential\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 180.0], name=\"Secondary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 182.0], name=\"Tertiary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 184.0], name=\"Trunk\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 185.0], name=\"Unclassified\")\n",
    "    road_section_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Map/sections.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# road_type_config = road_section_map.config\n",
    "# file = open(\"visualization_config.txt\", 'w')\n",
    "# file.write(\"{'road_type_config': \" + str(road_type_config))\n",
    "# file.write(\"}\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Infrastructure\n",
    "#### Traffic signals and stop signs\n",
    "\n",
    "Traffic signals and stop signs have been created in ...\n",
    "\n",
    "## To do: get infrastructure data from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_infra_path = data_path + \"/Manual-made dataset (do not touch)/Network/Infrastructure/\"\n",
    "\n",
    "stop_signs = to_gdf_from_csv(network_infra_path + \"Stop signs location/Stop_Signs.csv\")\n",
    "traffic_lights = to_gdf_from_csv(network_infra_path + \"Traffic lights location/Traffic_Lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler:\n",
    "    intersections = KeplerGl(height=600)\n",
    "    intersections.add_data(data = stop_signs, name=\"Stop signs\")\n",
    "    intersections.add_data(data = traffic_lights, name=\"Traffic lights\")\n",
    "    intersections.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Infrastructure/signs_and_lights.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Done) To do Edson: add the data in the following cell to the csv file for the Aimsun specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_csv = open(diagnostic_filepath, 'a') # append info to end of file\n",
    "diagnostic_csv.write(\"Number of stop signs: \" + str(stop_signs.__OBJECTID.count()) + \"\\n\")\n",
    "diagnostic_csv.write(\"Number of traffic lights: \" + str(traffic_lights.__OBJECTID.count()) + \"\\n\")\n",
    "diagnostic_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Done) |To do Edson: simplify if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kepler_map(dir_sections, dir_detectors, years):\n",
    "    \"\"\"\n",
    "    Creates kepler map to be visualized in a notebook\n",
    "    \n",
    "    Parameters\n",
    "    :param dir_sections: directory of sections.shp file \n",
    "    :param dir_detectors: directory of location_year_detector.shp files for all years\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating Kepler Map\")\n",
    "    sections_df = gpd.GeoDataFrame.from_file(dir_sections + \"sections.shp\")\n",
    "    sections_df = sections_df.to_crs(epsg=4326)\n",
    "    sections_df = sections_df[['eid', 'geometry']] # remove unnecessary data\n",
    "    sections_df = sections_df.rename(columns={'eid': 'Road_Id'})\n",
    "\n",
    "    kepler_map = KeplerGl(height=600)\n",
    "    kepler_map.add_data(data=sections_df, name='Road Sections')\n",
    "    for year in years:\n",
    "        shp_filename = \"pems_detectors.shp\" if year == 'PeMS' else (\"location_%s_detector.shp\" % year)\n",
    "\n",
    "        detector_df = gpd.GeoDataFrame.from_file(dir_detectors + shp_filename)\n",
    "        detector_df = detector_df.to_crs(epsg=4326)\n",
    "        id_name = 'Id' if 'Id' in detector_df.columns else 'ID'\n",
    "        detector_df = detector_df[[id_name, 'geometry']]\n",
    "        detector_df = detector_df.rename(columns={id_name: 'Detector_Id'})\n",
    "\n",
    "        kepler_map.add_data(data=detector_df, name='Detectors ' + year)\n",
    "\n",
    "    return kepler_map\n",
    "\n",
    "# import spatial_join as sj\n",
    "data_process_folder = dbx + \"/Private Structured data collection/Data processing/\"\n",
    "detectors_folder = data_process_folder + \"Raw/Demand/Flow_speed/detectors/\"\n",
    "sections_folder = data_process_folder + \"Raw/Network/Aimsun/\"\n",
    "\n",
    "# Visualize Fremont network with detectors\n",
    "detector_years = ['2013', '2015', '2017', '2019', 'PeMS']\n",
    "kepler_map = create_kepler_map(sections_folder, detectors_folder, detector_years)\n",
    "kepler_map.save_to_html(file_name=data_process_folder + \"Kepler maps/Network/Infrastructure/detectors.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw demand:\n",
    "- SFCTA demand data\n",
    "- Fremont neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SFCTA demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path):\n",
    "    int_int_trips = pd.read_csv(int_int_path)\n",
    "    int_ext_trips = pd.read_csv(int_ext_path)\n",
    "    ext_int_trips = pd.read_csv(ext_int_path)\n",
    "\n",
    "    internal_trips = pd.DataFrame.merge(int_int_trips, int_ext_trips, 'outer')\n",
    "    internal_trips = pd.DataFrame.merge(internal_trips, ext_int_trips, 'outer')\n",
    "    internal_trips['start_time'] = internal_trips['start_time'].apply(lambda x: pd.Timestamp(x))\n",
    "    return internal_trips\n",
    "\n",
    "def shift_time(demand_df, column):\n",
    "    demand_df[column] += pd.to_timedelta(-8, unit='h')\n",
    "    demand_df[column] = demand_df[column].apply(lambda x: x.time())\n",
    "    \n",
    "def get_demand_between_time(demand_df, begin_time, end_time):\n",
    "    return demand_df[(demand_df.start_time >= begin_time) & (demand_df.start_time <= end_time)]\n",
    "    \n",
    "def cluster_demand_15min(df):\n",
    "    demand_df = df.copy()\n",
    "    demand_df['start_time'] = demand_df['start_time'].apply(lambda x: str(x.replace(minute=int(x.minute/15)*15,second = 0)))\n",
    "    return demand_df.groupby(['start_time']).size().reset_index(name='counts')\n",
    "\n",
    "SFCTA_path = data_path+ '/Data processing/Raw/Demand/OD demand/SFCTA demand data/'\n",
    "int_int_path = SFCTA_path + \"internal_fremont_legs.csv\"\n",
    "int_ext_path = SFCTA_path + \"starting_fremont_legs.csv\"\n",
    "ext_int_path = SFCTA_path + \"ending_fremont_legs.csv\"\n",
    "\n",
    "print(\"Loading SFCTA trips...\")\n",
    "internal_trips = get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path)\n",
    "print(\"Shifting time...\")\n",
    "shift_time(internal_trips, 'start_time')\n",
    "print(\"Get afternoon demand\")\n",
    "begin_time = datetime.time(14, 0, 0)\n",
    "end_time = datetime.time(20, 0, 0)\n",
    "afternoon_demand = get_demand_between_time(internal_trips, begin_time, end_time)\n",
    "print(\"Clustering...\")\n",
    "clustered_internal_trips = cluster_demand_15min(internal_trips)\n",
    "clustered_afternoon_trips = cluster_demand_15min(afternoon_demand)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_demand_profile(df, every_nth, rot, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(df.start_time, df.counts)\n",
    "    for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "        if n % every_nth != 0:\n",
    "            label.set_visible(False)\n",
    "    plt.xticks(rotation=rot)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"time frame\")\n",
    "    plt.ylabel(\"trip count\")\n",
    "    plt.show()\n",
    "    \n",
    "print_demand_profile(clustered_internal_trips, 8, 60, \"SFCTA demand profile full day\")\n",
    "print_demand_profile(clustered_afternoon_trips, 4, 45, \"SFCTA demand profile afternoon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- do some check about the data using the processed data (especially demand profile and demand not assigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def fxy(start_lat, start_lng, end_lat, end_lng):\n",
    "    return LineString([(start_lng, start_lat), (end_lng, end_lat)])\n",
    "\n",
    "begin_time = datetime.time(16, 0, 0)\n",
    "end_time = datetime.time(16, 2, 0)\n",
    "\n",
    "if print_kepler:\n",
    "    afternoon_demand['geometry'] = afternoon_demand.apply(lambda x: fxy(x['start_node_lat'], x['start_node_lng'], x['end_node_lat'], x['end_node_lng']), axis=1)\n",
    "    afternoon_demand_gpd = gpd.GeoDataFrame(afternoon_demand, crs='epsg:4326', geometry = afternoon_demand.geometry)\n",
    "    demand_in_time = afternoon_demand_gpd[(afternoon_demand_gpd.start_time >= begin_time) & (afternoon_demand_gpd.start_time <= end_time)]\n",
    "\n",
    "    demand_map = KeplerGl(height=600)\n",
    "    demand_map.add_data(data = demand_in_time[['leg_id','geometry']], name=\"Demand\")\n",
    "    demand_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/SFCTA_1600_1602.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transportation Analysis Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External TAZs\n",
    "#### Neighborhoods\n",
    "#### Internal TAZs\n",
    "## To do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/theophile/Dropbox/Private Structured data collection/Data processing/Kepler maps/Demand/External_TAZs.html!\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "if print_kepler:\n",
    "    path_taz = data_path + \"/Manual-made dataset (do not touch)/Demand/OD demand/External TAZ/External Centroid zones/\"\n",
    "    external_taz = to_gdf(path_taz + \"/ExternalCentroidZones.shp\")\n",
    "    external_taz_map = KeplerGl(height=600)\n",
    "    external_taz_map.add_data(data = external_taz, name=\"External TAZs\")\n",
    "    external_taz_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/External_TAZs.html\")\n",
    "    \n",
    "## to do: neighborhood - internal TAZs (all togethers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do from Ayush code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 OD demand\n",
    "\n",
    "\n",
    "## (Done, Question below) To do Edson: \n",
    "- put this in a function and debug (the issue might be an issue of ESPG)\n",
    "- Does putting in a function mean a python file or just a function here in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Vehicles Analyzed at 6 PM (Internal): 3642\n",
      "Number of Vehicles Analyzed at 6 PM (External): 1263\n",
      "Number of Vehicles Analyzed at 6 PM (Total): 4905\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/edson/Dropbox/Private Structured data collection/Data processing/Kepler maps/Demand/6pm_demand_batches_map.html!\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def get_gdf(path):\n",
    "    gdf = gpd.GeoDataFrame.from_file(path)\n",
    "    gdf = gdf.set_geometry('geometry')\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    return gdf\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "path_taz = data_path + \"/Data processing/Auxiliary files/Demand/OD demand/TAZ\"\n",
    "internal_taz = get_gdf(path_taz + \"/Internal_TAZ.shp\")\n",
    "# external_taz = get_gdf(path_taz + \"/External_TAZ.shp\")\n",
    "external_centroids = get_gdf(path_taz + \"/External_centroids.shp\")\n",
    "\n",
    "# Get gravity centers for all TAZs (internal and external)\n",
    "centroid_gravity = {}\n",
    "for i in range(len(internal_taz['geometry'])):\n",
    "    centroid_gravity[internal_taz['CentroidID'][i]] = internal_taz['geometry'][i].centroid\n",
    "for i in range(len(external_centroids['geometry'])):\n",
    "    centroid_gravity[external_centroids['CentroidID'][i]] = external_centroids['geometry'][i]\n",
    "\n",
    "od_demand_path = data_path + \"/Data processing/Temporary exports to be copied to processed data/Demand/OD demand/\" \n",
    "internal_od = pd.read_csv(od_demand_path + \"Internal OD grouped by timestamp.csv\")\n",
    "external_od = pd.read_csv(od_demand_path + \"External OD grouped by timestamp.csv\")\n",
    "\n",
    "# Remove centroids with no demand\n",
    "external_od = external_od[external_od[\"counts\"] != 0]\n",
    "internal_od = internal_od[internal_od[\"counts\"] != 0]\n",
    "\n",
    "# Fix analysis timestep at 6 PM\n",
    "internal_od_6_pm = internal_od[internal_od[\"dt_15\"]==\"18:00:00\"]\n",
    "external_od_6_pm = external_od[external_od[\"dt_15\"]==\"18:0\"]\n",
    "\n",
    "internal_od_6_pm = internal_od_6_pm.reset_index()\n",
    "external_od_6_pm = external_od_6_pm.reset_index()\n",
    "\n",
    "external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(external_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = external_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = external_od_6_pm['CentroidID_D'][i]\n",
    "    demand = external_od_6_pm['counts'][i]\n",
    "    external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "internal_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "int_ext_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(internal_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = internal_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = internal_od_6_pm['CentroidID_D'][i]\n",
    "    demand = internal_od_6_pm['counts'][i]\n",
    "    if origin_id in centroid_gravity and dest_id in centroid_gravity:\n",
    "        if \"ext\" not in origin_id and \"ext\" not in dest_id:\n",
    "            internal_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "        else:\n",
    "                int_ext_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Internal):\", internal_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (External):\", external_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Total):\", internal_od_6_pm.counts.sum() + external_od_6_pm.counts.sum())\n",
    "\n",
    "if print_kepler:\n",
    "    demand_6_pm_map = KeplerGl(height=600, config=dict_config[\"OD Demand\"])\n",
    "    demand_6_pm_map.add_data(data = external_centroids, name=\"External Centroids\")\n",
    "\n",
    "    int_batches = {\"Internal Demand\":[(0, 2), (2, 3), (3, 4), (4, 5), (5, float('inf'))]}\n",
    "    int_ext_batches = {\"Internal/External Demand\":[(0, 2), (5, 10), (10, 15), (15, float('inf'))]}\n",
    "    ext_batches = {\"External Demand\":[(0, 10), (10, 50), (50, 250), (250, float('inf'))]}\n",
    "    dem_map = {\"Internal Demand\":internal_demand, \"Internal/External Demand\":int_ext_demand, \"External Demand\":external_demand}\n",
    "\n",
    "    for batch_list in [int_batches, int_ext_batches, ext_batches]:\n",
    "        for b_name, batches in batch_list.items():\n",
    "            for batch in batches:\n",
    "                demand_6_pm_map.add_data(data = dem_map[b_name][(dem_map[b_name]['counts'] >= batch[0])\\\n",
    "                                            & (dem_map[b_name]['counts'] < batch[1])],\\\n",
    "                                     name=\"{} in [{},{}) veh/15 min\".format(b_name, batch[0], batch[1]))\n",
    "\n",
    "    demand_6_pm_map.add_data(data = internal_taz, name=\"Internal TAZs\")\n",
    "\n",
    "    demand_6_pm_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/6pm_demand_batches_map.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing KeplerGl config for 7_6pm_demand_batches_map.html\n",
    "'''\n",
    "write_configs(dict_config, \"OD Demand\", fremont_map.config)\n",
    "dict_config = get_configs()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Flow data\n",
    "\n",
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Done) To do Edson:\n",
    "- for all detectors in 2019 save the flow profile for the entire day and for the afternoon (1pm-8pm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_detector_plot(city_detector_id, city_detector_flow, output_folder):\n",
    "    \"\"\"\n",
    "    Plot the demand profile specified city detector ID (integer with format 2019XX)\n",
    "    \"\"\"\n",
    "    curr_detector_flow = city_detector_flow[city_detector_flow['Detector_Id'] == city_detector_id]\n",
    "    curr_detector_flow = curr_detector_flow.transpose()[5:]\n",
    "    curr_detector_flow['dt_15'] = curr_detector_flow.index\n",
    "    curr_detector_flow.index = range(len(curr_detector_flow))\n",
    "    curr_detector_flow.columns = ['count', 'dt_15']\n",
    "    cols = curr_detector_flow.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    curr_detector_flow = curr_detector_flow[cols]\n",
    "    curr_detector_flow = curr_detector_flow.rename(columns={'count': 'Flow (Veh/15min)'})\n",
    "    save_trip_profile(curr_detector_flow, city_detector_id, output_folder)\n",
    "    save_trip_profile(curr_detector_flow, city_detector_id, output_folder, is_afternoon=True)\n",
    "\n",
    "def save_trip_profile(curr_detector_flow, detector_id, output_folder, is_afternoon=False):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if is_afternoon:\n",
    "        one_pm = 13 * 4  # 1pm\n",
    "        eight_pm = 20 * 4  # 8pm\n",
    "        ax.plot(curr_detector_flow.dt_15[one_pm: eight_pm], curr_detector_flow['Flow (Veh/15min)'][one_pm: eight_pm])\n",
    "    else:\n",
    "        ax.plot(curr_detector_flow.dt_15, curr_detector_flow['Flow (Veh/15min)'])\n",
    "\n",
    "    every_nth = 8\n",
    "    for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "        if n % every_nth != 0:\n",
    "            label.set_visible(False)\n",
    "\n",
    "    trip_type = \"trip profile\"\n",
    "    if is_afternoon:\n",
    "        trip_type = \"afternoon \" + trip_type\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title(\"detector \" + str(detector_id) + \" \" + trip_type)\n",
    "    plt.xlabel(\"time frame\")\n",
    "    plt.ylabel(\"trip count\")\n",
    "    plt.savefig(output_folder + \"detector_\" + str(detector_id) + \"_\" + trip_type.replace(\" \", \"_\"))\n",
    "    plt.close(fig)\n",
    "    \n",
    "# city detector flow info\n",
    "city_detector_flow = pd.read_csv(data_path + '/Data processing/Auxiliary files/Demand/Flow_speed/flow_processed_2019.csv')\n",
    "output_folder = data_path + \"/Data processing/Kepler maps/Demand/Flow_speed/Trip_profiles/\"\n",
    "for detector_id in city_detector_flow.Detector_Id:\n",
    "    city_detector_plot(detector_id, city_detector_flow, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Speed data\n",
    "\n",
    "## To do\n",
    "\n",
    "### 4.3 Trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehSectTraj = pd.read_csv(outputs_path + '/vehSectTrajectory.csv')\n",
    "\n",
    "vehSectTraj_temp = vehSectTraj.groupby(\"sectionId\").mean()\n",
    "section_sim_data = pd.merge(sections, vehSectTraj_temp,\n",
    "                        left_on='id', right_on='sectionId', how='left', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a94d0ae92f14c08b8a3af80991e276c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Sections without traffic': {'index': [3, 4, 20, 21, 22, 23, 24, 25, 33, 35, 36, 37, 38, 39, 40…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectory_map = KeplerGl(height=800)\n",
    "trajectory_map.add_data(data=get_null_rows(section_sim_data, section_sim_data.travelTime), name = \"Sections without traffic\")\n",
    "trajectory_map.add_data(data=get_not_null_rows(section_sim_data, section_sim_data.travelTime), name = \"Sections with traffic\")\n",
    "section_sim_data.crs = 'epsg:4326'\n",
    "trajectory_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Travel time data\n",
    "\n",
    "## To do\n",
    "from Travel time pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation data\n",
    "\n",
    "## To do: add the biplots here\n",
    "from Aimsun analyser pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key performance indicators\n",
    "\n",
    "## To do:\n",
    "- travel time distribution\n",
    "- isochrone\n",
    "\n",
    "from KPI pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
