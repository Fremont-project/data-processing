{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KeplerGl Visual Rendering\n",
    "\n",
    "The goal of this notebook is to visually render various generated/acquired datasets using Uber's open-source KeplerGL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- explain how the data has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Modules and Auxilary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell contains the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "from fremontdropbox import get_dropbox_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generates the important paths used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "dbx = get_dropbox_location()\n",
    "data_path = dbx + '/Private Structured data collection'\n",
    "project_del_path = dbx + \"/Private Structured data collection/Manual-made dataset (do not touch)/Network/Map/Project Delimitation/Project_delimitation.shp\"\n",
    "aimsun_path = dbx + '/Private Structured data collection/Aimsun/Inputs/'\n",
    "map_path = data_path + \"/Data processing/Kepler maps/Network/Map/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converts files into GeoDataframe objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf(path):\n",
    "    gdf = gpd.GeoDataFrame.from_file(path)\n",
    "    gdf.crs = 'epsg:4326'\n",
    "    return gdf\n",
    "\n",
    "def to_gdf_from_shp(filename):\n",
    "    return to_gdf(aimsun_path + filename)\n",
    "\n",
    "def to_gdf_from_csv(path):\n",
    "# https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-wkt-format\n",
    "    df = pd.read_csv(path)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, crs='epsg:4326', geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prints diagnostic information for each table field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diagnostic_information(tablename, table, table_map, field, fieldname, first = False, key = None):\n",
    "    field_type = np.sort(field.unique())\n",
    "    \n",
    "    if first:\n",
    "        print(\"--------{}s-------\".format(tablename))\n",
    "        print(\"Total number of {}s: {}\".format(tablename, str(table['id'].count())))\n",
    "    \n",
    "    print(\"{} values: {}\".format(fieldname, str(field_type)))\n",
    "    if key:\n",
    "        print(key)\n",
    "\n",
    "    for i in field_type:\n",
    "        print(\"Number of {}s with {} {}: {}\".format(tablename, fieldname, str(i), str(table[table[fieldname] == i]['id'].count())))\n",
    "        if print_kepler:\n",
    "            table_map.add_data(data = table[table[fieldname] == i], name= \"{} {}\".format(fieldname, str(i)))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs(config_file='visualization_config.txt'):\n",
    "    configs = open(config_file, 'r')\n",
    "    config_dict = ast.literal_eval(configs.read())\n",
    "    configs.close()\n",
    "    return config_dict\n",
    "    \n",
    "def write_configs(config_dict, key, config, config_file='visualization_config.txt'):\n",
    "    config_dict[key] = config\n",
    "    print(config_dict, file=open(config_file, 'w'))\n",
    "\n",
    "dict_config = get_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell toggles printing debugging information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_kepler = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project delimitation\n",
    "\n",
    "Project delimitation has been created in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to /Users/daniel/Dropbox/Private Structured data collection/Data processing/Kepler maps/delimitation.html!\n"
     ]
    }
   ],
   "source": [
    "project_del = to_gdf(project_del_path)\n",
    "\n",
    "if print_kepler:\n",
    "    del_map = KeplerGl(height=600)\n",
    "    del_map.add_data(data = project_del[project_del.Type == \"Delimitation\"], name=\"Project delimitation\")\n",
    "    del_map.add_data(data = project_del[project_del.Type == \"Box\"], name=\"Project box delimitation\")\n",
    "    del_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/delimitation.html\")\n",
    "    del_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network data\n",
    "\n",
    "## To do: \n",
    "1. Get the different screenshot of the section characteristics (Theo)\n",
    "2. Put the data in a CSV file (Theo)\n",
    "3. Understand what is the meaning of node types and section function class\n",
    "4. If possible render data about (Daniel M.):\n",
    "    - traffic signal plans\n",
    "    - master control plans\n",
    "    - changes in network between 2013, 2017 and 2019\n",
    "5. produce similar results with OSM directly (Ayush)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Map\n",
    "#### Aimsun map\n",
    "\n",
    "Aimsun map has been created in ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new demand\n",
    "aimsun_complete = False\n",
    "\n",
    "detectors = to_gdf_from_shp('detectors.shp')\n",
    "meterings = to_gdf_from_shp('meterings.shp')\n",
    "nodes = to_gdf_from_shp('nodes.shp')\n",
    "sections = to_gdf_from_shp('sections.shp')\n",
    "sectionsGeo = to_gdf_from_shp('sectionsGeo.shp')\n",
    "turnings = to_gdf_from_shp('turnings.shp')\n",
    "if aimsun_complete:\n",
    "    centroids = to_gdf_from_shp('centroids.shp')\n",
    "    centroid_connections = to_gdf_from_shp('centroid_connections.shp')\n",
    "#     polygons = to_gdf_from_shp('polygons.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell is useful to better understand and render the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "--------Nodes-------\n",
      "Total number of Nodes: 2013\n",
      "nodetype values: [0. 1. 2. 3.]\n",
      "Number of Nodes with nodetype 0.0: 585\n",
      "Number of Nodes with nodetype 1.0: 1259\n",
      "Number of Nodes with nodetype 2.0: 35\n",
      "Number of Nodes with nodetype 3.0: 134\n",
      "\n",
      "--------Sections-------\n",
      "Total number of Sections: 5626\n",
      "rd_type values: [175. 177. 179. 180. 182. 184. 185.]\n",
      "175 = Motorway, 177 = Primary, 179 = Residential, 180 = Secondary, 182.0 = Tertiary, 184.0 = Trunk, 185.0 = Unclassified\n",
      "Number of Sections with rd_type 175.0: 111\n",
      "Number of Sections with rd_type 177.0: 373\n",
      "Number of Sections with rd_type 179.0: 2916\n",
      "Number of Sections with rd_type 180.0: 393\n",
      "Number of Sections with rd_type 182.0: 189\n",
      "Number of Sections with rd_type 184.0: 51\n",
      "Number of Sections with rd_type 185.0: 1593\n",
      "\n",
      "speed values: [ 16.  40.  50.  56.  64.  72.  88.  90. 104. 120.]\n",
      "(Values are in km/h.)\n",
      "Number of Sections with speed 16.0: 4\n",
      "Number of Sections with speed 40.0: 44\n",
      "Number of Sections with speed 50.0: 5116\n",
      "Number of Sections with speed 56.0: 176\n",
      "Number of Sections with speed 64.0: 70\n",
      "Number of Sections with speed 72.0: 102\n",
      "Number of Sections with speed 88.0: 2\n",
      "Number of Sections with speed 90.0: 12\n",
      "Number of Sections with speed 104.0: 36\n",
      "Number of Sections with speed 120.0: 64\n",
      "\n",
      "capacity values: [  500.   700.   800.   900.  1000.  1400.  1600.  1800.  2100.  2400.\n",
      "  2700.  3200.  3500.  3600.  4000.  4200.  4500.  4800.  5400.  6000.\n",
      "  6300.  7200.  8400. 10500. 12600.]\n",
      "(Capacity is in veh/h.)\n",
      "Number of Sections with capacity 500.0: 1591\n",
      "Number of Sections with capacity 700.0: 2995\n",
      "Number of Sections with capacity 800.0: 146\n",
      "Number of Sections with capacity 900.0: 138\n",
      "Number of Sections with capacity 1000.0: 2\n",
      "Number of Sections with capacity 1400.0: 91\n",
      "Number of Sections with capacity 1600.0: 216\n",
      "Number of Sections with capacity 1800.0: 183\n",
      "Number of Sections with capacity 2100.0: 58\n",
      "Number of Sections with capacity 2400.0: 44\n",
      "Number of Sections with capacity 2700.0: 44\n",
      "Number of Sections with capacity 3200.0: 3\n",
      "Number of Sections with capacity 3500.0: 1\n",
      "Number of Sections with capacity 3600.0: 31\n",
      "Number of Sections with capacity 4000.0: 2\n",
      "Number of Sections with capacity 4200.0: 22\n",
      "Number of Sections with capacity 4500.0: 3\n",
      "Number of Sections with capacity 4800.0: 2\n",
      "Number of Sections with capacity 5400.0: 1\n",
      "Number of Sections with capacity 6000.0: 3\n",
      "Number of Sections with capacity 6300.0: 24\n",
      "Number of Sections with capacity 7200.0: 1\n",
      "Number of Sections with capacity 8400.0: 14\n",
      "Number of Sections with capacity 10500.0: 9\n",
      "Number of Sections with capacity 12600.0: 2\n",
      "\n",
      "func_class values: [1 2 3 5]\n",
      "Number of Sections with func_class 1: 111\n",
      "Number of Sections with func_class 2: 51\n",
      "Number of Sections with func_class 3: 955\n",
      "Number of Sections with func_class 5: 4509\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/daniel/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/nodes_to_check.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-203446d028f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# fremont_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprint_kepler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mnode_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"nodes_to_check.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0msection_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Sections/types.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mspeed_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Sections/speed.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keplergl/keplergl.py\u001b[0m in \u001b[0;36msave_to_html\u001b[0;34m(self, data, config, file_name, read_only)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mframe_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeplergl_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<body><script>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"</script>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeplergl_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/daniel/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/Map/nodes_to_check.html'"
     ]
    }
   ],
   "source": [
    "# fremont_map = KeplerGl(height=600)\n",
    "if print_kepler:\n",
    "    node_map = KeplerGl(height=600)\n",
    "    section_map = KeplerGl(height=600)\n",
    "    speed_map = KeplerGl(height=600)\n",
    "    capacity_map = KeplerGl(height=600)    \n",
    "    func_type_map = KeplerGl(height=600)\n",
    "    \n",
    "section_key = \"175 = Motorway, 177 = Primary, 179 = Residential, 180 = Secondary, 182.0 = Tertiary, 184.0 = Trunk, 185.0 = Unclassified\"\n",
    "speed_key = \"(Values are in km/h.)\"\n",
    "capacity_key = \"(Capacity is in veh/h.)\"\n",
    "\n",
    "print_diagnostic_information(\"Node\", nodes, node_map, nodes.nodetype, \"nodetype\", first = True)    \n",
    "    \n",
    "print_diagnostic_information(\"Section\", sections, section_map, sections.rd_type, \"rd_type\", first = True, key = section_key)\n",
    "print_diagnostic_information(\"Section\", sections, speed_map, sections.speed, \"speed\", key = speed_key)\n",
    "print_diagnostic_information(\"Section\", sections, capacity_map, sections.capacity, \"capacity\", key = capacity_key)\n",
    "print_diagnostic_information(\"Section\", sections, func_type_map, sections.func_class, \"func_class\")\n",
    "\n",
    "# fremont_map\n",
    "if print_kepler:\n",
    "    node_map.save_to_html(file_name = map_path + \"nodes_to_check.html\")\n",
    "    section_map.save_to_html(file_name = map_path + \"Sections/types.html\")\n",
    "    speed_map.save_to_html(file_name = map_path + \"Sections/speed.html\")\n",
    "    capacity_map.save_to_html(file_name = map_path + \"Sections/capacity.html\")\n",
    "    func_type_map.save_to_html(file_name = map_path + \"Sections/funct_type.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/daniel/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/aimsun_data.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-8c3fe31493d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maimsun_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroid_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"centroid_connections\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         fremont_map.add_data(data = polygons, name=\"polygons\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maimsun_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/Data processing/Kepler maps/Network/aimsun_data.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keplergl/keplergl.py\u001b[0m in \u001b[0;36msave_to_html\u001b[0;34m(self, data, config, file_name, read_only)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mframe_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeplergl_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<body><script>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"</script>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeplergl_html\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/daniel/Dropbox/Private Structured data collection/Data processing/Kepler maps/Network/aimsun_data.html'"
     ]
    }
   ],
   "source": [
    "if print_kepler:\n",
    "    aimsun_map = KeplerGl(height=600)\n",
    "    aimsun_map.add_data(data = detectors, name=\"Detectors\")\n",
    "    aimsun_map.add_data(data = meterings, name=\"meterings\")\n",
    "    aimsun_map.add_data(data = nodes, name=\"nodes\")\n",
    "    aimsun_map.add_data(data = sections, name=\"sections\")\n",
    "    aimsun_map.add_data(data = sectionsGeo, name=\"sectionsGeo\")\n",
    "    aimsun_map.add_data(data = turnings, name=\"turnings\")\n",
    "    if aimsun_complete:\n",
    "        aimsun_map.add_data(data = centroids, name=\"centroids\")\n",
    "        aimsun_map.add_data(data = centroid_connections, name=\"centroid_connections\")\n",
    "#         fremont_map.add_data(data = polygons, name=\"polygons\")\n",
    "    aimsun_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/aimsun_data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aimsun centroids and centroids connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler and aimsun_complete:\n",
    "    centroid_map = KeplerGl(height=600, config=dict_config['centroid_config'])\n",
    "    centroid_map.add_data(data = centroids[centroids['name'].str.contains('ext')], name=\"External centroids\")\n",
    "    centroid_map.add_data(data = centroids[centroids['name'].str.contains('int')], name=\"Internal centroids\")\n",
    "    centroid_map.add_data(data = centroid_connections, name=\"Centroid connections\")\n",
    "    centroid_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'from'], name=\"Outgoing centroid connections\")\n",
    "    centroid_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'to'], name=\"Incoming centroid connections\")\n",
    "    centroid_map.add_data(data = sections, name=\"sections\")\n",
    "    centroid_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Map/centroids.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aimsun road section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler:\n",
    "    road_section_map = KeplerGl(height=600, config=dict_config['road_type_config'])\n",
    "\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 175.0], name=\"Motorway\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 177.0], name=\"Primary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 179.0], name=\"Residential\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 180.0], name=\"Secondary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 182.0], name=\"Tertiary\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 184.0], name=\"Trunk\")\n",
    "    road_section_map.add_data(data = sections[sections['rd_type'] == 185.0], name=\"Unclassified\")\n",
    "    road_section_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Map/sections.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# road_type_config = road_section_map.config\n",
    "# file = open(\"visualization_config.txt\", 'w')\n",
    "# file.write(\"{'road_type_config': \" + str(road_type_config))\n",
    "# file.write(\"}\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Infrastructure\n",
    "#### Traffic signals and stop signs\n",
    "\n",
    "Traffic signals and stop signs have been created in ...\n",
    "\n",
    "## To do: get infrastructure data from OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_infra_path = data_path + \"/Manual-made dataset (do not touch)/Network/Infrastructure/\"\n",
    "\n",
    "stop_signs = to_gdf_csv(network_infra_path + \"Stop signs location/Stop_Signs.csv\")\n",
    "traffic_lights = to_gdf_csv(network_infra_path + \"Traffic lights location/Traffic_Lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_kepler:\n",
    "    intersections = KeplerGl(height=600)\n",
    "    intersections.add_data(data = stop_signs, name=\"Stop signs\")\n",
    "    intersections.add_data(data = traffic_lights, name=\"Traffic lights\")\n",
    "    intersections.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Network/Infrastructure/signs_and_lights.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: add the data in the following cell to the csv file for the Aimsun specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of stop signs: \" + str(stop_signs.__OBJECTID.count()))\n",
    "print(\"Number of traffic lights: \" + str(traffic_lights.__OBJECTID.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw demand:\n",
    "- SFCTA demand data\n",
    "- Fremont neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SFCTA demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path):\n",
    "    int_int_trips = pd.read_csv(int_int_path)\n",
    "    int_ext_trips = pd.read_csv(int_ext_path)\n",
    "    ext_int_trips = pd.read_csv(ext_int_path)\n",
    "\n",
    "    internal_trips = pd.DataFrame.merge(int_int_trips, int_ext_trips, 'outer')\n",
    "    internal_trips = pd.DataFrame.merge(internal_trips, ext_int_trips, 'outer')\n",
    "    internal_trips['start_time'] = internal_trips['start_time'].apply(lambda x: pd.Timestamp(x))\n",
    "    return internal_trips\n",
    "\n",
    "def shift_time(demand_df, column):\n",
    "    demand_df[column] += pd.to_timedelta(-8, unit='h')\n",
    "    demand_df[column] = demand_df[column].apply(lambda x: x.time())\n",
    "    \n",
    "def get_demand_between_time(demand_df, begin_time, end_time):\n",
    "    return demand_df[(demand_df.start_time >= begin_time) & (demand_df.start_time <= end_time)]\n",
    "    \n",
    "def cluster_demand_15min(df):\n",
    "    demand_df = df.copy()\n",
    "    demand_df['start_time'] = demand_df['start_time'].apply(lambda x: str(x.replace(minute=int(x.minute/15)*15,second = 0)))\n",
    "    return demand_df.groupby(['start_time']).size().reset_index(name='counts')\n",
    "\n",
    "SFCTA_path = data_path+ '/Data processing/Raw/Demand/OD demand/SFCTA demand data/'\n",
    "int_int_path = SFCTA_path + \"internal_fremont_legs.csv\"\n",
    "int_ext_path = SFCTA_path + \"starting_fremont_legs.csv\"\n",
    "ext_int_path = SFCTA_path + \"ending_fremont_legs.csv\"\n",
    "\n",
    "print(\"Loading SFCTA trips...\")\n",
    "internal_trips = get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path)\n",
    "print(\"Shifting time...\")\n",
    "shift_time(internal_trips, 'start_time')\n",
    "print(\"Get afternoon demand\")\n",
    "begin_time = datetime.time(14, 0, 0)\n",
    "end_time = datetime.time(20, 0, 0)\n",
    "afternoon_demand = get_demand_between_time(internal_trips, begin_time, end_time)\n",
    "print(\"Clustering...\")\n",
    "clustered_internal_trips = cluster_demand_15min(internal_trips)\n",
    "clustered_afternoon_trips = cluster_demand_15min(afternoon_demand)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_demand_profile(df, every_nth, rot, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(df.start_time, df.counts)\n",
    "    for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "        if n % every_nth != 0:\n",
    "            label.set_visible(False)\n",
    "    plt.xticks(rotation=rot)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"time frame\")\n",
    "    plt.ylabel(\"trip count\")\n",
    "    plt.show()\n",
    "    \n",
    "print_demand_profile(clustered_internal_trips, 8, 60, \"SFCTA demand profile full day\")\n",
    "print_demand_profile(clustered_afternoon_trips, 4, 45, \"SFCTA demand profile afternoon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "- do some check about the data using the processed data (especially demand profile and demand not assigned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def fxy(start_lat, start_lng, end_lat, end_lng):\n",
    "    return LineString([(start_lng, start_lat), (end_lng, end_lat)])\n",
    "\n",
    "begin_time = datetime.time(16, 0, 0)\n",
    "end_time = datetime.time(16, 2, 0)\n",
    "\n",
    "if print_kepler:\n",
    "    afternoon_demand['geometry'] = afternoon_demand.apply(lambda x: fxy(x['start_node_lat'], x['start_node_lng'], x['end_node_lat'], x['end_node_lng']), axis=1)\n",
    "    afternoon_demand_gpd = gpd.GeoDataFrame(afternoon_demand, crs='epsg:4326', geometry = afternoon_demand.geometry)\n",
    "    demand_in_time = afternoon_demand_gpd[(afternoon_demand_gpd.start_time >= begin_time) & (afternoon_demand_gpd.start_time <= end_time)]\n",
    "\n",
    "    demand_map = KeplerGl(height=600)\n",
    "    demand_map.add_data(data = demand_in_time[['leg_id','geometry']], name=\"Demand\")\n",
    "    demand_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/SFCTA_1600_1602.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transportation Analysis Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External TAZs\n",
    "#### Neighborhoods\n",
    "#### Internal TAZs\n",
    "## To do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "if print_kepler:\n",
    "    path_taz = data_path + \"/Manual-made dataset (do not touch)/Demand/OD demand/External TAZ/External Centroid zones/\"\n",
    "    external_taz = to_gdf(path_taz + \"/ExternalCentroidZones.shp\")\n",
    "    external_taz_map = KeplerGl(height=600)\n",
    "    external_taz_map.add_data(data = external_taz, name=\"External TAZs\")\n",
    "    external_taz_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/External_TAZs.html\")\n",
    "    \n",
    "## to do: neighborhood - internal TAZs (all togethers)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroid connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 OD demand\n",
    "\n",
    "\n",
    "## To do: put this in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "path_taz = data_path + \"/Data processing/Auxiliary files/Demand/OD demand/TAZ\"\n",
    "internal_taz = to_gdf(path_taz + \"/Internal_TAZ.shp\")\n",
    "# external_taz = to_gdf(path_taz + \"/External_TAZ.shp\")\n",
    "external_centroids = to_gdf(path_taz + \"/External_centroids.shp\")\n",
    "\n",
    "# Get gravity centers for all TAZs (internal and external)\n",
    "centroid_gravity = {}\n",
    "for i in range(len(internal_taz['geometry'])):\n",
    "    centroid_gravity[internal_taz['CentroidID'][i]] = internal_taz['geometry'][i].centroid\n",
    "for i in range(len(external_centroids['geometry'])):\n",
    "    centroid_gravity[external_centroids['CentroidID'][i]] = external_centroids['geometry'][i]\n",
    "\n",
    "od_demand_path = data_path + \"/Data processing/Temporary exports to be copied to processed data/Demand/OD demand/\" \n",
    "internal_od = pd.read_csv(od_demand_path + \"Internal OD grouped by timestamp.csv\")\n",
    "external_od = pd.read_csv(od_demand_path + \"External OD grouped by timestamp.csv\")\n",
    "\n",
    "# Remove centroids with no demand\n",
    "external_od = external_od[external_od[\"counts\"] != 0]\n",
    "internal_od = internal_od[internal_od[\"counts\"] != 0]\n",
    "\n",
    "# Fix analysis timestep at 6 PM\n",
    "internal_od_6_pm = internal_od[internal_od[\"dt_15\"]==\"18:00:00\"]\n",
    "external_od_6_pm = external_od[external_od[\"dt_15\"]==\"18:0\"]\n",
    "\n",
    "internal_od_6_pm = internal_od_6_pm.reset_index()\n",
    "external_od_6_pm = external_od_6_pm.reset_index()\n",
    "\n",
    "external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(external_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = external_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = external_od_6_pm['CentroidID_D'][i]\n",
    "    demand = external_od_6_pm['counts'][i]\n",
    "    external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "internal_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "int_ext_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(internal_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = internal_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = internal_od_6_pm['CentroidID_D'][i]\n",
    "    demand = internal_od_6_pm['counts'][i]\n",
    "    if origin_id in centroid_gravity and dest_id in centroid_gravity:\n",
    "        if \"ext\" not in origin_id and \"ext\" not in dest_id:\n",
    "            internal_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "        else:\n",
    "                int_ext_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Internal):\", internal_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (External):\", external_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Total):\", internal_od_6_pm.counts.sum() + external_od_6_pm.counts.sum())\n",
    "\n",
    "if print_kepler:\n",
    "    demand_6_pm_map = KeplerGl(height=600, config=dict_config[\"OD Demand\"])\n",
    "    demand_6_pm_map.add_data(data = external_centroids, name=\"External Centroids\")\n",
    "\n",
    "    int_batches = {\"Internal Demand\":[(0, 2), (2, 3), (3, 4), (4, 5), (5, float('inf'))]}\n",
    "    int_ext_batches = {\"Internal/External Demand\":[(0, 2), (5, 10), (10, 15), (15, float('inf'))]}\n",
    "    ext_batches = {\"External Demand\":[(0, 10), (10, 50), (50, 250), (250, float('inf'))]}\n",
    "    dem_map = {\"Internal Demand\":internal_demand, \"Internal/External Demand\":int_ext_demand, \"External Demand\":external_demand}\n",
    "\n",
    "    for batch_list in [int_batches, int_ext_batches, ext_batches]:\n",
    "        for b_name, batches in batch_list.items():\n",
    "            for batch in batches:\n",
    "                demand_6_pm_map.add_data(data = dem_map[b_name][(dem_map[b_name]['counts'] >= batch[0])\\\n",
    "                                            & (dem_map[b_name]['counts'] < batch[1])],\\\n",
    "                                     name=\"{} in [{},{}) veh/15 min\".format(b_name, batch[0], batch[1]))\n",
    "\n",
    "    demand_6_pm_map.add_data(data = internal_taz, name=\"Internal TAZs\")\n",
    "\n",
    "    demand_6_pm_map.save_to_html(file_name=data_path+\"/Data processing/Kepler maps/Demand/6pm_demand_batches_map.html\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing KeplerGl config for 7_6pm_demand_batches_map.html\n",
    "'''\n",
    "write_configs(dict_config, \"OD Demand\", fremont_map.config)\n",
    "dict_config = get_configs()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Flow data\n",
    "\n",
    "## To do\n",
    "\n",
    "### 4.2 Speed data\n",
    "\n",
    "## To do\n",
    "\n",
    "### 4.3 Trajectory data\n",
    "\n",
    "## To do\n",
    "\n",
    "### 4.4 Travel time data\n",
    "\n",
    "## To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation data\n",
    "\n",
    "## To do: add the biplots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key performance indicators\n",
    "\n",
    "## To do:\n",
    "- travel time distribution\n",
    "- isochrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is to do later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Street Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ogr\n",
    "\n",
    "driver=ogr.GetDriverByName('OSM')\n",
    "path_osm = data_path + \"/Raw data (do not touch)/Network/Map/OSM/\"\n",
    "\n",
    "\n",
    "data = driver.Open(path_osm + 'map1111.osm')\n",
    "\n",
    "for i in range(data.GetLayerCount()):\n",
    "    print(data.GetLayerByIndex(i).GetName())\n",
    "    \n",
    "layer = data.GetLayer('points')\n",
    "\n",
    "features=[x for x in layer]\n",
    "print(len(features))\n",
    "\n",
    "data_list=[]\n",
    "i = 0\n",
    "for feature in features:\n",
    "    i = i + 1\n",
    "    data=feature.ExportToJson(as_object=True)\n",
    "    coords=data['geometry']['coordinates']\n",
    "    shapely_geo=Point(coords[0],coords[1])\n",
    "    name=data['properties']['name']\n",
    "    highway=data['properties']['highway']\n",
    "    print(highway)\n",
    "    other_tags=data['properties']['other_tags']\n",
    "    if other_tags and 'amenity' in other_tags:\n",
    "        feat=[x for x in other_tags.split(',') if 'amenity' in x][0]\n",
    "        amenity=feat[feat.rfind('>')+2:feat.rfind('\"')]\n",
    "    else:\n",
    "        amenity=None\n",
    "    data_list.append([name,highway,amenity,shapely_geo])\n",
    "    if i > 20:\n",
    "        break\n",
    "# gdf=gpd.GeoDataFrame(data_list,columns=['Name','Highway','Amenity','geometry'],crs={'init': 'epsg:4326'}).to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
