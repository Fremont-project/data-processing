{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual rendering notebook\n",
    "\n",
    "The goal of this notebook is to provide visual rendering of data using Kepler.gl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global variables\n",
    "\n",
    "# Setting up the Coordinate Reference Systems up front in the necessary format.\n",
    "crs_degree = {'init': 'epsg:4326'} # CGS_WGS_1984 (what the GPS uses)\n",
    "\n",
    "# --- Paths\n",
    "\n",
    "# Root path of Fremont Dropbox\n",
    "import os\n",
    "import sys\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from fremontdropbox import get_dropbox_location\n",
    "# Root path of the Dropbox business account\n",
    "dbx = get_dropbox_location()\n",
    "\n",
    "# Temporary! Location of the folder where the restructuring is currently happening\n",
    "data_path = dbx + '/Private Structured data collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf(path):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        path: path of the file to read as a geodataframe\n",
    "        \n",
    "    return:\n",
    "        a GeoDataFrame (with Geopandas) corresponding to the file path\n",
    "    \"\"\"\n",
    "    gdf = gpd.GeoDataFrame.from_file(path)\n",
    "    gdf = gdf.to_crs('epsg:4326')\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "config_file = open('visualization_config.txt', 'r')\n",
    "text = config_file.read()\n",
    "dict_config = ast.literal_eval(text)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project delimitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cfd38e94234d1cb451da4ca7ace510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Project delimitation': {'index': [0], 'columns': ['Type', 'geometry'], 'data': [['Delimitation…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_del_path = dbx + \"/Private Structured data collection/Manual-made dataset (do not touch)/Network/Map/Project Delimitation/Project_delimitation.shp\"\n",
    "project_del = to_gdf(project_del_path)\n",
    "\n",
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = project_del[project_del.Type == \"Delimitation\"], name=\"Project delimitation\")\n",
    "fremont_map.add_data(data = project_del[project_del.Type == \"Box\"], name=\"Project box delimitation\")\n",
    "# fremont_map.add_data(data = gdf, name=\" delimitation\")\n",
    "\n",
    "\n",
    "fremont_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aimsun map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aimsun_path = dbx + '/Private Structured data collection/Aimsun/Inputs/'\n",
    "\n",
    "detectors = to_gdf(aimsun_path +'detectors.shp')\n",
    "meterings = to_gdf(aimsun_path +'meterings.shp')\n",
    "# centroids = to_gdf(aimsun_path +'centroids.shp')\n",
    "# centroid_connections = to_gdf(aimsun_path +'centroid_connections.shp')\n",
    "nodes = to_gdf(aimsun_path +'nodes.shp')\n",
    "# polygons = to_gdf(aimsun_path +'polygons.shp')\n",
    "sections = to_gdf(aimsun_path +'sections.shp')\n",
    "sectionsGeo = to_gdf(aimsun_path +'sectionsGeo.shp')\n",
    "turnings = to_gdf(aimsun_path +'turnings.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cell is useful to better understand and render the network\n",
    "## To do Theo: get the different screenshot of the section characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Nodes-------\n",
      "\n",
      "Total number of nodes: 2013\n",
      "Nodes type values: [0. 1. 2. 3.]\n",
      "Number of nodes with type 0.0: 585\n",
      "Number of nodes with type 1.0: 1259\n",
      "Number of nodes with type 2.0: 35\n",
      "Number of nodes with type 3.0: 134\n",
      "\n",
      "------Sections------\n",
      "Total number of section: 5626\n",
      "\n",
      "Road type values: [175. 177. 179. 180. 182. 184. 185.]\n",
      "175 = Motorway, 177 = Primary, 179 = Residential, 180 = Secondary, 182.0 = Tertiary, 184.0 = Trunk, 185.0 = Unclassified\n",
      "Number of sections with type 175.0: 111\n",
      "Number of sections with type 177.0: 373\n",
      "Number of sections with type 179.0: 2916\n",
      "Number of sections with type 180.0: 393\n",
      "Number of sections with type 182.0: 189\n",
      "Number of sections with type 184.0: 51\n",
      "Number of sections with type 185.0: 1593\n",
      "\n",
      "Speed values: [ 16.  40.  50.  56.  64.  72.  88.  90. 104. 120.]\n",
      "Number of sections with speed 16.0 (km/h): 4\n",
      "Number of sections with speed 40.0 (km/h): 44\n",
      "Number of sections with speed 50.0 (km/h): 5116\n",
      "Number of sections with speed 56.0 (km/h): 176\n",
      "Number of sections with speed 64.0 (km/h): 70\n",
      "Number of sections with speed 72.0 (km/h): 102\n",
      "Number of sections with speed 88.0 (km/h): 2\n",
      "Number of sections with speed 90.0 (km/h): 12\n",
      "Number of sections with speed 104.0 (km/h): 36\n",
      "Number of sections with speed 120.0 (km/h): 64\n",
      "\n",
      "Capacity values: [  500.   700.   800.   900.  1000.  1400.  1600.  1800.  2100.  2400.\n",
      "  2700.  3200.  3500.  3600.  4000.  4200.  4500.  4800.  5400.  6000.\n",
      "  6300.  7200.  8400. 10500. 12600.]\n",
      "Number of sections with capacity 500.0 (veh/h): 1591\n",
      "Number of sections with capacity 700.0 (veh/h): 2995\n",
      "Number of sections with capacity 800.0 (veh/h): 146\n",
      "Number of sections with capacity 900.0 (veh/h): 138\n",
      "Number of sections with capacity 1000.0 (veh/h): 2\n",
      "Number of sections with capacity 1400.0 (veh/h): 91\n",
      "Number of sections with capacity 1600.0 (veh/h): 216\n",
      "Number of sections with capacity 1800.0 (veh/h): 183\n",
      "Number of sections with capacity 2100.0 (veh/h): 58\n",
      "Number of sections with capacity 2400.0 (veh/h): 44\n",
      "Number of sections with capacity 2700.0 (veh/h): 44\n",
      "Number of sections with capacity 3200.0 (veh/h): 3\n",
      "Number of sections with capacity 3500.0 (veh/h): 1\n",
      "Number of sections with capacity 3600.0 (veh/h): 31\n",
      "Number of sections with capacity 4000.0 (veh/h): 2\n",
      "Number of sections with capacity 4200.0 (veh/h): 22\n",
      "Number of sections with capacity 4500.0 (veh/h): 3\n",
      "Number of sections with capacity 4800.0 (veh/h): 2\n",
      "Number of sections with capacity 5400.0 (veh/h): 1\n",
      "Number of sections with capacity 6000.0 (veh/h): 3\n",
      "Number of sections with capacity 6300.0 (veh/h): 24\n",
      "Number of sections with capacity 7200.0 (veh/h): 1\n",
      "Number of sections with capacity 8400.0 (veh/h): 14\n",
      "Number of sections with capacity 10500.0 (veh/h): 9\n",
      "Number of sections with capacity 12600.0 (veh/h): 2\n",
      "\n",
      "Function class values: [1 2 3 5]\n",
      "Number of sections with function class 1: 111\n",
      "Number of sections with function class 2: 51\n",
      "Number of sections with function class 3: 955\n",
      "Number of sections with function class 5: 4509\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fremont_map = KeplerGl(height=600)\n",
    "\n",
    "node_type = np.sort(nodes.nodetype.unique())\n",
    "print(\"--------Nodes-------\")\n",
    "print()\n",
    "print(\"Total number of nodes: \" + str(nodes['id'].count()))\n",
    "print(\"Nodes type values: \" + str(node_type))\n",
    "for i in node_type:\n",
    "    print(\"Number of nodes with type \" + str(i) + \": \" + str(nodes[nodes['nodetype'] == i]['id'].count()))\n",
    "    if False:\n",
    "        fremont_map.add_data(data = nodes[nodes['nodetype'] == i], name=\"Nodes type \" + str(i))\n",
    "\n",
    "    \n",
    "print()\n",
    "print(\"------Sections------\")\n",
    "\n",
    "print(\"Total number of section: \" + str(sections['id'].count()))\n",
    "print()\n",
    "section_type = np.sort(sections.rd_type.unique())\n",
    "print(\"Road type values: \" + str(section_type))\n",
    "print(\"175 = Motorway, 177 = Primary, 179 = Residential, 180 = Secondary, 182.0 = Tertiary, 184.0 = Trunk, 185.0 = Unclassified\")\n",
    "for i in section_type:\n",
    "    print(\"Number of sections with type \" + str(i) + \": \" + str(sections[sections['rd_type'] == i]['id'].count()))\n",
    "    if False:\n",
    "        fremont_map.add_data(data = sections[sections['rd_type'] == i], name=\"Sections type \" + str(i))\n",
    "    \n",
    "print()\n",
    "section_speed = np.sort(sections.speed.unique())\n",
    "print(\"Speed values: \" + str(section_speed))\n",
    "for i in section_speed:\n",
    "    print(\"Number of sections with speed \" + str(i) + \" (km/h): \" + str(sections[sections['speed'] == i]['id'].count()))\n",
    "    if False:\n",
    "        fremont_map.add_data(data = sections[sections['speed'] == i], name=\"Sections speed \" + str(i))\n",
    "\n",
    "print()\n",
    "section_cap = np.sort(sections.capacity.unique())\n",
    "print(\"Capacity values: \" + str(section_cap))\n",
    "for i in section_cap:\n",
    "    print(\"Number of sections with capacity \" + str(i) + \" (veh/h): \" + str(sections[sections['capacity'] == i]['id'].count()))\n",
    "    if False:\n",
    "        fremont_map.add_data(data = sections[sections['capacity'] == i], name=\"Sections capacity \" + str(i))\n",
    "    \n",
    "print()\n",
    "section_func = np.sort(sections.func_class.unique())\n",
    "print(\"Function class values: \" + str(section_func))\n",
    "for i in section_func:\n",
    "    print(\"Number of sections with function class \" + str(i) + \": \" + str(sections[sections['func_class'] == i]['id'].count()))\n",
    "    if False:\n",
    "        fremont_map.add_data(data = sections[sections['func_class'] == i], name=\"Sections function class \" + str(i))\n",
    "    \n",
    "# fremont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid_config = centroid_map.config\n",
    "# file.write(\",'centroid_config': \" + str(centroid_config))\n",
    "# file.write(\"}\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c866c7048fa4a0db3c09dc066119dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Detectors': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = detectors, name=\"Detectors\")\n",
    "fremont_map.add_data(data = meterings, name=\"meterings\")\n",
    "fremont_map.add_data(data = nodes, name=\"nodes\")\n",
    "# fremont_map.add_data(data = polygons, name=\"polygons\")\n",
    "fremont_map.add_data(data = sections, name=\"sections\")\n",
    "fremont_map.add_data(data = sectionsGeo, name=\"sectionsGeo\")\n",
    "fremont_map.add_data(data = turnings, name=\"turnings\")\n",
    "# fremont_map.add_data(data = centroids, name=\"centroids\")\n",
    "# fremont_map.add_data(data = centroid_connections, name=\"centroid_connections\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aimsun centroids and centroids connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'centroid_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-40951953b2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcentroid_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeplerGl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'centroid_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcentroid_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ext'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"External centroids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcentroid_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Internal centroids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcentroid_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcentroid_connections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Centroid connections\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# fremont_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'from'], name=\"Outgoing centroid connections\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'centroid_config'"
     ]
    }
   ],
   "source": [
    "centroid_map = KeplerGl(height=600, config=dict_config['centroid_config'])\n",
    "centroid_map.add_data(data = centroids[centroids['name'].str.contains('ext')], name=\"External centroids\")\n",
    "centroid_map.add_data(data = centroids[centroids['name'].str.contains('int')], name=\"Internal centroids\")\n",
    "centroid_map.add_data(data = centroid_connections, name=\"Centroid connections\")\n",
    "# fremont_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'from'], name=\"Outgoing centroid connections\")\n",
    "# fremont_map.add_data(data = centroid_connections[centroid_connections['direction'] == 'to'], name=\"Incoming centroid connections\")\n",
    "centroid_map.add_data(data = sections, name=\"sections\")\n",
    "centroid_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aimsun road section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5c16b7c275414180360ca4d22b5393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(config={'version': 'v1', 'config': {'visState': {'filters': [], 'layers': [{'id': 'cmvn55k', 'type': …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "road_section_map = KeplerGl(height=600, config=dict_config['road_type_config'])\n",
    "\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 175.0], name=\"Motorway\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 177.0], name=\"Primary\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 179.0], name=\"Residential\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 180.0], name=\"Secondary\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 182.0], name=\"Tertiary\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 184.0], name=\"Trunk\")\n",
    "road_section_map.add_data(data = sections[sections['rd_type'] == 185.0], name=\"Unclassified\")\n",
    "\n",
    "road_section_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# road_type_config = road_section_map.config\n",
    "# file = open(\"visualization_config.txt\", 'w')\n",
    "# file.write(\"{'road_type_config': \" + str(road_type_config))\n",
    "# file.write(\"}\")\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic signals and stop signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gdf_csv(path):\n",
    "# https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-wkt-format\n",
    "    df = pd.read_csv(path)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, crs='epsg:4326', geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_infra_path = data_path + \"/Manual-made dataset (do not touch)/Network/Infrastructure/\"\n",
    "\n",
    "stop_signs = to_gdf_csv(network_infra_path + \"Stop signs location/Stop_Signs.csv\")\n",
    "traffic_lights = to_gdf_csv(network_infra_path + \"Traffic lights location/Traffic_Lights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d5bc94d0a2498783686ff398426b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Stop signs': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = stop_signs, name=\"Stop signs\")\n",
    "fremont_map.add_data(data = traffic_lights, name=\"Traffic lights\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop signs: 313\n",
      "Number of traffic lights: 123\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stop signs: \" + str(stop_signs.__OBJECTID.count()))\n",
    "print(\"Number of traffic lights: \" + str(traffic_lights.__OBJECTID.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFCTA demand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFCTA trips...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path):\n",
    "    \"\"\"\n",
    "    To do\n",
    "    Reading and merging the files of the SFCTA demand data\n",
    "    \"\"\"\n",
    "    int_int_trips = pd.read_csv(int_int_path)\n",
    "    int_ext_trips = pd.read_csv(int_ext_path)\n",
    "    ext_int_trips = pd.read_csv(ext_int_path)\n",
    "\n",
    "    internal_trips = pd.DataFrame.merge(int_int_trips, int_ext_trips, 'outer')\n",
    "    internal_trips = pd.DataFrame.merge(internal_trips, ext_int_trips, 'outer')\n",
    "    return internal_trips\n",
    "\n",
    "SFCTA_path = data_path+ '/Data processing/Raw/Demand/OD demand/SFCTA demand data/'\n",
    "int_int_path = SFCTA_path + \"internal_fremont_legs.csv\"\n",
    "int_ext_path = SFCTA_path + \"starting_fremont_legs.csv\"\n",
    "ext_int_path = SFCTA_path + \"ending_fremont_legs.csv\"\n",
    "\n",
    "print(\"Loading SFCTA trips...\")\n",
    "internal_trips = get_sfcta_dataframe(int_int_path, int_ext_path, ext_int_path)\n",
    "internal_trips.start_time = internal_trips.start_time.apply(lambda x: pd.Timestamp(x).time())\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      leg_id start_time  start_node_lat  start_node_lng  end_node_lat  \\\n",
      "21  88262892   17:44:00        37.50659      -121.93902      37.50078   \n",
      "24  88422482   19:47:00        37.50879      -121.94868      37.50078   \n",
      "37  88935641   19:28:00        37.50002      -121.93801      37.50078   \n",
      "69  89489427   19:00:00        37.51180      -121.95203      37.51287   \n",
      "70  89524371   19:55:00        37.51018      -121.94178      37.51198   \n",
      "\n",
      "    end_node_lng  \n",
      "21    -121.93948  \n",
      "24    -121.93948  \n",
      "37    -121.93948  \n",
      "69    -121.94508  \n",
      "70    -121.95112  \n",
      "43229\n"
     ]
    }
   ],
   "source": [
    "# print(internal_trips.head())\n",
    "afternoon_demand = internal_trips[(internal_trips.start_time >= datetime.time(14, 0, 0)) & (internal_trips.start_time <= datetime.time(20, 0, 0))]\n",
    "print(afternoon_demand.head())\n",
    "print(afternoon_demand.leg_id.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             leg_id  start_node_lat  start_node_lng  end_node_lat  \\\n",
      "count  4.322900e+04    43229.000000    43229.000000  43229.000000   \n",
      "mean   9.542290e+07       37.520589     -121.948976     37.510473   \n",
      "std    2.144402e+06        0.108612        0.079963      0.091773   \n",
      "min    8.408609e+07       36.987680     -122.713120     36.976260   \n",
      "25%    9.560135e+07       37.500780     -121.958390     37.495980   \n",
      "50%    9.575408e+07       37.525840     -121.938940     37.512870   \n",
      "75%    9.592057e+07       37.542900     -121.920970     37.536030   \n",
      "max    1.070561e+08       38.463380     -121.478470     38.351290   \n",
      "\n",
      "       end_node_lng  \n",
      "count  43229.000000  \n",
      "mean    -121.954313  \n",
      "std        0.076749  \n",
      "min     -122.595370  \n",
      "25%     -121.955310  \n",
      "50%     -121.942500  \n",
      "75%     -121.924490  \n",
      "max     -121.548700  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/miniconda3/envs/dev/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def fxy(start_lat, start_lng, end_lat, end_lng):\n",
    "    return LineString([(start_lng, start_lat), (end_lng, end_lat)])\n",
    "\n",
    "afternoon_demand['geometry'] = afternoon_demand.apply(lambda x: fxy(x['start_node_lat'], x['start_node_lng'], x['end_node_lat'], x['end_node_lng']), axis=1)\n",
    "\n",
    "afternoon_demand_gpd = gpd.GeoDataFrame(afternoon_demand, crs='epsg:4326', geometry = afternoon_demand.geometry)\n",
    "print(afternoon_demand_gpd.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6bda1c94b541d58f6b79d48cb1e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Demand': {'index': [10069, 23970, 35507, 37361, 42014, 42227, 46974, 51686, 51868, 53382, 5403…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "begin_time = datetime.time(16, 0, 0)\n",
    "end_time = datetime.time(16, 2, 0)\n",
    "demand_in_time = afternoon_demand_gpd[(afternoon_demand_gpd.start_time >= begin_time) & (afternoon_demand_gpd.start_time <= end_time)]\n",
    "\n",
    "# print(demand_in_time)\n",
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = demand_in_time[['leg_id','geometry']], name=\"Demand\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transportation Analysis Zones\n",
    "\n",
    "## To do Ayush: \n",
    "1. Put in red external demand. Put in blue internal demand. Maybe cluster some internal TAZs together to get a better rendering of the demand data. Set the width of the lines to be a function of the lines. If possible get some legend about how width is related to count.\n",
    "\n",
    "### To do: we should merge some internal centroids for visualization purposes. \n",
    "Maybe for visualization we can have only 30 internal centroids (just merged some together temporaly for the visualization).\n",
    "Then for the width, the easiest thing might be to create some batches for the data. Something like:\n",
    "`fremont_map.add_data(data = internal_demand[internal_demand['counts'] < 5], name=\"Internal Demand < 5 veh/15 min\")`\n",
    "`fremont_map.add_data(data = internal_demand[(internal_demand['counts'] >= 5) & (internal_demand['counts'] < 20)], name=\"Internal Demand in [5,20) veh/h\")\")`\n",
    "`...`\n",
    "\n",
    "Then serialize a config where the width of the line depends on the counts and color are: red for external-external, dark blue for internal-external, light blue for internal-internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Vehicles Analyzed at 6 PM (Internal): 3331\n",
      "Number of Vehicles Analyzed at 6 PM (External): 1263\n",
      "Number of Vehicles Analyzed at 6 PM (Total): 4594\n",
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5c8cf6eff647ce85ba64da3853eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'External/External Demand': {'index': [0, 1, 2, 3, 4, 5, 7], 'columns': ['CentroidID_O', 'Centr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "path_taz = data_path + \"/Data processing/Auxiliary files/Demand/OD demand/TAZ\"\n",
    "internal_taz = to_gdf(path_taz + \"/Internal_TAZ.shp\")\n",
    "# external_taz = to_gdf(path_taz + \"/External_TAZ.shp\")\n",
    "external_centroids = to_gdf(path_taz + \"/External_centroids.shp\")\n",
    "\n",
    "# Get gravity centers for all TAZs (internal and external)\n",
    "centroid_gravity = {}\n",
    "for i in range(len(internal_taz['geometry'])):\n",
    "    centroid_gravity[internal_taz['CentroidID'][i]] = internal_taz['geometry'][i].centroid\n",
    "for i in range(len(external_centroids['geometry'])):\n",
    "    centroid_gravity[external_centroids['CentroidID'][i]] = external_centroids['geometry'][i]\n",
    "\n",
    "od_demand_path = data_path + \"/Data processing/Temporary exports to be copied to processed data/Demand/OD demand/\" \n",
    "internal_od = pd.read_csv(od_demand_path + \"Internal OD grouped by timestamp.csv\")\n",
    "external_od = pd.read_csv(od_demand_path + \"External OD grouped by timestamp.csv\")\n",
    "\n",
    "# Remove centroids with no demand\n",
    "external_od = external_od[external_od[\"counts\"] != 0]\n",
    "internal_od = internal_od[internal_od[\"counts\"] != 0]\n",
    "\n",
    "# Fix analysis timestep at 6 PM\n",
    "internal_od_6_pm = internal_od[internal_od[\"dt_15\"]==\"18:00\"]\n",
    "external_od_6_pm = external_od[external_od[\"dt_15\"]==\"18:0\"]\n",
    "\n",
    "internal_od_6_pm = internal_od_6_pm.reset_index()\n",
    "external_od_6_pm = external_od_6_pm.reset_index()\n",
    "\n",
    "external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(external_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = external_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = external_od_6_pm['CentroidID_D'][i]\n",
    "    demand = external_od_6_pm['counts'][i]\n",
    "    external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "internal_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "internal_external_demand = gpd.GeoDataFrame(columns=['CentroidID_O', 'CentroidID_D', \"counts\", 'geometry'])\n",
    "for i in range(len(internal_od_6_pm['CentroidID_O'])):\n",
    "    origin_id = internal_od_6_pm['CentroidID_O'][i]\n",
    "    dest_id = internal_od_6_pm['CentroidID_D'][i]\n",
    "    demand = internal_od_6_pm['counts'][i]\n",
    "    if origin_id in centroid_gravity and dest_id in centroid_gravity:\n",
    "        if \"ext\" not in origin_id and \"ext\" not in dest_id:\n",
    "            internal_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "        else:\n",
    "                internal_external_demand.loc[i] = [origin_id, dest_id, demand, LineString([centroid_gravity[origin_id], centroid_gravity[dest_id]])]\n",
    "\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Internal):\", internal_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (External):\", external_od_6_pm.counts.sum())\n",
    "print(\"Number of Vehicles Analyzed at 6 PM (Total):\", internal_od_6_pm.counts.sum() + external_od_6_pm.counts.sum())\n",
    "\n",
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = external_demand[external_demand['counts'] > 2], name=\"External/External Demand\")\n",
    "fremont_map.add_data(data = internal_demand[internal_demand['counts'] > 2], name=\"Internal/Internal Demand\")\n",
    "fremont_map.add_data(data = internal_external_demand[internal_external_demand['counts'] > 2], name=\"Internal/External Demand\")\n",
    "fremont_map.add_data(data = internal_taz, name=\"Internal TAZs\")\n",
    "fremont_map.add_data(data = external_centroids, name=\"External centroids\")\n",
    "\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Street Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points\n",
      "lines\n",
      "multilinestrings\n",
      "multipolygons\n",
      "other_relations\n",
      "1885\n",
      "motorway_junction\n",
      "motorway_junction\n",
      "crossing\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "motorway_junction\n",
      "traffic_signals\n",
      "turning_circle\n",
      "traffic_signals\n",
      "motorway_junction\n",
      "turning_circle\n",
      "turning_circle\n",
      "turning_circle\n"
     ]
    }
   ],
   "source": [
    "import ogr\n",
    "\n",
    "driver=ogr.GetDriverByName('OSM')\n",
    "path_osm = data_path + \"/Raw data (do not touch)/Network/Map/OSM/\"\n",
    "\n",
    "\n",
    "data = driver.Open(path_osm + 'map1111.osm')\n",
    "\n",
    "for i in range(data.GetLayerCount()):\n",
    "    print(data.GetLayerByIndex(i).GetName())\n",
    "    \n",
    "layer = data.GetLayer('points')\n",
    "\n",
    "features=[x for x in layer]\n",
    "print(len(features))\n",
    "\n",
    "data_list=[]\n",
    "i = 0\n",
    "for feature in features:\n",
    "    i = i + 1\n",
    "    data=feature.ExportToJson(as_object=True)\n",
    "    coords=data['geometry']['coordinates']\n",
    "    shapely_geo=Point(coords[0],coords[1])\n",
    "    name=data['properties']['name']\n",
    "    highway=data['properties']['highway']\n",
    "    print(highway)\n",
    "    other_tags=data['properties']['other_tags']\n",
    "    if other_tags and 'amenity' in other_tags:\n",
    "        feat=[x for x in other_tags.split(',') if 'amenity' in x][0]\n",
    "        amenity=feat[feat.rfind('>')+2:feat.rfind('\"')]\n",
    "    else:\n",
    "        amenity=None\n",
    "    data_list.append([name,highway,amenity,shapely_geo])\n",
    "    if i > 20:\n",
    "        break\n",
    "# gdf=gpd.GeoDataFrame(data_list,columns=['Name','Highway','Amenity','geometry'],crs={'init': 'epsg:4326'}).to_crs(epsg=3310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External TAZs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13db0c79e6b14e2c8ec2cbf1a40c8c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'External TAZs': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'columns': ['OBJECTID_1', 'FID_1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "path_taz = data_path + \"/Manual-made dataset (do not touch)/Demand/OD demand/External TAZ/External Centroid zones/\"\n",
    "external_taz = to_gdf(path_taz + \"/ExternalCentroidZones.shp\")\n",
    "\n",
    "fremont_map = KeplerGl(height=600)\n",
    "fremont_map.add_data(data = external_taz, name=\"External TAZs\")\n",
    "fremont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
