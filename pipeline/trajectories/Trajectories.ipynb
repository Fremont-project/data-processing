{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories Clustering and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of this Notebook\n",
    "Parsing and clustering the avaiable trajectory data and analyzing aggregate measurements.\n",
    "***\n",
    "**Outputs:**\n",
    "- No file outputs (yet). All visualizations and analysis contained within IPython Notebook.\n",
    "\n",
    "**Inputs:**\n",
    "- TRAJECTORIES.CSV [Manually Made] from raw trajectories data. Available on Dropbox in `Theophile Cabannes/Data Collection/Raw data/Here data/step_019_organize_by_provider`.\n",
    "\n",
    "**Temporary Files Within the Pipeline:** \n",
    "- No temporary files.\n",
    "\n",
    "**Dependent Scripts:**\n",
    "- No script dependencies.\n",
    "\n",
    "**Dependent Libraries:**\n",
    "- [numpy](https://numpy.org), can be installed with `pip install numpy`\n",
    "- pandas\n",
    "- os\n",
    "- csv\n",
    "- json\n",
    "- geopy\n",
    "- matplotlib\n",
    "- gmplot\n",
    "- dipy\n",
    "- shapely\n",
    "- keplergl\n",
    "***\n",
    "**Sections:**\n",
    "- A. [Parse Raw Trajectory Data](#section_ID_a)\n",
    "- B. [Clustering](#section_ID_b)\n",
    "- C. [Plotting, Mapping & Analysis](#section_ID_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To dos\n",
    "0. Discuss with Michal about the way you did the clustering, as it might be helpful for him to reuse some of your code.\n",
    "1. Use the module fremont dropbox to get the folders from the dropbox (see next cell)\n",
    "2. Create both files trajectories and trajectories condensed in the current iPython notebook.\n",
    "    - Put them in `/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Trajectories`\n",
    "3. Use the external and internal TAZs instead of the sklearn clustering to cluster the trajectories depending on their origin and destination\n",
    "    - TAZ are shapefiles in `Private Structured data collection/Data processing/Raw/Demand/OD demand/TAZ`\n",
    "4. Write a function that takes as input the ids of the origin and destination TAZ and output the corresponding trajectories using Kepler.gl\n",
    "5. Remove `trajectories.csv` and `trajectories_condensed.csv` from GitHub (they are under NDA)\n",
    "6. Generate all Kepler.gl maps in `/Private Structured data collection/Data processing/Temporary exports to be copied to processed data`\n",
    "\n",
    "### To do later\n",
    "7. Match paths to road sections (see Jane McFarlan for that)\n",
    "8. For every O-D pairs (where O and D are TAZ id), and 15 minutes time step output the corresponding paths used by drivers\n",
    "9. Compare the paths used by drivers using Here data, with the ones used by drivers in Aimsun simulations.\n",
    "10. For the path going from South of I-680N to North of I-680N, deduces the percentage of drivers using local roads instead of staying on the Highway for different time of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/theophile/Fremont Dropbox/Theophile Cabannes/Private Structured data collection/Data processing/Raw/Demand/Flow_speed/Here data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fremontdropbox import get_dropbox_location\n",
    "\n",
    "dropbox_dir = get_dropbox_location()\n",
    "\n",
    "rootdir = dropbox_dir + \"/Private Structured data collection/Data processing/Raw/Demand/Flow_speed/Here data\"\n",
    "print(rootdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopy.distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gmplot import gmplot\n",
    "from keplergl import KeplerGl\n",
    "from dipy.segment.metric import Metric\n",
    "from dipy.segment.metric import ResampleFeature\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from shapely.geometry import MultiPoint\n",
    "from geopy.distance import great_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ID_a\"></a>\n",
    "## A. Parse Raw Trajectory Data into Singular CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootdir = './step_019_organize_by_provider'\n",
    "counter = 0\n",
    "with open('trajectories.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Time\", \"Speed\", \"Heading\", \"Origin X\", \"Origin Y\", \"Dest X\", \"Dest Y\", \"Source\"])\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            counter += 1\n",
    "            path = os.path.join(subdir, file)\n",
    "\n",
    "            if (not path == \"./step_019_organize_by_provider\\.ipynb_checkpoints\\Test_python-checkpoint.ipynb\"):\n",
    "\n",
    "                with open(path) as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                for feature in data['features']:\n",
    "                    \n",
    "                    trajectory = [feature['properties']['time'], feature['properties']['speed'], feature['properties']['heading'],\\\n",
    "                                    feature['geometry']['coordinates'][0][0], feature['geometry']['coordinates'][0][1],\\\n",
    "                                    feature['geometry']['coordinates'][1][0], feature['geometry']['coordinates'][1][1], os.path.basename(path).split(\".\")[0]]\n",
    "                    \n",
    "                    writer.writerow(trajectory)\n",
    "                    # print(trajectory)\n",
    "\n",
    "    print(\"All trajectory data has been parsed.\", counter, \"files total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rootdir = './step_019_organize_by_provider'\n",
    "counter = 0\n",
    "with open('trajectories_condensed.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Time Start\", \"Time End\", \"Origin X\", \"Origin Y\", \"Dest X\", \"Dest Y\", \"Source\"])\n",
    "\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            counter += 1\n",
    "            path = os.path.join(subdir, file)\n",
    "\n",
    "            if (not path == \"./step_019_organize_by_provider\\.ipynb_checkpoints\\Test_python-checkpoint.ipynb\"):\n",
    "\n",
    "                with open(path) as f:\n",
    "                    data = json.load(f)\n",
    "                start, end = data['features'][0], data['features'][-1]\n",
    "                \n",
    "                trajectory = [start['properties']['time'], end['properties']['time'], start['geometry']['coordinates'][0][0],\\\n",
    "                                start['geometry']['coordinates'][0][1], end['geometry']['coordinates'][1][0],\\\n",
    "                                end['geometry']['coordinates'][1][1], os.path.basename(path).split(\".\")[0]]\n",
    "                \n",
    "                writer.writerow(trajectory)\n",
    "                    # print(trajectory)\n",
    "\n",
    "    print(\"All trajectory data has been parsed.\", counter, \"files total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ID_b\"></a>\n",
    "## B. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.read_csv(\"trajectories.csv\")\n",
    "\n",
    "# represent GPS points as (lat, lon)\n",
    "coords = df.as_matrix(columns=['Origin Y', 'Origin X'])\n",
    "# earth's radius in km\n",
    "kms_per_radian = 6371.0088\n",
    "# define epsilon as 0.1 kilometers, converted to radians for use by haversine\n",
    "epsilon = 0.1 / kms_per_radian\n",
    "\n",
    "# eps is the max distance that points can be from each other to be considered in a cluster\n",
    "# min_samples is the minimum cluster size (everything else is classified as noise)\n",
    "db = DBSCAN(eps=epsilon, min_samples=100, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "cluster_labels = db.labels_\n",
    "# get the number of clusters (ignore noisy samples which are given the label -1)\n",
    "num_clusters = len(set(cluster_labels) - set([-1]))\n",
    "\n",
    "print ('Clustered ' + str(len(df)) + ' points to ' + str(num_clusters) + ' clusters')\n",
    "\n",
    "# turn the clusters in to a pandas series\n",
    "clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ID_c\"></a>\n",
    "## C. Plotting, Mapping, and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "\n",
    "# get the centroid point for each cluster\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "lats, lons = zip(*centermost_points)\n",
    "\n",
    "rep_points = pd.DataFrame({'lon':lons, 'lat':lats})\n",
    "fig, ax = plt.subplots(figsize=[15, 10])\n",
    "\n",
    "rs_scatter = ax.scatter(rep_points['lon'][0], rep_points['lat'][0], c='#99cc99', edgecolor='None', alpha=0.7, s=maxes[0]/10)\n",
    "\n",
    "for i in range(1, num_clusters):\n",
    "    ax.scatter(rep_points['lon'][i], rep_points['lat'][i], c='#99cc99', edgecolor='None', alpha=0.7, s=maxes[i]*2)\n",
    "\n",
    "df_scatter = ax.scatter(df['Origin X'], df['Origin Y'], c='k', alpha=0.9, s=3)\n",
    "\n",
    "ax.set_title('Full GPS trace vs. DBSCAN clusters')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.legend([df_scatter, rs_scatter], ['GPS Points', 'Cluster Centers'], loc='upper right')\n",
    "\n",
    "labels = ['Cluster {0}'.format(i) for i in range(1, num_clusters+1)]\n",
    "for label, x, y in zip(labels, rep_points['lon'], rep_points['lat']):\n",
    "    plt.annotate(\n",
    "        label, \n",
    "        xy = (x, y), xytext = (-25, -30),\n",
    "        textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'white', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = []\n",
    "def groupByTime(row):\n",
    "    t = df[(df['Origin Y']==row[0]) & (df['Origin X']==row[1])]['Time'].iloc[0]\n",
    "    return t[ (t[:t.index(':')].index(\" \")):][0:3]\n",
    "for i in range(num_clusters):\n",
    "    hours = np.apply_along_axis(groupByTime, 1, clusters[i]).tolist()\n",
    "    M.append(list(map(int, hours)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(len(M), figsize=(12.5,50))\n",
    "maxes = []\n",
    "for i in range(len(M)):\n",
    "    y, x, _ = axarr[i].hist(list(M[i]))\n",
    "    maxes.append(y.max())\n",
    "    axarr[i].set_title(\"Cluster {0}\".format(i + 1))\n",
    "    axarr[i].set_xlabel(\"Hour\")\n",
    "    axarr[i].set_ylabel(\"Trajectories\")\n",
    "f.tight_layout(pad=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmap = gmplot.GoogleMapPlotter(df[\"Origin Y\"][0], df[\"Origin X\"][0], 11)\n",
    "gmap.plot(df[\"Origin Y\"], df[\"Origin X\"], \"cornflowerblue\", edge_width=1)\n",
    "\n",
    "gmap.draw(\"trajectories_map.html\")\n",
    "\n",
    "print(\"Plotted trajectories.\")\n",
    "\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(df[\"Origin Y\"][0], df[\"Origin X\"][0], 11)\n",
    "gmap.plot(df[\"Origin Y\"], df[\"Origin X\"], \"cornflowerblue\", edge_width=1)\n",
    "gmap.heatmap(rep_points['lat'], rep_points['lon'], radius=20)\n",
    "\n",
    "gmap.draw(\"trajectories_map_with_clusters.html\")\n",
    "\n",
    "print(\"Plotted trajectories with clusters.\")\n",
    "\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(df[\"Origin Y\"][0], df[\"Origin X\"][0], 11)\n",
    "gmap.heatmap(rep_points['lat'], rep_points['lon'], radius=20)\n",
    "\n",
    "gmap.draw(\"map_with_clusters.html\")\n",
    "\n",
    "print(\"Plotted clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trajectories_condensed.csv\")\n",
    "\n",
    "map1 = KeplerGl(height=500, data={\"d1\":df})\n",
    "map1\n",
    "\n",
    "map1.save_to_html(file_name='map1.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
