{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global variables\n",
    "\n",
    "# Setting up the Coordinate Reference Systems up front in the necessary format.\n",
    "crs_degree = {'init': 'epsg:4326'} # CGS_WGS_1984 (what the GPS uses)\n",
    "\n",
    "# --- Paths\n",
    "\n",
    "# Root path of Fremont Dropbox\n",
    "import os\n",
    "import sys\n",
    "import demand_util\n",
    "\n",
    "# updates modules when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from fremontdropbox import get_dropbox_location\n",
    "# Root path of the Dropbox business account\n",
    "dbx = get_dropbox_location()\n",
    "\n",
    "# Temporary! Location of the folder where the restructuring is currently happening\n",
    "data_path = dbx + '/Private Structured data collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import geometry\n",
    "# array analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.cluster as skc\n",
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# geo spacial data analysis\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from keplergl import KeplerGl\n",
    "import fiona\n",
    "\n",
    "# assorted parsing and modeling tools\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "from pytz import utc\n",
    "from shutil import copyfile, copytree\n",
    "from shapely.ops import nearest_points, unary_union\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPoint\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import polyline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# importing all the Kepler.gl configurations\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfcta_folder = os.path.join(data_path, \"Data processing\", \"Raw\", \"Demand\", \"OD demand\", \"SFCTA demand data\")\n",
    "sections_path = os.path.join(data_path, \"Aimsun\", \"Inputs\", \"sections.shp\")\n",
    "sections_shp = gpd.GeoDataFrame.from_file(sections_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_delimitation = []\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))\n",
    "project_delimitation.append((-121.94099807399999, 37.554268507000074))\n",
    "project_delimitation.append((-121.91790942699998, 37.549823434000075))\n",
    "project_delimitation.append((-121.89348666299998, 37.52770136500004, ))\n",
    "project_delimitation.append((-121.90056572499998, 37.52292299800007))\n",
    "project_delimitation.append((-121.90817571699995, 37.52416183400004))\n",
    "project_delimitation.append((-121.91252749099999, 37.51845069500007))\n",
    "project_delimitation.append((-121.91349347899995, 37.513972023000065))\n",
    "project_delimitation.append((-121.90855417099999, 37.503837324000074))\n",
    "project_delimitation.append((-121.91358547299996, 37.50097863000008))\n",
    "project_delimitation.append((-121.90798018999999, 37.49080413200005))\n",
    "project_delimitation.append((-121.91894942199997, 37.48791568200005))\n",
    "project_delimitation.append((-121.92029048799998, 37.488706567000065))\n",
    "project_delimitation.append((-121.93070953799997, 37.48509600500006))\n",
    "project_delimitation.append((-121.93254686299997, 37.48864173700008))\n",
    "project_delimitation.append((-121.94079404499996, 37.50416395900004))\n",
    "project_delimitation.append((-121.94569804899999, 37.51332606200003))\n",
    "project_delimitation.append((-121.94918207899997, 37.520371545000046))\n",
    "project_delimitation.append((-121.95305006999996, 37.52804520800004))\n",
    "project_delimitation.append((-121.953966735, 37.53272020000003))\n",
    "project_delimitation.append((-121.95428756799998, 37.53817435800005))\n",
    "project_delimitation.append((-121.95506236799997, 37.54107322100003))\n",
    "project_delimitation.append((-121.95676186899999, 37.54656695700004))\n",
    "project_delimitation.append((-121.95529950799994, 37.54980786700003))\n",
    "project_delimitation.append((-121.95261192399994, 37.550479763000055))\n",
    "project_delimitation.append((-121.94988481799999, 37.55277211300006))\n",
    "project_delimitation.append((-121.94613010599994, 37.55466923100005))\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_by_here_api(start, end, stop_on_error=False):\n",
    "    \"\"\"\n",
    "    Using Here API to get smooth path from start to end location\n",
    "    where start and end are Point objects\n",
    "    \"\"\"\n",
    "    here_url = 'https://route.ls.hereapi.com/routing/7.2/calculateroute.json?'\n",
    "    api_key = 'PRIVATE'\n",
    "\n",
    "    # convention on .shp files and kepler are lng, lat\n",
    "    # Here API convention is lat, lng\n",
    "    start_pos = 'geo!{},{}'.format(start.y, start.x)\n",
    "    end_pos = 'geo!{},{}'.format(end.y, end.x)\n",
    "    params = {'apiKey': api_key,\n",
    "              'mode': 'fastest;car;traffic:disabled',\n",
    "              'representation': 'display',\n",
    "              'waypoint0': start_pos,\n",
    "              'waypoint1': end_pos}\n",
    "\n",
    "    response = requests.get(here_url, params=params)\n",
    "    if response.ok:\n",
    "        body = response.json()\n",
    "        route = body['response']['route']\n",
    "        if route:\n",
    "            points = []\n",
    "            points.append((start.x, start.y))\n",
    "            maneuver = route[0]['leg'][0]['maneuver']\n",
    "            for m in maneuver:\n",
    "                path = m['shape']\n",
    "                for p in path:\n",
    "                    lat_lng = p.split(',')\n",
    "                    lat = float(lat_lng[0])\n",
    "                    lng = float(lat_lng[1])\n",
    "                    # keep .shp file convention, lng, lat\n",
    "                    points.append((lng, lat))\n",
    "            points.append((end.x, end.y))\n",
    "            return points\n",
    "        else:\n",
    "            if stop_on_error:\n",
    "                stop_code('no routes found for start={}, destination={}'.format(start, end))\n",
    "            return None\n",
    "    else:\n",
    "        if stop_on_error:\n",
    "            response.raise_for_status()\n",
    "        # Here API throws an error on some bodies of water, keep going if stop on error is false\n",
    "        return None\n",
    "        # print('response={}'.format(response))\n",
    "        # print('start={}, destination={}'.format(start_pos, end_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_external_taz(dir_taz, sections_df, output_dir=None):\n",
    "    \"\"\"\n",
    "    3 Steps for Create external TAZs\n",
    "    1. Create a external demand delimitation:\n",
    "    - load SFCTA data as Geopandas point (one point = one origin or one destination)\n",
    "    - Get convex hull of the point\n",
    "    - Use the convex hull (+ buffer) as the external demand delimitation\n",
    "    2. create external centroids:\n",
    "    - select road with no fnode and capacity above 800 from sections_df\n",
    "    - create a point at the end of all selected road\n",
    "    - plot the points, get a list of points to remove visually\n",
    "    3. create external TAZs:\n",
    "    - create a mesh a points inside the external demand delimitation and outside the internal demand delimitation (project delimitation)\n",
    "    - use a Direction API (maybe Here direction):\n",
    "    for every mesh point:\n",
    "        Query path from mesh point to center of the project area\n",
    "        Find the closest external centroid to the path. Test that all paths are not to far from existing\n",
    "            external centroid --> if not, we might be missing one external centroid.\n",
    "        Associate the external centroid to the mesh point.\n",
    "        create external TAZ from mesh of points (if you reach point, Theo has already done it for internal TAZs)\n",
    "\n",
    "    @param dir_taz:         folder containing prefix_fremont_legs.csv where prefix=ending, internal and starting\n",
    "    @param sections_df:     geo pandas data frame of the aimsun sections\n",
    "    \"\"\"\n",
    "    # 1. Create a external demand delimitation:\n",
    "    # load the 3 csv files\n",
    "    ending_csv = pd.read_csv(os.path.join(dir_taz, \"ending_fremont_legs.csv\"))\n",
    "    internal_csv = pd.read_csv(os.path.join(dir_taz, \"internal_fremont_legs.csv\"))\n",
    "    starting_csv = pd.read_csv(os.path.join(dir_taz, \"starting_fremont_legs.csv\"))\n",
    "\n",
    "    # get the points from the csv's (start and end points)\n",
    "    def get_points(csv_df):\n",
    "        all_points = []\n",
    "        node_types = ['start', 'end']\n",
    "        for node_type in node_types:\n",
    "            points = list(zip(csv_df[node_type + '_node_lng'], csv_df[node_type + '_node_lat']))\n",
    "            all_points.extend(points)\n",
    "        return all_points\n",
    "\n",
    "    points = []\n",
    "    points.extend(get_points(ending_csv))\n",
    "    points.extend(get_points(internal_csv))\n",
    "    points.extend(get_points(starting_csv))\n",
    "    points = np.array(points)\n",
    "\n",
    "    # get convex hull of points\n",
    "    hull = ConvexHull(points)\n",
    "    hull_points = points[hull.vertices, :]\n",
    "\n",
    "    # add buffer to convex hull\n",
    "    def normalize(point):\n",
    "        norm = np.linalg.norm(point)\n",
    "        return point / norm if norm > 0 else point\n",
    "\n",
    "    # for each point calculate the direction to expand for buffer\n",
    "    buffer_directions = []\n",
    "    for i in range(len(hull_points)):\n",
    "        point = hull_points[i]\n",
    "        left_neighbor = hull_points[(i-1) % len(hull_points)]\n",
    "        right_neighbor = hull_points[(i+1) % len(hull_points)]\n",
    "        left_arrow = point - left_neighbor\n",
    "        right_arrow = point - right_neighbor\n",
    "        left_arrow = normalize(left_arrow)\n",
    "        right_arrow = normalize(right_arrow)\n",
    "        buffer_directions.append(normalize(left_arrow + right_arrow))\n",
    "    buffer_directions = np.array(buffer_directions)\n",
    "\n",
    "    # calculate the new (expanded) hull points with buffer\n",
    "    buffer_coefficient = .05\n",
    "    expanded_hull_points = hull_points + buffer_coefficient * buffer_directions\n",
    "\n",
    "    # 2. create external centroids:\n",
    "    # select roads with no fnode and capacity above 800 from sections_df\n",
    "    sections_df = sections_df[pd.isnull(sections_df['fnode']) & (sections_df['capacity'] > 800)]\n",
    "    sections_df = sections_df[['eid', 'geometry']]\n",
    "\n",
    "    # filter out roads that are visually erroneous -> a road not entering the project area (Fremont)\n",
    "    # sections_df.to_csv('selected_roads.csv')   # roads to obtained visually\n",
    "    roads_to_remove = [56744, 30676, 35572, 56534]\n",
    "    sections_df = sections_df.astype({'eid': 'int32'})\n",
    "    sections_df = sections_df[~sections_df['eid'].isin(roads_to_remove)]\n",
    "\n",
    "    # create external centroid nodes -> create a point at the terminal end of these roads\n",
    "    # that is, for each road find the end of the road that is closer to the external delimitation (convex hull)\n",
    "    external_centroid_nodes = []\n",
    "    internal_centroid_nodes = []  # need later to compute center point of project area\n",
    "    circle = np.concatenate((expanded_hull_points, expanded_hull_points[0][None, :]), axis=0)\n",
    "    external_delimitation = LineString(circle)\n",
    "    for road in sections_df['geometry']:\n",
    "        start_point = Point(road.coords[0])\n",
    "        end_point = Point(road.coords[-1])\n",
    "\n",
    "        if external_delimitation.distance(start_point) < external_delimitation.distance(end_point):\n",
    "            # start is external centroid\n",
    "            external_centroid_nodes.append(start_point)\n",
    "            internal_centroid_nodes.append(end_point)\n",
    "        else:\n",
    "            # end is external centroid\n",
    "            external_centroid_nodes.append(end_point)\n",
    "            internal_centroid_nodes.append(start_point)\n",
    "\n",
    "    # 3. create external TAZs:\n",
    "    # create mesh of points\n",
    "    mesh_density = 0.001  # should be 0.001 (creates 2 million points)\n",
    "    x_min, x_max = np.min(expanded_hull_points[:, 0]), np.max(expanded_hull_points[:, 0])\n",
    "    y_min, y_max = np.min(expanded_hull_points[:, 1]), np.max(expanded_hull_points[:, 1])\n",
    "    x, y = np.meshgrid(np.arange(x_min, x_max, mesh_density), np.arange(y_min, y_max, mesh_density))\n",
    "    x = x.reshape(x.shape[0] * x.shape[1])\n",
    "    y = y.reshape(y.shape[0] * y.shape[1])\n",
    "    mesh_points = list(zip(x, y))\n",
    "    print('created {} mesh points'.format(len(mesh_points)))\n",
    "    x = y = points = None  # free up memory\n",
    "\n",
    "    # keep those inside external delimitation and outside project delimitation\n",
    "    external_delimitation_poly = Polygon(expanded_hull_points)\n",
    "    project_delimitation_poly = Polygon(project_delimitation)\n",
    "    external_minus_project = external_delimitation_poly.difference(project_delimitation_poly)\n",
    "    # bottleneck (iterating over points and using contains method is slow)\n",
    "    mesh_points = list(filter(lambda p: external_minus_project.contains(p), MultiPoint(mesh_points)))\n",
    "    print('kept {} mesh points'.format(len(mesh_points)))\n",
    "\n",
    "    # compute center of project area\n",
    "    internal_centroid_nodes = np.array([(p.x, p.y) for p in internal_centroid_nodes])\n",
    "    project_center = np.mean(internal_centroid_nodes, axis=0)\n",
    "    project_center = Point(project_center[0], project_center[1])\n",
    "\n",
    "    # for each mesh point find closest external centroid to its query path\n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])\n",
    "\n",
    "    testing = True\n",
    "    sample_size = 500\n",
    "    info_point_to_center = []  # desired result\n",
    "    intersection_to_centroid_paths = []\n",
    "\n",
    "    # for testing sample mesh points at random and run them\n",
    "    if testing:\n",
    "        mesh_points = random.sample(mesh_points, sample_size)\n",
    "\n",
    "    distance_to_centroid_threshold = 0.005\n",
    "    for point in mesh_points:\n",
    "        path = get_path_by_here_api(point, project_center, stop_on_error=False)\n",
    "        if not path:\n",
    "            # not path found, ie. start is body of water hence no car path to destination\n",
    "            # from sample testing, Google API takes this into account but not Here API\n",
    "            continue  # next mesh point\n",
    "\n",
    "        # find intersection (point) of path and project delimitation\n",
    "        path = LineString(path)\n",
    "        intersect_point = project_delimitation_line.intersection(path)\n",
    "        if not isinstance(intersect_point, Point):\n",
    "            # usually API error, ie. start is body of water, path includes a segment that jumps from water\n",
    "            # to fremont intersecting project delimitation multiple times\n",
    "            continue  # next mesh point\n",
    "\n",
    "        # find closest centroid to intersection point\n",
    "        min_distance = 999999\n",
    "        closest_centroid = None\n",
    "        for centroid in external_centroid_nodes:\n",
    "            dist = intersect_point.distance(centroid)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                closest_centroid = centroid\n",
    "\n",
    "        if min_distance < distance_to_centroid_threshold:\n",
    "            # path intersection to centroid\n",
    "            intersection_to_centroid = [(intersect_point.x, intersect_point.y), (closest_centroid.x, closest_centroid.y)]\n",
    "            intersection_to_centroid_paths.append(LineString(intersection_to_centroid))\n",
    "\n",
    "            # write result to csv\n",
    "            info_point_to_center.append([point, project_center, closest_centroid, min_distance, path])\n",
    "\n",
    "    #if testing:\n",
    "        #kepler_map = KeplerGl(height=600)\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_center]}, crs='epsg:4326'), name='project_center')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': external_centroid_nodes}, crs='epsg:4326'), name='external_centroids')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_delimitation_line]}, crs='epsg:4326'), name='project_delimitation')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [external_delimitation]}, crs='epsg:4326'), name='external_delimitation')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': mesh_points}, crs='epsg:4326'), name='mesh_points')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [l[-1] for l in info_point_to_center]}, crs='epsg:4326'), name='paths')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': intersection_to_centroid_paths}, crs='epsg:4326'), name='intersection_to_centroid_paths')\n",
    "        #file_path = 'mesh_points_to_external_centroids.html'\n",
    "        #if output_dir:\n",
    "            #file_path = os.path.join(output_dir, file_path)\n",
    "        #kepler_map.save_to_html(file_name=file_path)\n",
    "\n",
    "    #create dataframe\n",
    "    \n",
    "\n",
    "    info_points_col = ['origin_mesh_point','destination','closest_external_centroid','distance_to_centroid','path']\n",
    "    info_points_df = pd.DataFrame(info_point_to_center, columns = info_points_col)\n",
    "    \n",
    "    def to_csv(file_name, header, lines):\n",
    "        def add_quotes(val):\n",
    "            return \"\\\"\" + str(val) + \"\\\"\" if ',' in str(val) else str(val)\n",
    "\n",
    "        csv = open(file_name, 'w')\n",
    "        csv.write(header + '\\n')\n",
    "        for line in lines:\n",
    "            csv.write(','.join(map(add_quotes, line)) + '\\n')\n",
    "\n",
    "    # write results to csv\n",
    "    mesh_points_to_centroid_file_path = 'mesh_point_to_centroid.csv'\n",
    "    if output_dir:\n",
    "        mesh_points_to_centroid_file_path = os.path.join(output_dir, mesh_points_to_centroid_file_path)\n",
    "    to_csv(mesh_points_to_centroid_file_path,\n",
    "           'origin_mesh_point,destination,closest_external_centroid,distance_to_centroid,path',\n",
    "           info_point_to_center)\n",
    "    return\n",
    "    #render    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_taz_from_csv(csv_file, output_dir=None):\n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])\n",
    "    \n",
    "    info_points_df = pd.read_csv(csv_file)\n",
    "    external_centroid_nodes = info_points_df['closest_external_centroid'].map(convert_point_to_coord).tolist()\n",
    "    external_centroid_nodes = [Point(coord[0], coord[1]) for coord in external_centroid_nodes]\n",
    "    \n",
    "    kepler_map = KeplerGl(height=600)\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_center]}, crs='epsg:4326'), name='project_center')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': external_centroid_nodes}, crs='epsg:4326'), name='external_centroids')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_delimitation_line]}, crs='epsg:4326'), name='project_delimitation')\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [external_delimitation]}, crs='epsg:4326'), name='external_delimitation')\n",
    "    \n",
    "    taz_id = 0\n",
    "    boundary_list = []\n",
    "    taz_name_list = []\n",
    "    external_centroids = info_points_df['closest_external_centroid'].unique()\n",
    "    for centroid in external_centroids:\n",
    "        taz_df = info_points_df.loc[info_points_df['closest_external_centroid']==centroid]\n",
    "        taz_points = np.array(taz_df['origin_mesh_point'].map(convert_point_to_coord).tolist())\n",
    "        taz_hull = ConvexHull(taz_points)\n",
    "        taz_boundary = taz_points[taz_hull.vertices, :]\n",
    "        taz_poly = Polygon(taz_boundary)\n",
    "        taz_name = 'External TAZ' + str(taz_id)\n",
    "        kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': taz_poly}, crs='epsg:4326',index=[0]), name=taz_name)\n",
    "        taz_name_list.append(taz_name)\n",
    "        boundary_list.append(taz_poly)\n",
    "        taz_id += 1\n",
    "    taz_gpd = gpd.GeoDataFrame({'taz_name':taz_name_list,'geometry':boundary_list})\n",
    "    file_path = 'external_taz.html'\n",
    "    if output_dir:\n",
    "        file_path = os.path.join(output_dir, file_path)\n",
    "    kepler_map.save_to_html(file_name=file_path)\n",
    "    return taz_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_point_to_coord(string):\n",
    "    lon, lat = string.split(' ')[1:]\n",
    "    lon = float(lon.split('(')[1])\n",
    "    lat = float(lat.split(')')[0])\n",
    "    return [lon, lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n",
      "Map saved to /Users/trevorwu/Dropbox/Private Structured data collection/Data processing/Kepler maps/HereAPI/external_taz.html!\n"
     ]
    }
   ],
   "source": [
    "# print(sections_shp.columns)\n",
    "# print(sections_shp.head)\n",
    "output_dir = os.path.join(data_path, 'Data processing', 'Kepler maps', 'HereAPI')\n",
    "mesh_points_to_centroid_file_path = 'mesh_point_to_centroid.csv'\n",
    "taz_gpd = render_taz_from_csv(os.path.join(output_dir, mesh_points_to_centroid_file_path), output_dir=output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
