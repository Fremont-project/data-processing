{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of Fremont Dropbox\n",
    "import os\n",
    "import sys\n",
    "import demand_util\n",
    "\n",
    "# array analysis\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import sklearn.cluster as skc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# geo spacial data analysis\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from rtree import index\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# assorted parsing and modeling tools\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import alphashape\n",
    "import multiprocessing \n",
    "from pytz import utc\n",
    "from pyproj import Proj, transform\n",
    "from shutil import copyfile, copytree\n",
    "from sklearn.neighbors import BallTree\n",
    "from matplotlib.path import Path as MPath\n",
    "from here_util import shortest_path_by_travel_time\n",
    "from shapely.ops import nearest_points, unary_union\n",
    "from shapely.geometry import box, Point, LineString, Polygon, MultiPoint, MultiPolygon, GeometryCollection\n",
    "\n",
    "# import polyline\n",
    "from pathlib import Path\n",
    "\n",
    "# importing all the Kepler.gl configurations\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates modules when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_delimitation = []\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))\n",
    "project_delimitation.append((-121.94099807399999, 37.554268507000074))\n",
    "project_delimitation.append((-121.91790942699998, 37.549823434000075))\n",
    "project_delimitation.append((-121.89348666299998, 37.52770136500004))\n",
    "project_delimitation.append((-121.90056572499998, 37.52292299800007))\n",
    "project_delimitation.append((-121.90817571699995, 37.52416183400004))\n",
    "project_delimitation.append((-121.91252749099999, 37.51845069500007))\n",
    "project_delimitation.append((-121.91349347899995, 37.513972023000065))\n",
    "project_delimitation.append((-121.90855417099999, 37.503837324000074))\n",
    "project_delimitation.append((-121.91358547299996, 37.50097863000008))\n",
    "project_delimitation.append((-121.90798018999999, 37.49080413200005))\n",
    "project_delimitation.append((-121.91894942199997, 37.48791568200005))\n",
    "project_delimitation.append((-121.92029048799998, 37.488706567000065))\n",
    "project_delimitation.append((-121.93070953799997, 37.48509600500006))\n",
    "project_delimitation.append((-121.93254686299997, 37.48864173700008))\n",
    "project_delimitation.append((-121.94079404499996, 37.50416395900004))\n",
    "project_delimitation.append((-121.94569804899999, 37.51332606200003))\n",
    "project_delimitation.append((-121.94918207899997, 37.520371545000046))\n",
    "project_delimitation.append((-121.95305006999996, 37.52804520800004))\n",
    "project_delimitation.append((-121.953966735, 37.53272020000003))\n",
    "project_delimitation.append((-121.95428756799998, 37.53817435800005))\n",
    "project_delimitation.append((-121.95506236799997, 37.54107322100003))\n",
    "project_delimitation.append((-121.95676186899999, 37.54656695700004))\n",
    "project_delimitation.append((-121.95529950799994, 37.54980786700003))\n",
    "project_delimitation.append((-121.95261192399994, 37.550479763000055))\n",
    "project_delimitation.append((-121.94988481799999, 37.55277211300006))\n",
    "project_delimitation.append((-121.94613010599994, 37.55466923100005))\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Coordinate Reference Systems up front in the necessary format.\n",
    "crs_degree = {'init': 'epsg:4326'} # CGS_WGS_1984 (what the GPS uses)\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from fremontdropbox import get_dropbox_location\n",
    "# Root path of the Dropbox business account\n",
    "dbx = get_dropbox_location()\n",
    "\n",
    "# Temporary! Location of the folder where the restructuring is currently happening\n",
    "data_path = dbx + '/Private Structured data collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theo\\AppData\\Roaming\\Python\\Python37\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "sfcta_folder = os.path.join(data_path, \"Data processing\", \"Raw\", \"Demand\", \"OD demand\", \"SFCTA demand data\")\n",
    "sections_path = os.path.join(data_path, \"Aimsun\", \"Inputs\", \"sections.shp\")\n",
    "sections_shp = gpd.GeoDataFrame.from_file(sections_path)\n",
    "sections_shp = sections_shp.to_crs(crs_degree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_csv(csv_df):\n",
    "    all_points = []\n",
    "    node_types = ['start', 'end']\n",
    "    for node_type in node_types:\n",
    "        points = list(zip(csv_df[node_type + '_node_lng'], csv_df[node_type + '_node_lat']))\n",
    "        all_points.extend(points)\n",
    "    return all_points\n",
    "\n",
    "def to_csv(file_name, header, lines):\n",
    "    def add_quotes(val):\n",
    "        return \"\\\"\" + str(val) + \"\\\"\" if ',' in str(val) else str(val)\n",
    "\n",
    "    csv = open(file_name, 'w')\n",
    "    csv.write(header + '\\n')\n",
    "    for line in lines:\n",
    "        csv.write(','.join(map(add_quotes, line)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_external_delim(dir_taz):\n",
    "    ''' \n",
    "    Create a external demand delimitation:\n",
    "    - load SFCTA data as Geopandas point (one point = one origin or one destination)\n",
    "    - Get convex hull of the point\n",
    "    - Use the convex hull (+ buffer) as the external demand delimitation\n",
    "    '''\n",
    "    # load the 3 csv files\n",
    "    ending_csv = pd.read_csv(os.path.join(dir_taz, \"ending_fremont_legs.csv\"))\n",
    "    internal_csv = pd.read_csv(os.path.join(dir_taz, \"internal_fremont_legs.csv\"))\n",
    "    starting_csv = pd.read_csv(os.path.join(dir_taz, \"starting_fremont_legs.csv\"))\n",
    "\n",
    "    # get the points from the csv's (start and end points)\n",
    "    points = []\n",
    "    points.extend(from_csv(ending_csv))\n",
    "    points.extend(from_csv(internal_csv))\n",
    "    points.extend(from_csv(starting_csv))\n",
    "    points = np.array(points)\n",
    "\n",
    "    # get convex hull of points\n",
    "    hull = ConvexHull(points)\n",
    "    hull_points = points[hull.vertices, :]\n",
    "\n",
    "    # add buffer to convex hull\n",
    "    def normalize(point):\n",
    "        norm = np.linalg.norm(point)\n",
    "        return point / norm if norm > 0 else point\n",
    "\n",
    "    # for each point calculate the direction to expand for buffer\n",
    "    buffer_directions = []\n",
    "    for i in range(len(hull_points)):\n",
    "        point = hull_points[i]\n",
    "        left_neighbor = hull_points[(i-1) % len(hull_points)]\n",
    "        right_neighbor = hull_points[(i+1) % len(hull_points)]\n",
    "        left_arrow = point - left_neighbor\n",
    "        right_arrow = point - right_neighbor\n",
    "        left_arrow = normalize(left_arrow)\n",
    "        right_arrow = normalize(right_arrow)\n",
    "        buffer_directions.append(normalize(left_arrow + right_arrow))\n",
    "    buffer_directions = np.array(buffer_directions)\n",
    "\n",
    "    # calculate the new (expanded) hull points with buffer\n",
    "    buffer_coefficient = .05\n",
    "    expanded_hull_points = hull_points + buffer_coefficient * buffer_directions\n",
    "    \n",
    "    return expanded_hull_points\n",
    "\n",
    "def create_external_centroids(sections_df, expanded_hull_points):\n",
    "    '''\n",
    "    Create external centroids:\n",
    "    - select road with no fnode and capacity above 800 from sections_df\n",
    "    - create a point at the end of all selected road\n",
    "    - plot the points, get a list of points to remove visually\n",
    "    '''\n",
    "    # select roads with no fnode and capacity above 800 from sections_df\n",
    "    sections_df = sections_df[pd.isnull(sections_df['fnode']) & (sections_df['capacity'] > 800)]\n",
    "    sections_df = sections_df[['eid', 'geometry']]\n",
    "\n",
    "    # filter out roads that are visually erroneous -> a road not entering the project area (Fremont)\n",
    "    # sections_df.to_csv('selected_roads.csv')   # roads to remove obtained visually\n",
    "    ######################################################################################\n",
    "    ########################## NEED AN AUTOMATED WAY TO DO THIS ##########################\n",
    "    ######################################################################################\n",
    "    roads_to_remove = [56744, 30676, 35572, 56534]\n",
    "    sections_df = sections_df.astype({'eid': 'int32'})\n",
    "    sections_df = sections_df[~sections_df['eid'].isin(roads_to_remove)]\n",
    "\n",
    "    # create external centroid nodes -> create a point at the terminal end of these roads\n",
    "    # that is, for each road find the end of the road that is closer to the external delimitation (convex hull)\n",
    "    external_centroid_nodes = []\n",
    "    internal_centroid_nodes = []  # need later to compute center point of project area\n",
    "    circle = np.concatenate((expanded_hull_points, expanded_hull_points[0][None, :]), axis=0)\n",
    "    external_delimitation = LineString(circle)\n",
    "    for road in sections_df['geometry']:\n",
    "        start_point = Point(road.coords[0])\n",
    "        end_point = Point(road.coords[-1])\n",
    "\n",
    "        if external_delimitation.distance(start_point) < external_delimitation.distance(end_point):\n",
    "            # start is external centroid\n",
    "            external_centroid_nodes.append(start_point)\n",
    "            internal_centroid_nodes.append(end_point)\n",
    "        else:\n",
    "            # end is external centroid\n",
    "            external_centroid_nodes.append(end_point)\n",
    "            internal_centroid_nodes.append(start_point)\n",
    "            \n",
    "    return external_centroid_nodes, internal_centroid_nodes\n",
    "\n",
    "def create_mesh(expanded_hull_points, project_delimitation, mesh_density=0.01, dense_portion=0.25, return_map=False):\n",
    "    ''' \n",
    "    Create mesh of points.\n",
    "    \n",
    "    Note: mesh_density 0.001 creates 2 million points\n",
    "    '''\n",
    "    external_delimitation_poly = Polygon(expanded_hull_points)\n",
    "    project_delimitation_poly = Polygon(project_delimitation)\n",
    "    external_minus_project = external_delimitation_poly.difference(project_delimitation_poly)\n",
    "    proj_center = project_delimitation_poly.centroid\n",
    "    \n",
    "    x_min, x_max = np.min(expanded_hull_points[:, 0]), np.max(expanded_hull_points[:, 0])\n",
    "    y_min, y_max = np.min(expanded_hull_points[:, 1]), np.max(expanded_hull_points[:, 1])\n",
    "    x, y = np.meshgrid(np.arange(x_min, x_max, mesh_density), np.arange(y_min, y_max, mesh_density))\n",
    "    points = np.vstack((x.ravel(), y.ravel())).T\n",
    "    \n",
    "    cx_min, cx_max = proj_center.x - ((x_max - x_min) * dense_portion), proj_center.x + ((x_max - x_min) * dense_portion)\n",
    "    cy_min, cy_max = proj_center.y - ((y_max - y_min) * dense_portion), proj_center.y + ((y_max - y_min) * dense_portion)\n",
    "    cx, cy = np.meshgrid(np.arange(cx_min, cx_max, mesh_density / 4), np.arange(cy_min, cy_max, mesh_density / 4))\n",
    "    cpoints = np.vstack((cx.ravel(), cy.ravel())).T\n",
    "    \n",
    "    points = np.unique(np.concatenate([points, cpoints], axis=0), axis=0)\n",
    "    \n",
    "    ext_bounds = np.asarray(list(zip(*external_minus_project.exterior.coords.xy)))\n",
    "    int_bounds = np.asarray(list(zip(*external_minus_project.interiors[0].coords.xy)))\n",
    "    ext_path, int_path = MPath(ext_bounds), MPath(int_bounds)\n",
    "    \n",
    "    ext_mask = ext_path.contains_points(points)\n",
    "    int_mask = np.invert(int_path.contains_points(points))\n",
    "    mask = np.bitwise_and(ext_mask, int_mask)\n",
    "    \n",
    "    mesh_points = points[mask]\n",
    "    \n",
    "    if return_map:\n",
    "        mesh_map = KeplerGl(height=600)\n",
    "        mesh_map.add_data(data=gpd.GeoDataFrame(geometry=gpd.points_from_xy(mesh_points[:, 0], mesh_points[:, 1])))\n",
    "        return mesh_map\n",
    "                          \n",
    "    return mesh_points\n",
    "\n",
    "def clean_ext_taz(external_taz, project_delimitation):\n",
    "    '''\n",
    "    Clean and process generated External TAZ polygons.\n",
    "    '''\n",
    "    for idx, hull in external_taz.iterrows():\n",
    "        if type(hull['geometry']) != Polygon:\n",
    "            bounds = Polygon(max(hull['geometry'], key=lambda a: a.area).exterior)\n",
    "            secondary = min(hull['geometry'], key=lambda a: a.area)\n",
    "            if secondary.area > 0.001:\n",
    "                nearest_idx = min([i for i in range(len(external_taz.index)) if i != idx], \\\n",
    "                                   key=lambda i: external_taz.loc[i, 'geometry'].distance(secondary))\n",
    "                nearest = external_taz.loc[nearest_idx, 'geometry']\n",
    "                external_taz.loc[nearest_idx, 'geometry'] = nearest.union(secondary)\n",
    "        else:\n",
    "            bounds = hull['geometry']\n",
    "        external_taz.loc[idx, 'geometry'] = bounds\n",
    "\n",
    "    for i in range(len(external_taz.index)):\n",
    "        for j in range(len(external_taz.index)):\n",
    "            hull_i, hull_j = external_taz.loc[i, 'geometry'], external_taz.loc[j, 'geometry']\n",
    "            if hull_i.intersects(hull_j):\n",
    "                hull_diff = hull_i.intersection(hull_j)\n",
    "                max_hull = max([hull_i, hull_j], key=lambda a: a.area)\n",
    "\n",
    "                if max_hull == hull_i:\n",
    "                    external_taz.loc[i, 'geometry'] = hull_i.difference(hull_diff)\n",
    "                    external_taz.loc[j, 'geometry'] = hull_j.union(hull_diff)\n",
    "                else:\n",
    "                    external_taz.loc[i, 'geometry'] = hull_i.union(hull_diff)\n",
    "                    external_taz.loc[j, 'geometry'] = hull_j.difference(hull_diff)     \n",
    "\n",
    "    for idx, hull in external_taz.iterrows():\n",
    "        if type(hull['geometry']) != Polygon:\n",
    "            bounds = Polygon(max(hull['geometry'], key=lambda a: a.area).exterior)\n",
    "            secondary = min(hull['geometry'], key=lambda a: a.area)\n",
    "            nearest_idx = min([i for i in range(len(external_taz.index)) if i != idx], \\\n",
    "                               key=lambda i: external_taz.loc[i, 'geometry'].distance(secondary))\n",
    "            nearest = external_taz.loc[nearest_idx, 'geometry']\n",
    "            external_taz.loc[nearest_idx, 'geometry'] = nearest.union(secondary)\n",
    "        else:\n",
    "            bounds = hull['geometry']\n",
    "        external_taz.loc[idx, 'geometry'] = bounds\n",
    "\n",
    "    for idx, hull in external_taz.iterrows():\n",
    "        external_taz.loc[idx, 'geometry'] = external_taz.loc[idx, 'geometry'].difference(Polygon(project_delimitation))\n",
    "    \n",
    "    return external_taz\n",
    "\n",
    "def create_external_taz(dir_taz, sections_df, project_delimitation,\\\n",
    "                        mesh_density=0.01, output_dir=None, return_with_map=False):\n",
    "    \"\"\"\n",
    "    3 Steps for Create external TAZs\n",
    "    1. Create a external demand delimitation:\n",
    "    - load SFCTA data as Geopandas point (one point = one origin or one destination)\n",
    "    - Get convex hull of the point\n",
    "    - Use the convex hull (+ buffer) as the external demand delimitation\n",
    "    2. create external centroids:\n",
    "    - select road with no fnode and capacity above 800 from sections_df\n",
    "    - create a point at the end of all selected road\n",
    "    - plot the points, get a list of points to remove visually\n",
    "    3. create external TAZs:\n",
    "    - create a mesh a points inside the external demand delimitation and outside the internal demand delimitation (project delimitation)\n",
    "    - use a Direction API (maybe Here direction):\n",
    "    for every mesh point:\n",
    "        Query path from mesh point to center of the project area\n",
    "        Find the closest external centroid to the path. Test that all paths are not to far from existing\n",
    "            external centroid --> if not, we might be missing one external centroid.\n",
    "        Associate the external centroid to the mesh point.\n",
    "        create external TAZ from mesh of points (if you reach point, Theo has already done it for internal TAZs)\n",
    "\n",
    "    @param dir_taz:         folder containing prefix_fremont_legs.csv where prefix=ending, internal and starting\n",
    "    @param sections_df:     geo pandas data frame of the aimsun sections\n",
    "    \"\"\"\n",
    "    # 1. Create a external demand delimitation:\n",
    "    expanded_hull_points = create_external_delim(dir_taz)\n",
    "    # 2. create external centroids:\n",
    "    external_centroid_nodes, internal_centroid_nodes = create_external_centroids(sections_df, expanded_hull_points)\n",
    "    # 3. create external TAZs:\n",
    "    \n",
    "    #create mesh\n",
    "    mesh_points = create_mesh(expanded_hull_points, project_delimitation, mesh_density=mesh_density, dense_portion=0.1,\\\n",
    "                              return_map=False)\n",
    "    \n",
    "    # create balltree from mesh points\n",
    "    mesh_df = pd.DataFrame()\n",
    "    mesh_df[\"lon\"], mesh_df[\"lat\"] = mesh_points[:,0], mesh_points[:,1]\n",
    "    mesh_df['class'] = -1  \n",
    "    mesh_size = len(mesh_df.index)\n",
    "    mesh_bt = BallTree(np.deg2rad(mesh_df[['lat', 'lon']].values), metric='haversine')\n",
    "    \n",
    "    # compute center of project area\n",
    "    internal_centroid_nodes = np.array([(p.x, p.y) for p in internal_centroid_nodes])\n",
    "    project_center = np.mean(internal_centroid_nodes, axis=0)\n",
    "    project_center = Point(project_center[0], project_center[1])\n",
    "\n",
    "    # for each mesh point find closest external centroid to its query path (parallelized classification)\n",
    "    # BOTTLENECK. COMPLETION TIME INVERSELY RELATED TO NUM_CORES ON MACHINE.\n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])   \n",
    "    \n",
    "    print(\"Starting classification, {} points in total...\".format(mesh_size))\n",
    " \n",
    "    pool = multiprocessing.Pool() \n",
    "\n",
    "    start = time.time()\n",
    "    result_async = [(row, pool.apply_async(shortest_path_by_travel_time,\\\n",
    "                                     args = (Point(row['lon'], row['lat']), project_center)))\\\n",
    "                    for index, row in mesh_df.iterrows()] \n",
    "    paths = [(row, res.get()) for row, res in result_async] \n",
    "    print(\"Computed paths from points, took {} in total.\".format(time.time() - start))\n",
    "    \n",
    "    pool.close()\n",
    "    \n",
    "    del pool, result_async\n",
    "    \n",
    "    for row, path in paths:\n",
    "        if not path:\n",
    "            query_point = np.deg2rad(np.asarray([[row['lat'], row['lon']]]))\n",
    "            distances, indices = mesh_bt.query(query_point, k=1)\n",
    "            if mesh_df.loc[list(indices[0])[0]]['class'] != -1:\n",
    "                mesh_df.loc[(mesh_df['lon'] == row['lon']) & (mesh_df['lat'] == row['lat']),\\\n",
    "                            ['class']] = mesh_df.loc[list(indices[0])[0]]['class']\n",
    "                continue\n",
    "            else:\n",
    "                path = LineString([Point(row['lon'], row['lat']), project_center])\n",
    "                \n",
    "        intersect_point = project_delimitation_line.intersection(path)\n",
    "\n",
    "        # find closest centroid to intersection point\n",
    "        min_distance, closest_centroid, best_classification = float('inf'), None, None\n",
    "        for idx, centroid in enumerate(external_centroid_nodes):\n",
    "            dist = intersect_point.distance(centroid)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                closest_centroid = centroid\n",
    "                best_classification = idx\n",
    "        \n",
    "        mesh_df.loc[(mesh_df['lon'] == row['lon']) & (mesh_df['lat'] == row['lat']), ['class']] = best_classification\n",
    "    \n",
    "    # create TAZs from classified mesh\n",
    "    mesh_squares = gpd.GeoDataFrame(data=mesh_df, geometry=gpd.points_from_xy(mesh_df.lon, mesh_df.lat))\n",
    "    mesh_squares.geometry = mesh_squares.buffer(mesh_density/1.99).envelope\n",
    "    external_taz = mesh_squares[['class', 'geometry']].dissolve(by='class')\n",
    "    external_taz = clean_ext_taz(external_taz, project_delimitation)\n",
    "    \n",
    "    if return_with_map:\n",
    "        ext_gdf = gpd.GeoDataFrame(geometry=external_centroid_nodes)\n",
    "        int_gdf = gpd.GeoDataFrame(geometry=internal_centroid_nodes)\n",
    "        mesh_data = gpd.GeoDataFrame(data=mesh_df, \\\n",
    "                                     geometry=gpd.points_from_xy(mesh_df.lon, mesh_df.lat))[['class', 'geometry']]\n",
    "        \n",
    "        ext_taz_map = KeplerGl(height=1000)\n",
    "        ext_taz_map.add_data(data=external_taz, name=\"External TAZs\")\n",
    "        ext_taz_map.add_data(data=ext_gdf, name = \"External Centroids\")\n",
    "        ext_taz_map.add_data(data=int_gdf, name = \"Internal Centroids\")\n",
    "        ext_taz_map.add_data(data=mesh_data, name = \"Classification Set\")\n",
    "        \n",
    "        return external_taz, ext_taz_map\n",
    "    \n",
    "    return external_taz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfcta_folder = os.path.join(data_path, \"Data processing\", \"Raw\", \"Demand\", \"OD demand\", \"SFCTA demand data\")\n",
    "sections_path = os.path.join(data_path, \"Aimsun\", \"Inputs\", \"sections.shp\")\n",
    "\n",
    "sections_shp = gpd.GeoDataFrame.from_file(sections_path)\n",
    "sections_shp = sections_shp.to_crs(crs_degree) \n",
    "# print(sections_shp.columns)\n",
    "# print(sections_shp.head)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    external_taz, ext_taz_map = create_external_taz(sfcta_folder, sections_shp, project_delimitation, return_with_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_taz.to_file(\"external_taz.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_point_to_coord(string):\n",
    "    lon, lat = string.split(' ')[1:]\n",
    "    lon = float(lon.split('(')[1])\n",
    "    lat = float(lat.split(')')[0])\n",
    "    return [lon, lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_taz_from_csv(csv_file, output_dir=None):\n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])\n",
    "    \n",
    "    info_points_df = pd.read_csv(csv_file)\n",
    "    external_centroid_nodes = info_points_df['closest_external_centroid'].map(convert_point_to_coord).tolist()\n",
    "    external_centroid_nodes = [Point(coord[0], coord[1]) for coord in external_centroid_nodes]\n",
    "    \n",
    "    kepler_map = KeplerGl(height=600)\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_center]}, crs='epsg:4326'), name='project_center')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': external_centroid_nodes}, crs='epsg:4326'), name='external_centroids')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_delimitation_line]}, crs='epsg:4326'), name='project_delimitation')\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [external_delimitation]}, crs='epsg:4326'), name='external_delimitation')\n",
    "    \n",
    "    taz_id = 0\n",
    "    boundary_list = []\n",
    "    taz_name_list = []\n",
    "    external_centroids = info_points_df['closest_external_centroid'].unique()\n",
    "    for centroid in external_centroids:\n",
    "        taz_df = info_points_df.loc[info_points_df['closest_external_centroid']==centroid]\n",
    "        taz_points = np.array(taz_df['origin_mesh_point'].map(convert_point_to_coord).tolist())\n",
    "        taz_hull = ConvexHull(taz_points)\n",
    "        taz_boundary = taz_points[taz_hull.vertices, :]\n",
    "        taz_poly = Polygon(taz_boundary)\n",
    "        taz_name = 'External TAZ' + str(taz_id)\n",
    "        kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': taz_poly}, crs='epsg:4326',index=[0]), name=taz_name)\n",
    "        taz_name_list.append(taz_name)\n",
    "        boundary_list.append(taz_poly)\n",
    "        taz_id += 1\n",
    "    taz_gpd = gpd.GeoDataFrame({'taz_name':taz_name_list,'geometry':boundary_list})\n",
    "    file_path = 'external_taz.html'\n",
    "    if output_dir:\n",
    "        file_path = os.path.join(output_dir, file_path)\n",
    "    kepler_map.save_to_html(file_name=file_path)\n",
    "    return taz_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sections_shp.columns)\n",
    "# print(sections_shp.head)\n",
    "output_dir = os.path.join(data_path, 'Data processing', 'Kepler maps', 'HereAPI')\n",
    "mesh_points_to_centroid_file_path = 'mesh_point_to_centroid.csv'\n",
    "taz_gpd = render_taz_from_csv(os.path.join(output_dir, mesh_points_to_centroid_file_path), output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "1. Split the function create_external_taz into several subfunction that needs to be run sequentially\n",
    "    1. Create a external demand delimitation:\n",
    "    2. Create external centroids:\n",
    "    3. Create external TAZs:\n",
    "        1. Create a mesh grid\n",
    "        2. Associate every mesh grid to an external centroid\n",
    "        3. From the mesh grid create the external TAZ (Theo can do it)\n",
    "2. For every sub-function write some code to check the function (or render the output of the function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
