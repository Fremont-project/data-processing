{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root path of Fremont Dropbox\n",
    "import os\n",
    "import sys\n",
    "import demand_util\n",
    "\n",
    "# array analysis\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import sklearn.cluster as skc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# geo spacial data analysis\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from rtree import index\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# assorted parsing and modeling tools\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "from pytz import utc\n",
    "from osrm_util import *\n",
    "from shutil import copyfile, copytree\n",
    "from shapely.ops import nearest_points, unary_union\n",
    "from shapely.geometry import box, Point, LineString, Polygon, MultiPoint, MultiPolygon, GeometryCollection\n",
    "\n",
    "# import polyline\n",
    "from pathlib import Path\n",
    "\n",
    "# importing all the Kepler.gl configurations\n",
    "import ast\n",
    "\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates modules when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_delimitation = []\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))\n",
    "project_delimitation.append((-121.94099807399999, 37.554268507000074))\n",
    "project_delimitation.append((-121.91790942699998, 37.549823434000075))\n",
    "project_delimitation.append((-121.89348666299998, 37.52770136500004, ))\n",
    "project_delimitation.append((-121.90056572499998, 37.52292299800007))\n",
    "project_delimitation.append((-121.90817571699995, 37.52416183400004))\n",
    "project_delimitation.append((-121.91252749099999, 37.51845069500007))\n",
    "project_delimitation.append((-121.91349347899995, 37.513972023000065))\n",
    "project_delimitation.append((-121.90855417099999, 37.503837324000074))\n",
    "project_delimitation.append((-121.91358547299996, 37.50097863000008))\n",
    "project_delimitation.append((-121.90798018999999, 37.49080413200005))\n",
    "project_delimitation.append((-121.91894942199997, 37.48791568200005))\n",
    "project_delimitation.append((-121.92029048799998, 37.488706567000065))\n",
    "project_delimitation.append((-121.93070953799997, 37.48509600500006))\n",
    "project_delimitation.append((-121.93254686299997, 37.48864173700008))\n",
    "project_delimitation.append((-121.94079404499996, 37.50416395900004))\n",
    "project_delimitation.append((-121.94569804899999, 37.51332606200003))\n",
    "project_delimitation.append((-121.94918207899997, 37.520371545000046))\n",
    "project_delimitation.append((-121.95305006999996, 37.52804520800004))\n",
    "project_delimitation.append((-121.953966735, 37.53272020000003))\n",
    "project_delimitation.append((-121.95428756799998, 37.53817435800005))\n",
    "project_delimitation.append((-121.95506236799997, 37.54107322100003))\n",
    "project_delimitation.append((-121.95676186899999, 37.54656695700004))\n",
    "project_delimitation.append((-121.95529950799994, 37.54980786700003))\n",
    "project_delimitation.append((-121.95261192399994, 37.550479763000055))\n",
    "project_delimitation.append((-121.94988481799999, 37.55277211300006))\n",
    "project_delimitation.append((-121.94613010599994, 37.55466923100005))\n",
    "project_delimitation.append((-121.94277062699996, 37.55273259000006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Coordinate Reference Systems up front in the necessary format.\n",
    "crs_degree = {'init': 'epsg:4326'} # CGS_WGS_1984 (what the GPS uses)\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from fremontdropbox import get_dropbox_location\n",
    "# Root path of the Dropbox business account\n",
    "dbx = get_dropbox_location()\n",
    "\n",
    "# Temporary! Location of the folder where the restructuring is currently happening\n",
    "data_path = dbx + '/Private Structured data collection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jainc\\AppData\\Roaming\\Python\\Python37\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "sfcta_folder = os.path.join(data_path, \"Data processing\", \"Raw\", \"Demand\", \"OD demand\", \"SFCTA demand data\")\n",
    "sections_path = os.path.join(data_path, \"Aimsun\", \"Inputs\", \"sections.shp\")\n",
    "sections_shp = gpd.GeoDataFrame.from_file(sections_path)\n",
    "sections_shp = sections_shp.to_crs(crs_degree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_csv(csv_df):\n",
    "    all_points = []\n",
    "    node_types = ['start', 'end']\n",
    "    for node_type in node_types:\n",
    "        points = list(zip(csv_df[node_type + '_node_lng'], csv_df[node_type + '_node_lat']))\n",
    "        all_points.extend(points)\n",
    "    return all_points\n",
    "\n",
    "def to_csv(file_name, header, lines):\n",
    "    def add_quotes(val):\n",
    "        return \"\\\"\" + str(val) + \"\\\"\" if ',' in str(val) else str(val)\n",
    "\n",
    "    csv = open(file_name, 'w')\n",
    "    csv.write(header + '\\n')\n",
    "    for line in lines:\n",
    "        csv.write(','.join(map(add_quotes, line)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multisplit_polygon(geometry, threshold=0.005, count=0):\n",
    "    \"\"\"\n",
    "    Split a Polygon repeatedly across it's shortest dimension\n",
    "    Copyright (c) 2016, Joshua Arnott\n",
    "    Source: https://snorfalorpagus.net/blog/2016/03/13/splitting-large-polygons-for-faster-intersections/\n",
    "    \"\"\"\n",
    "    bounds = geometry.bounds\n",
    "    width = bounds[2] - bounds[0]\n",
    "    height = bounds[3] - bounds[1]\n",
    "    if max(width, height) <= threshold or count == 250:\n",
    "        # either the polygon is smaller than the threshold, or the maximum\n",
    "        # number of recursions has been reached\n",
    "        return [geometry]\n",
    "    if height >= width:\n",
    "        # split left to right\n",
    "        a = box(bounds[0], bounds[1], bounds[2], bounds[1]+height/2)\n",
    "        b = box(bounds[0], bounds[1]+height/2, bounds[2], bounds[3])\n",
    "    else:\n",
    "        # split top to bottom\n",
    "        a = box(bounds[0], bounds[1], bounds[0]+width/2, bounds[3])\n",
    "        b = box(bounds[0]+width/2, bounds[1], bounds[2], bounds[3])\n",
    "    result = []\n",
    "    for d in (a, b,):\n",
    "        c = geometry.intersection(d)\n",
    "        if not isinstance(c, GeometryCollection):\n",
    "            c = [c]\n",
    "        for e in c:\n",
    "            if isinstance(e, (Polygon, MultiPolygon)):\n",
    "                result.extend(multisplit_polygon(e, threshold, count+1))\n",
    "    if count > 0:\n",
    "        return result\n",
    "    # convert multipart into singlepart\n",
    "    final_result = []\n",
    "    for g in result:\n",
    "        if isinstance(g, MultiPolygon):\n",
    "            final_result.extend(g)\n",
    "        else:\n",
    "            final_result.append(g)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = multisplit_polygon(Polygon(project_delimitation), 0.005)\n",
    "# split_gdf = gpd.GeoDataFrame(geometry=split)\n",
    "# one_gdf = gpd.GeoDataFrame(geometry=[Polygon(project_delimitation)])\n",
    "# map_2 = KeplerGl(height=1000)\n",
    "# map_2.add_data(data=split_gdf, name = \"Split GDF\")\n",
    "# map_2.add_data(data=one_gdf, name = \"One GDF\")\n",
    "# map_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "def shortest_path_by_travel_time(start, end):\n",
    "    \"\"\"\n",
    "    Using OSM to get shortest path (weighted by free-flow travel time) from start to end location\n",
    "    where start and end are Point objects, and graph is the NetworkX graph of the region\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_result = simple_route(\n",
    "                                  [start.x, start.y], [end.x, end.y],\n",
    "                                  output='route', overview=\"full\", geometry='wkt')\n",
    "        shortest_path = wkt.loads(query_result[0]['geometry'])\n",
    "        \n",
    "        #print('yay')\n",
    "        \n",
    "        return shortest_path\n",
    "    \n",
    "    except JSONDecodeError as err:\n",
    "        \n",
    "        #print('darn')\n",
    "        #print(err)\n",
    "        #print([start.x, start.y], [end.x, end.y])\n",
    "        \n",
    "        return shortest_path_by_travel_time(start, end)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_external_delim(dir_taz):\n",
    "    ''' \n",
    "    Create a external demand delimitation:\n",
    "    - load SFCTA data as Geopandas point (one point = one origin or one destination)\n",
    "    - Get convex hull of the point\n",
    "    - Use the convex hull (+ buffer) as the external demand delimitation\n",
    "    '''\n",
    "    # load the 3 csv files\n",
    "    ending_csv = pd.read_csv(os.path.join(dir_taz, \"ending_fremont_legs.csv\"))\n",
    "    internal_csv = pd.read_csv(os.path.join(dir_taz, \"internal_fremont_legs.csv\"))\n",
    "    starting_csv = pd.read_csv(os.path.join(dir_taz, \"starting_fremont_legs.csv\"))\n",
    "\n",
    "    # get the points from the csv's (start and end points)\n",
    "    points = []\n",
    "    points.extend(from_csv(ending_csv))\n",
    "    points.extend(from_csv(internal_csv))\n",
    "    points.extend(from_csv(starting_csv))\n",
    "    points = np.array(points)\n",
    "\n",
    "    # get convex hull of points\n",
    "    hull = ConvexHull(points)\n",
    "    hull_points = points[hull.vertices, :]\n",
    "\n",
    "    # add buffer to convex hull\n",
    "    def normalize(point):\n",
    "        norm = np.linalg.norm(point)\n",
    "        return point / norm if norm > 0 else point\n",
    "\n",
    "    # for each point calculate the direction to expand for buffer\n",
    "    buffer_directions = []\n",
    "    for i in range(len(hull_points)):\n",
    "        point = hull_points[i]\n",
    "        left_neighbor = hull_points[(i-1) % len(hull_points)]\n",
    "        right_neighbor = hull_points[(i+1) % len(hull_points)]\n",
    "        left_arrow = point - left_neighbor\n",
    "        right_arrow = point - right_neighbor\n",
    "        left_arrow = normalize(left_arrow)\n",
    "        right_arrow = normalize(right_arrow)\n",
    "        buffer_directions.append(normalize(left_arrow + right_arrow))\n",
    "    buffer_directions = np.array(buffer_directions)\n",
    "\n",
    "    # calculate the new (expanded) hull points with buffer\n",
    "    buffer_coefficient = .05\n",
    "    expanded_hull_points = hull_points + buffer_coefficient * buffer_directions\n",
    "    \n",
    "    return expanded_hull_points\n",
    "\n",
    "def create_external_centroids(sections_df, expanded_hull_points):\n",
    "    '''\n",
    "    Create external centroids:\n",
    "    - select road with no fnode and capacity above 800 from sections_df\n",
    "    - create a point at the end of all selected road\n",
    "    - plot the points, get a list of points to remove visually\n",
    "    '''\n",
    "    # select roads with no fnode and capacity above 800 from sections_df\n",
    "    sections_df = sections_df[pd.isnull(sections_df['fnode']) & (sections_df['capacity'] > 800)]\n",
    "    sections_df = sections_df[['eid', 'geometry']]\n",
    "\n",
    "    # filter out roads that are visually erroneous -> a road not entering the project area (Fremont)\n",
    "    # sections_df.to_csv('selected_roads.csv')   # roads to remove obtained visually\n",
    "    ######################################################################################\n",
    "    ########################## NEED AN AUTOMATED WAY TO DO THIS ##########################\n",
    "    ######################################################################################\n",
    "    roads_to_remove = [56744, 30676, 35572, 56534]\n",
    "    sections_df = sections_df.astype({'eid': 'int32'})\n",
    "    sections_df = sections_df[~sections_df['eid'].isin(roads_to_remove)]\n",
    "\n",
    "    # create external centroid nodes -> create a point at the terminal end of these roads\n",
    "    # that is, for each road find the end of the road that is closer to the external delimitation (convex hull)\n",
    "    external_centroid_nodes = []\n",
    "    internal_centroid_nodes = []  # need later to compute center point of project area\n",
    "    circle = np.concatenate((expanded_hull_points, expanded_hull_points[0][None, :]), axis=0)\n",
    "    external_delimitation = LineString(circle)\n",
    "    for road in sections_df['geometry']:\n",
    "        start_point = Point(road.coords[0])\n",
    "        end_point = Point(road.coords[-1])\n",
    "\n",
    "        if external_delimitation.distance(start_point) < external_delimitation.distance(end_point):\n",
    "            # start is external centroid\n",
    "            external_centroid_nodes.append(start_point)\n",
    "            internal_centroid_nodes.append(end_point)\n",
    "        else:\n",
    "            # end is external centroid\n",
    "            external_centroid_nodes.append(end_point)\n",
    "            internal_centroid_nodes.append(start_point)\n",
    "            \n",
    "    return external_centroid_nodes, internal_centroid_nodes\n",
    "\n",
    "def create_mesh(expanded_hull_points, project_delimitation, mesh_density=0.001, testing=False, sample_size=500):\n",
    "    ''' \n",
    "    Create mesh of points.\n",
    "    \n",
    "    Note: mesh_density 0.001 creates 2 million points\n",
    "    '''\n",
    "    x_min, x_max = np.min(expanded_hull_points[:, 0]), np.max(expanded_hull_points[:, 0])\n",
    "    y_min, y_max = np.min(expanded_hull_points[:, 1]), np.max(expanded_hull_points[:, 1])\n",
    "    x, y = np.meshgrid(np.arange(x_min, x_max, mesh_density), np.arange(y_min, y_max, mesh_density))\n",
    "    x = x.reshape(x.shape[0] * x.shape[1])\n",
    "    y = y.reshape(y.shape[0] * y.shape[1])\n",
    "    #mesh_points = list(zip(x, y))\n",
    "    mesh_points = pd.DataFrame(list(zip(x, y)), columns=['x', 'y']) \n",
    "    \n",
    "    print('created {} mesh points'.format(len(mesh_points)))\n",
    "    del x, y  # free up memory\n",
    "    \n",
    "    # for testing sample mesh points at random and run them\n",
    "    if testing:\n",
    "        mesh_points = random.sample(mesh_points, sample_size)\n",
    "        return mesh_points\n",
    "    \n",
    "    print('creating bounds')\n",
    "    # keep those inside external delimitation and outside project delimitation\n",
    "    external_delimitation_poly = Polygon(expanded_hull_points)\n",
    "    project_delimitation_poly = Polygon(project_delimitation)\n",
    "    external_minus_project = external_delimitation_poly.difference(project_delimitation_poly)\n",
    "    # bottleneck (iterating over points and using contains method is slow)\n",
    "    '''     \n",
    "    def mesh_filter(idx, point, polygons):\n",
    "        for i in idx.intersection((point.coords[0])):\n",
    "            if point.intersects(polygons[i]):\n",
    "                return True\n",
    "        return False\n",
    "            \n",
    "    ext_min_proj_polygons = multisplit_polygon(external_minus_project, 0.005)\n",
    "    idx = index.Index()\n",
    "    for pos, poly in enumerate(ext_min_proj_polygons):\n",
    "        idx.insert(pos, poly.bounds)\n",
    "        \n",
    "    mesh_points = list(filter(lambda p: mesh_filter(idx, p, ext_min_proj_polygons), MultiPoint(mesh_points)))\n",
    "    '''  \n",
    "\n",
    "    print('pre-filtering (polygon split)')\n",
    "    ext_min_proj_polygons = multisplit_polygon(external_minus_project, 0.3)\n",
    "    print('pre-filtering (polygon gdf)', len(ext_min_proj_polygons))\n",
    "    ext_min_proj_polygons_gdf = gpd.GeoDataFrame(geometry=ext_min_proj_polygons)\n",
    "    print('pre-filtering (mesh points gdf)')\n",
    "    mesh_points_gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(mesh_points.x, mesh_points.y))\n",
    "    print('filtering p1')\n",
    "    sjoin_gdf = gpd.sjoin(mesh_points_gdf, ext_min_proj_polygons_gdf, how='left', op='intersects')\n",
    "    print('filtering p2')\n",
    "    mesh_points = list(sjoin_gdf[sjoin_gdf['index_right'].notnull()]['geometry'].values)\n",
    "    \n",
    "    print('kept {} mesh points'.format(len(mesh_points)))\n",
    "    \n",
    "    return mesh_points\n",
    "\n",
    "\n",
    "def create_external_taz(dir_taz, sections_df, project_delimitation, output_dir=None):\n",
    "    \"\"\"\n",
    "    3 Steps for Create external TAZs\n",
    "    1. Create a external demand delimitation:\n",
    "    - load SFCTA data as Geopandas point (one point = one origin or one destination)\n",
    "    - Get convex hull of the point\n",
    "    - Use the convex hull (+ buffer) as the external demand delimitation\n",
    "    2. create external centroids:\n",
    "    - select road with no fnode and capacity above 800 from sections_df\n",
    "    - create a point at the end of all selected road\n",
    "    - plot the points, get a list of points to remove visually\n",
    "    3. create external TAZs:\n",
    "    - create a mesh a points inside the external demand delimitation and outside the internal demand delimitation (project delimitation)\n",
    "    - use a Direction API (maybe Here direction):\n",
    "    for every mesh point:\n",
    "        Query path from mesh point to center of the project area\n",
    "        Find the closest external centroid to the path. Test that all paths are not to far from existing\n",
    "            external centroid --> if not, we might be missing one external centroid.\n",
    "        Associate the external centroid to the mesh point.\n",
    "        create external TAZ from mesh of points (if you reach point, Theo has already done it for internal TAZs)\n",
    "\n",
    "    @param dir_taz:         folder containing prefix_fremont_legs.csv where prefix=ending, internal and starting\n",
    "    @param sections_df:     geo pandas data frame of the aimsun sections\n",
    "    \"\"\"\n",
    "    # 1. Create a external demand delimitation:\n",
    "    expanded_hull_points = create_external_delim(dir_taz)\n",
    "    # 2. create external centroids:\n",
    "    external_centroid_nodes, internal_centroid_nodes = create_external_centroids(sections_df, expanded_hull_points)\n",
    "    # 3. create external TAZs:\n",
    "    mesh_points = create_mesh(expanded_hull_points, project_delimitation, 0.01)\n",
    "\n",
    "    # compute center of project area\n",
    "    internal_centroid_nodes = np.array([(p.x, p.y) for p in internal_centroid_nodes])\n",
    "    project_center = np.mean(internal_centroid_nodes, axis=0)\n",
    "    project_center = Point(project_center[0], project_center[1])\n",
    "\n",
    "    # for each mesh point find closest external centroid to its query path \n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])   \n",
    "    distance_to_centroid_threshold = 0.005\n",
    "    intersection_to_centroid_paths = []\n",
    "    info_point_to_center = []  # desired result\n",
    "    \n",
    "    count = 1\n",
    "    for point in mesh_points:\n",
    "        path = shortest_path_by_travel_time(point, project_center)\n",
    "        \n",
    "        if count % 1000 == 0:\n",
    "            print(\"progress by another 1000!\")\n",
    "        count += 1\n",
    "        # find intersection (point) of path and project delimitation\n",
    "        intersect_point = project_delimitation_line.intersection(path)\n",
    "\n",
    "        # find closest centroid to intersection point\n",
    "        min_distance = float('inf')\n",
    "        closest_centroid = None\n",
    "        for centroid in external_centroid_nodes:\n",
    "            dist = intersect_point.distance(centroid)\n",
    "            if dist < min_distance:\n",
    "                min_distance = dist\n",
    "                closest_centroid = centroid\n",
    "        \n",
    "        #print(closest_centroid)\n",
    "        \n",
    "        if min_distance < distance_to_centroid_threshold:\n",
    "            # path intersection to centroid\n",
    "            intersection_to_centroid = [(intersect_point.x, intersect_point.y), (closest_centroid.x, closest_centroid.y)]\n",
    "            intersection_to_centroid_paths.append(LineString(intersection_to_centroid))\n",
    "\n",
    "            # write result to csv\n",
    "            info_point_to_center.append([(point.x, point.y), project_center, closest_centroid, min_distance, path])\n",
    "\n",
    "    #if testing:\n",
    "        #kepler_map = KeplerGl(height=600)\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_center]}, crs='epsg:4326'), name='project_center')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': external_centroid_nodes}, crs='epsg:4326'), name='external_centroids')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_delimitation_line]}, crs='epsg:4326'), name='project_delimitation')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [external_delimitation]}, crs='epsg:4326'), name='external_delimitation')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': mesh_points}, crs='epsg:4326'), name='mesh_points')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [l[-1] for l in info_point_to_center]}, crs='epsg:4326'), name='paths')\n",
    "        #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': intersection_to_centroid_paths}, crs='epsg:4326'), name='intersection_to_centroid_paths')\n",
    "        #file_path = 'mesh_points_to_external_centroids.html'\n",
    "        #if output_dir:\n",
    "            #file_path = os.path.join(output_dir, file_path)\n",
    "        #kepler_map.save_to_html(file_name=file_path)\n",
    "\n",
    "    #create dataframe\n",
    "    info_points_col = ['origin_mesh_point','destination','closest_external_centroid','distance_to_centroid','path']\n",
    "    info_points_df = pd.DataFrame(info_point_to_center, columns = info_points_col)\n",
    "\n",
    "    # write results to csv\n",
    "    mesh_points_to_centroid_file_path = 'mesh_point_to_centroid.csv'\n",
    "    if output_dir:\n",
    "        mesh_points_to_centroid_file_path = os.path.join(output_dir, mesh_points_to_centroid_file_path)\n",
    "    to_csv(mesh_points_to_centroid_file_path,\n",
    "           'origin_mesh_point,destination,closest_external_centroid,distance_to_centroid,path',\n",
    "           info_point_to_center)\n",
    "    return info_points_df\n",
    "    #render    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_taz_from_csv(csv_file, output_dir=None):\n",
    "    project_delimitation_line = LineString(project_delimitation + [project_delimitation[0]])\n",
    "    \n",
    "    info_points_df = pd.read_csv(csv_file)\n",
    "    external_centroid_nodes = info_points_df['closest_external_centroid'].map(convert_point_to_coord).tolist()\n",
    "    external_centroid_nodes = [Point(coord[0], coord[1]) for coord in external_centroid_nodes]\n",
    "    \n",
    "    kepler_map = KeplerGl(height=600)\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_center]}, crs='epsg:4326'), name='project_center')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': external_centroid_nodes}, crs='epsg:4326'), name='external_centroids')\n",
    "    kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [project_delimitation_line]}, crs='epsg:4326'), name='project_delimitation')\n",
    "    #kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': [external_delimitation]}, crs='epsg:4326'), name='external_delimitation')\n",
    "    \n",
    "    taz_id = 0\n",
    "    boundary_list = []\n",
    "    taz_name_list = []\n",
    "    external_centroids = info_points_df['closest_external_centroid'].unique()\n",
    "    for centroid in external_centroids:\n",
    "        taz_df = info_points_df.loc[info_points_df['closest_external_centroid']==centroid]\n",
    "        taz_points = np.array(taz_df['origin_mesh_point'].map(convert_point_to_coord).tolist())\n",
    "        taz_hull = ConvexHull(taz_points)\n",
    "        taz_boundary = taz_points[taz_hull.vertices, :]\n",
    "        taz_poly = Polygon(taz_boundary)\n",
    "        taz_name = 'External TAZ' + str(taz_id)\n",
    "        kepler_map.add_data(data=gpd.GeoDataFrame({'geometry': taz_poly}, crs='epsg:4326',index=[0]), name=taz_name)\n",
    "        taz_name_list.append(taz_name)\n",
    "        boundary_list.append(taz_poly)\n",
    "        taz_id += 1\n",
    "    taz_gpd = gpd.GeoDataFrame({'taz_name':taz_name_list,'geometry':boundary_list})\n",
    "    file_path = 'external_taz.html'\n",
    "    if output_dir:\n",
    "        file_path = os.path.join(output_dir, file_path)\n",
    "    kepler_map.save_to_html(file_name=file_path)\n",
    "    return taz_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_point_to_coord(string):\n",
    "    lon, lat = string.split(' ')[1:]\n",
    "    lon = float(lon.split('(')[1])\n",
    "    lat = float(lat.split(')')[0])\n",
    "    return [lon, lat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n",
      "Map saved to C:\\Users\\jainc\\Dropbox/Private Structured data collection\\Data processing\\Kepler maps\\HereAPI\\external_taz.html!\n"
     ]
    }
   ],
   "source": [
    "# print(sections_shp.columns)\n",
    "# print(sections_shp.head)\n",
    "output_dir = os.path.join(data_path, 'Data processing', 'Kepler maps', 'HereAPI')\n",
    "mesh_points_to_centroid_file_path = 'mesh_point_to_centroid.csv'\n",
    "taz_gpd = render_taz_from_csv(os.path.join(output_dir, mesh_points_to_centroid_file_path), output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sfcta_folder = os.path.join(data_path, \"Data processing\", \"Raw\", \"Demand\", \"OD demand\", \"SFCTA demand data\")\n",
    "sections_path = os.path.join(data_path, \"Aimsun\", \"Inputs\", \"sections.shp\")\n",
    "\n",
    "sections_shp = gpd.GeoDataFrame.from_file(sections_path)\n",
    "sections_shp = sections_shp.to_crs(crs_degree) \n",
    "# print(sections_shp.columns)\n",
    "# print(sections_shp.head)\n",
    "df_test = create_external_taz(sfcta_folder, sections_shp, project_delimitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "1. Split the function create_external_taz into several subfunction that needs to be run sequentially\n",
    "    1. Create a external demand delimitation:\n",
    "    2. Create external centroids:\n",
    "    3. Create external TAZs:\n",
    "        1. Create a mesh grid\n",
    "        2. Associate every mesh grid to an external centroid\n",
    "        3. From the mesh grid create the external TAZ (Theo can do it)\n",
    "2. For every sub-function write some code to check the function (or render the output of the function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "def newpoint():\n",
    "    return uniform(-180,180), uniform(-90, 90)\n",
    "\n",
    "points = [newpoint() for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13209.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11821.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13207.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>793397.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>692913.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1205259.2</td>\n",
       "      <td>13207.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12864.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13210.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2453.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>792028.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>691544.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1203890.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>826.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1119.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204755.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11819.1</td>\n",
       "      <td>2444.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2442.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>790552.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>690067.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1202413.5</td>\n",
       "      <td>2442.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2099.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1206094.7</td>\n",
       "      <td>1204801.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1203401.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1204799.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>828595.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>701132.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1204799.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1204456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13208.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2451.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>792026.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>691542.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1203887.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>824.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12858.1</td>\n",
       "      <td>830.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2101.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>791675.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>691191.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1203537.3</td>\n",
       "      <td>827.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1    2    3          4   5          6   7   8     9   \\\n",
       "0         0.0    13209.9  NaN  NaN    11821.0 NaN    13207.8 NaN NaN   NaN   \n",
       "1     13210.8        0.0  NaN  NaN     2453.9 NaN        2.1 NaN NaN   NaN   \n",
       "2         NaN        NaN  0.0  NaN        NaN NaN        NaN NaN NaN   NaN   \n",
       "3         NaN        NaN  NaN  0.0        NaN NaN        NaN NaN NaN  14.3   \n",
       "4     11819.1     2444.4  NaN  NaN        0.0 NaN     2442.3 NaN NaN   NaN   \n",
       "..        ...        ...  ...  ...        ...  ..        ...  ..  ..   ...   \n",
       "95  1206094.7  1204801.4  NaN  NaN  1203401.5 NaN  1204799.3 NaN NaN   NaN   \n",
       "96    13208.7        2.1  NaN  NaN     2451.8 NaN        0.0 NaN NaN   NaN   \n",
       "97        NaN        NaN  NaN  NaN        NaN NaN        NaN NaN NaN   NaN   \n",
       "98        NaN        NaN  NaN  NaN        NaN NaN        NaN NaN NaN   NaN   \n",
       "99    12858.1      830.0  NaN  NaN     2101.2 NaN      827.9 NaN NaN   NaN   \n",
       "\n",
       "    ...        90      91  92        93        94         95         96   97  \\\n",
       "0   ...  793397.8     NaN NaN  692913.6       NaN  1205259.2    13207.8  NaN   \n",
       "1   ...  792028.6     NaN NaN  691544.4       NaN  1203890.0        2.1  NaN   \n",
       "2   ...       NaN  1119.7 NaN       NaN       NaN        NaN        NaN  NaN   \n",
       "3   ...       NaN     NaN NaN       NaN  204755.2        NaN        NaN  NaN   \n",
       "4   ...  790552.1     NaN NaN  690067.9       NaN  1202413.5     2442.3  NaN   \n",
       "..  ...       ...     ...  ..       ...       ...        ...        ...  ...   \n",
       "95  ...  828595.2     NaN NaN  701132.8       NaN        0.0  1204799.3  NaN   \n",
       "96  ...  792026.5     NaN NaN  691542.3       NaN  1203887.9        0.0  NaN   \n",
       "97  ...       NaN     NaN NaN       NaN       NaN        NaN        NaN  0.0   \n",
       "98  ...       NaN     NaN NaN       NaN       NaN        NaN        NaN  NaN   \n",
       "99  ...  791675.9     NaN NaN  691191.7       NaN  1203537.3      827.9  NaN   \n",
       "\n",
       "     98         99  \n",
       "0   NaN    12864.5  \n",
       "1   NaN      826.9  \n",
       "2   NaN        NaN  \n",
       "3   NaN        NaN  \n",
       "4   NaN     2099.0  \n",
       "..  ...        ...  \n",
       "95  NaN  1204456.0  \n",
       "96  NaN      824.8  \n",
       "97  NaN        NaN  \n",
       "98  0.0        NaN  \n",
       "99  NaN        0.0  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = table(\n",
    "                          points,\n",
    "                          output='df')\n",
    "#shortest_path = wkt.loads(query_result[0]['geometry'])\n",
    "#print(query_result['geometry'])\n",
    "query_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = simple_route(\n",
    "                          [-121.941339, 37.5241981], [-121.9253273, 37.4977866],\n",
    "                          output='route', overview=\"full\", geometry='wkt')\n",
    "shortest_path = wkt.loads(query_result[0]['geometry'])\n",
    "#print(query_result['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(geometry=[shortest_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d1156d8f604d09a580fccaa35da347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Route': {'index': [0], 'columns': ['geometry'], 'data': [['LINESTRING (-121.9413399999999967 3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_2 = KeplerGl(height=1000)\n",
    "map_2.add_data(data=gdf, name = \"Route\")\n",
    "map_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = multisplit_polygon(Polygon(create_external_delim(sfcta_folder)), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_graphs = []\n",
    "streets_graphs = []\n",
    "empty_polygons = []\n",
    "total_time = 0\n",
    "\n",
    "for idx, s in enumerate(split):\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        osm_graph = ox.graph_from_polygon(s, truncate_by_edge=True)\n",
    "        osm_graph = ox.add_edge_speeds(osm_graph)\n",
    "        osm_graph = ox.add_edge_travel_times(osm_graph)\n",
    "        \n",
    "        nodes, streets = ox.graph_to_gdfs(osm_graph)\n",
    "        nodes_graphs.append(nodes)\n",
    "        streets_graphs.append(streets)\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        print(\"Subsection {} finished. Took {}\".format(idx, end - start))\n",
    "        \n",
    "        total_time += end - start\n",
    "        \n",
    "    except:\n",
    "        print(\"{} turned up empty\".format(s))\n",
    "        empty_polygons.append(s)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_all = pd.concat(nodes_graphs)\n",
    "network_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://github.com/keplergl/kepler.gl/blob/master/docs/keplergl-jupyter/user-guide.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c23f95bfc740dabc0251497f96dc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'Split': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'columns': ['geometry'],…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf = gpd.GeoDataFrame(geometry=split)\n",
    "map_2 = KeplerGl(height=1000)\n",
    "map_2.add_data(data=gdf, name = \"Split\")\n",
    "map_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
