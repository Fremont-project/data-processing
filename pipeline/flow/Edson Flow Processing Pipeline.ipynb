{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the pipeline\n",
    "\n",
    "<font color=red> The goal of this notebook is to associate the raw ADT ground-data for 2013, 2015, 2017 and 2019, and the raw speed ground-data in 2015 to the Aimsun network for the calibration of Aimsun simulations. First the raw data is processed in a common format, then, every detector is associated with a network link inside Aimsun. Later, every detector is associated with a road section to create heatmap and understand the evolution of flows over years. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of the pipeline: \n",
    "- One file matching detectors location to Aimsun road section\n",
    "- Two files with the processed flow data for 2013, 2015, 2017, 2019 (one for the city and one for PeMS)\n",
    "- One file with the processed speed data for 2015\n",
    "- One file with the flow data corresponding to road sections for 2013, 2015, 2017 and 2019\n",
    "- PCA on flow data\n",
    "\n",
    "### Inputs of the pipeline: \n",
    "**Raw data**\n",
    "- PeMS account [_publicly available_]. Can be created in http://pems.dot.ca.gov\n",
    "- PeMS detectors location [_publicly available_]. In the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/PeMS`\n",
    "- City average annual daily traffic (AADT) data for 2013, 2017 and 2019 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/City`\n",
    "- Kimley-Horn flow and speed data for 2015 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/Kimley\\ Horn\\ Data`\n",
    "\n",
    "**Manually made dataset**\n",
    "- Aimsun network\n",
    "- Detectors location\n",
    "- Road section layer\n",
    "- Doc files or city ADT data corresponding to the PDF files\n",
    "- Detectors ID to corresponding flow file name\n",
    "\n",
    "\n",
    "### Temporary files of the pipeline\n",
    "- CSV flow data\n",
    "    - City and Kimley Horn\n",
    "    - PeMS data\n",
    "- geographic information of road detectors for 2013, 2015, 2017, 2019\n",
    "- Flow_processed_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent scripts: \n",
    "- [pems_download.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pems_download.py): the script automatically download PeMs data for chosen date.\n",
    "- [pre_process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pre_process_flow.py): the script (A). parse the ADT data from xlsx, doc and csv files to csv files (B). Find the coordinates of the city detectors (C). Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS \n",
    "- [process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/process_flow.py): this script processes all the flow traffic data into one big CSV file from both city and PeMS data files\n",
    "- [process_years_together.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/process_years_together.py): this script combines the previous scripts and processes the data for 2013, 2015, 2017, 2019\n",
    "- [fremontdropbox](https://github.com/Fremont-project/data-processing/blob/master/fremontdropbox.py): \n",
    "\n",
    "### Dependent libraries:\n",
    "- os\n",
    "- sys\n",
    "- webbrowser\n",
    "- time\n",
    "- requests\n",
    "- pathlib.Path\n",
    "- textract\n",
    "- [numpy](https://numpy.org)\n",
    "- datetime\n",
    "- pandas\n",
    "- math\n",
    "- requests\n",
    "- glob\n",
    "- csv\n",
    "- re\n",
    "- Rtree (needs spatialindex, brew install spatialindex)\n",
    "- osmnx\n",
    "- keplergl\n",
    "- shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work done by the script\n",
    "1. Obtaining Data\n",
    "    - Turn city pdf to doc\n",
    "    - PeMS data download\n",
    "2. Preprocessing city and Kimley-Horn data\n",
    "    - Parsing the flow data [We are here]\n",
    "    - Parsing the speed data\n",
    "    - Geocoding location of the detectors\n",
    "    - Manually update the location of the detectors\n",
    "3. Processing the data\n",
    "    - Creating one file with the flow for all detectors\n",
    "    - Creating a layer of road section\n",
    "    - Matching the detectors to road section\n",
    "    - Creating one file with the flow over time for every road section\n",
    "    - Matching the detectors with the corresponding road section in Aimsun\n",
    "4. Exporting the data\n",
    "    - Exporting a csv file where Aimsun road sections are matched with detectors\n",
    "    - Exporting a csv file with flow for the different road sections\n",
    "    - Exporting traffic flow heatmap\n",
    "    - Exporting speed heatmap\n",
    "    - PCA on the traffic flow heatmap over years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- run PCA and create heatmap with ArcGIS (heatmap will be done by Theo)\n",
    "- Put the scripts together in one main script\n",
    "- add link from work done by the script to section of the iPython notebook\n",
    "- write tests for detectors shapefiles/\n",
    "- Once everything done: clean the pipeline, clean github folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fremontdropbox import get_dropbox_location\n",
    "\n",
    "dropbox_dir = get_dropbox_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Obtaining Data\n",
    "\n",
    "### A) PeMS Data Download \n",
    "    \n",
    "Script: pems_download.py <br>\n",
    "Download traffic data from the PEMS website (pews.dot.ca.gov) for years 2013, 2015, 2017 and 2019 <br>\n",
    "\n",
    "We download the data calling the method download(detector_ids, year, PeMS_dir) from the pems_download.py file. The process takes about 20 minutes to run. \n",
    "Run `print(help(pems.download))` to get some help.\n",
    "\n",
    "### B) Turn city pdf to doc\n",
    "\n",
    "Some of the flow data that we got from the city were in pdf files. To be able to parse them, we convert them to doc files using online website. This should be done before running the code. The doc files are inputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download in module pems_download:\n",
      "\n",
      "download(year, detector_ids, PeMS_dir)\n",
      "    This function downloads traffic data from the PeMS website (pems.dot.ca.gov).\n",
      "    This function has for input:\n",
      "        - PeMS detectors ID: detector_ids (an array of detectors)\n",
      "        - Year for the desired data: year (one year as a integer, should be 2013, 2015, 2017 or 2019)\n",
      "    \n",
      "    This function has for output:\n",
      "        - All corresponding PeMS detectors data file for the given year (and the given days encoded in the url).\n",
      "        - Stored in the download folder as PeMS_dir/PeMS_year/PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
      "        One xlsx file has two sheets:\n",
      "            - PeMS Report Description\n",
      "            - Report Data\n",
      "                - Contains the traffic flow data\n",
      "                - Each row gives the number of vehicles observed in one time step (5 minutes) per lane number over the columns.\n",
      "                - The first column gives the date and time stamp, and the columns that follow are lanes (i.e. Lane 1 Flow, Lane 2 Flow). We care about the column 'Flow (Veh/5 Minutes)' which is the total flow for every lane every 5 minutes.\n",
      "                - The column \"% Observed\" correspond to how much of the flow is due to real vehicles sensed or due to estimation from other days due to a technical issue that make the sensor not sensing every cars.\n",
      "        - Flow data are download for three days\n",
      "            - From the first Tuesday of March at 00:00am until the first thursday of March at 23:59pm\n",
      "    For the function to work, you need to:\n",
      "        - Log in to PeMS in the same browser that runs this Jupyter notebook\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pems_download as pems\n",
    "print(help(pems.download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* IMPORTANT *************\n",
    "# --> For this cell to work, you need to log in to PeMS in the same browser that runs this Jupyter notebook\n",
    "\n",
    "## The IDs of the PeMS detectors where obtained using ArcGIS software and an input file, pems_detectors.csv, containing the locations of all the PeMS dectectors in California\n",
    "## this should be done in Python!\n",
    "detector_ids = [403250, 403256, 403255, 403257, 418387, 418388, 400376,\n",
    "               413981, 413980, 413982, 402794, 413983, 413984, 413985,\n",
    "               413987, 413986, 402796, 413988, 402799, 403251, 403710,\n",
    "               403254, 403719, 400566, 418420, 418419, 418422, 418423,\n",
    "               402793, 403226, 414015, 414016, 402795, 402797, 414011,\n",
    "               402798]\n",
    "PeMS_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMS'\n",
    "\n",
    "# pems.download(2013, detector_ids, PeMS_dir)\n",
    "# pems.download(2015, detector_ids, PeMS_dir)\n",
    "# pems.download(2017, detector_ids, PeMS_dir)\n",
    "# pems.download(2019, detector_ids, PeMS_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO Later:\n",
    "- Find the list of detectors ID using the project delimitation shapefile. (to be done later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeMS during 03/05/2013 00:00:00-03/07/2013 23:59:59\n",
      "PeMS during 03/03/2015 00:00:00-03/05/2015 23:59:59\n",
      "PeMS during 03/07/2017 00:00:00-03/09/2017 23:59:59\n",
      "PeMS during 03/05/2019 00:00:00-03/07/2019 23:59:59\n"
     ]
    }
   ],
   "source": [
    "pems.PeMS_tester(2013, PeMS_dir, True)\n",
    "pems.PeMS_tester(2015, PeMS_dir, True)\n",
    "pems.PeMS_tester(2017, PeMS_dir, True)\n",
    "pems.PeMS_tester(2019, PeMS_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing and processing city data\n",
    "\n",
    "\n",
    "### Parsing city and Kimley Horn flow data from xlsx, doc and csv files to csv files\n",
    "\n",
    "Here, we process the Excel and PDF ADT data files (city data) into CSV files.\n",
    "\n",
    "- run `print(help(pre_process.parse_excel_2013)` to get some help for 2013 excel files\n",
    "- run `print(help(pre_process.parse_excel_2015)` to get some help for 2015 excel files\n",
    "- run `print(help(pre_process.parse_excel_2017)` to get some help for 2017 excel files\n",
    "- run `print(help(pre_process.parse_excel_2019)` to get some help for 2019 excel files\n",
    "- run `print(help(pre_process.parse_adt_as_file)` to get some help for the 2017 and 2019 doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function process_adt_data in module pre_process_flow:\n",
      "\n",
      "process_adt_data(year, Processed_dir, Input_dir)\n",
      "    This function processes the Excel and PDF ADT data files (city data) into CSV files. Note that one file corresponds to one main road and the traffic flow data recordings in it.\n",
      "    \n",
      "    This function has input:\n",
      "        - year which takes values 2013, 2015, 2017 or 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    The function has output:\n",
      "        - CSV files located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pre_process_flow as pre_process\n",
    "print(help(pre_process.process_adt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function parse_excel_2013 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2013(dfs)\n",
      "    ***2013 Excel files*** are structured in data sheets. The first data sheet \"Summary\" contains the main road, cross streets, city information and the start date of the recording. It also summarizes the data contained in all other sheets into a bar plot of traffic flow vs time of day bins (i.e Tuesday AM, Wednesday PM) for different flow directions and into a line plot of traffic flow vs. hour of day for different days of the week. The sheets that follow are named \"D1\", \"D2\",...\"DN\" where N denotes the N'th day since the start date. These sheets are structured into two tables, AM counts and PM counts. Each table row gives the traffic flow per timestep of 15 minutes. The first column is the time of day in hh:mm format follow by direction columns of traffic flow (NB, SB, EB, WB).\n",
      "\n",
      "None\n",
      "Help on function parse_excel_2015 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2015(dfs)\n",
      "    ***2015 Excel files***. The files come from Kimley Horn. Every excel file has 7 relevant sheets including the hidden sheets. \n",
      "    They are ['Data', 'ns Day 1', 'ns Day 2', 'ns Day 3', 'ew Day 1', 'ew Day 2', 'ew Day 3']. Here, we are only going to process the 'Data' sheet.\n",
      "\n",
      "None\n",
      "Help on function parse_excel_2017 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2017(dfs)\n",
      "    ***2017 Excel files*** are structured in one data sheet giving a header and a table for traffic flow. The header gives the start date and time of the recording, \n",
      "    site code and sensor location, and the table gives traffic flow per a 15 minute timestep. The table's first two columns give the date and time and the following \n",
      "    columns give traffic flow per directions.\n",
      "\n",
      "None\n",
      "Help on function speed_data_parser in module pre_process_flow:\n",
      "\n",
      "speed_data_parser(speed_data_dir, Processed_dir)\n",
      "    This scripts parse the speed data 2015\n",
      "    Input: Kimley_Horn_flow_dir, Processed_dir\n",
      "\n",
      "None\n",
      "Help on function parse_adt_as_file in module pre_process_flow:\n",
      "\n",
      "parse_adt_as_file(file_path, year, out_folder)\n",
      "    Process Doc files\n",
      "    \n",
      "    ***2017 and 2019 PDF files*** are structured with a header and 3 tables of traffic flow data (one table per day of subsequent days). \n",
      "    The header gives the site location and other miscellaneous meta data. Each table is titled by the date and timestep (15 minutes) of \n",
      "    the recording. A table is organized by columns each representing the hour of day (0 - 23). Hence for a given column, the first row \n",
      "    gives the hour of the day, the second gives the total flow for the hour, and the third to last row (4 rows total) gives traffic flow per\n",
      "     15 minute timestep for the hour.\n",
      "    \n",
      "    \n",
      "    Years 2017 and 2019 files have the same structure hence we can reuse this code.\n",
      "    Structure refers to data organized in 3 tables split by *\n",
      "    Note no doc files for 2013.\n",
      "\n",
      "None\n",
      "Help on function parse_adt_as_file in module pre_process_flow:\n",
      "\n",
      "parse_adt_as_file(file_path, year, out_folder)\n",
      "    Process Doc files\n",
      "    \n",
      "    ***2017 and 2019 PDF files*** are structured with a header and 3 tables of traffic flow data (one table per day of subsequent days). \n",
      "    The header gives the site location and other miscellaneous meta data. Each table is titled by the date and timestep (15 minutes) of \n",
      "    the recording. A table is organized by columns each representing the hour of day (0 - 23). Hence for a given column, the first row \n",
      "    gives the hour of the day, the second gives the total flow for the hour, and the third to last row (4 rows total) gives traffic flow per\n",
      "     15 minute timestep for the hour.\n",
      "    \n",
      "    \n",
      "    Years 2017 and 2019 files have the same structure hence we can reuse this code.\n",
      "    Structure refers to data organized in 3 tables split by *\n",
      "    Note no doc files for 2013.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(pre_process.parse_excel_2013))\n",
    "print(help(pre_process.parse_excel_2015))\n",
    "print(help(pre_process.parse_excel_2017))\n",
    "print(help(pre_process.speed_data_parser))\n",
    "print(help(pre_process.parse_adt_as_file))\n",
    "print(help(pre_process.parse_adt_as_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- Comment the function speed data parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed'\n",
    "Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Flow_processed'\n",
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n",
    "\n",
    "# Preprocess 2013, 2015, 2017 and 2019 flow data\n",
    "# pre_process.process_adt_data(2013, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2015, Processed_dir, Kimley_Horn_flow_dir)\n",
    "# pre_process.process_adt_data(2017, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2019, Processed_dir, City_dir)\n",
    "\n",
    "# Preprocess 2015 speed data\n",
    "# pre_process.speed_data_parser(Kimley_Horn_flow_dir, Processed_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc files are all good for both 2017 and 2019!\n"
     ]
    }
   ],
   "source": [
    "#test the elibility of the doc files in 2017 and 2019. \n",
    "pre_process.doc_file_tester(City_dir, [2017, 2019])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process city and PeMS data\n",
    "\n",
    "- run `print(help(pre_process.get_geo_data))` to get some help.\n",
    "\n",
    "# <font color=red> To do (re do from scratch): </font>\n",
    "1. Write flow_processed_generator_city\n",
    "    - It should create the files:\n",
    "        - flow_processed_city.csv\n",
    "        - year_info.csv for year = 2013, 2015, 2017 and 2019\n",
    "    - It should only read:\n",
    "        - The files in `Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Flow_processed/YEAR processed`\n",
    "2. Write flow_processed_generator_pems\n",
    "    - It should create the files:\n",
    "        - flow_processed_pems.csv\n",
    "    - It should only read:\n",
    "        - The files in `Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMS/PeMS_YEAR`\n",
    "3. Write test functions to make sure the detectors id are correct:\n",
    "    - Compare ids in flow_processed_city and flow_processed_pems to the ones in the detectors shapefiles.\n",
    "    - You can refer to the [google doc](https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0)\n",
    "\n",
    "Later:\n",
    "4. Work on the shapefile to correct the IDs of the detectors.\n",
    "\n",
    "Flow_processed_city.csv should be like:\n",
    "\n",
    "| Name | Direction | Id | year | Day 1 | Day 1 - 0:0 | Day 1 - 0:15 | Day 1 - 0:30 | ... | Day 3 - 23:45 |\n",
    "|------|------|------|------|------|------|------|------|------|------|\n",
    "| Auto Mall Pkwy betw. Fremont & I680.csv | EB | 1 | 2013 | 2/12/13 | 20 | 27 | 13 | ... | 13 |\n",
    "| Auto Mall Pkwy betw. Fremont & I680.csv | WB | 2 | 2013 | 2/12/13 | 36 | 25 | 18 | ... | 13 |\n",
    "\n",
    "Flow_processed_pems.csv should be like (I think that we can use PeMS id for ID):\n",
    "\n",
    "| Year | Name | Id | %Observed | Day 1 | Day 1 - 0:0 | ... | Day 3 - 23:45 |\n",
    "|------|------|------|------|------|------|------|------|\n",
    "| 2013 | PeMS Detector 403250 | 243 | 0 | 3/5/13 0:00 | 256 | ... | 13 |\n",
    "| 2013 | PeMS Detector 403256 | 246 | 0 | 3/5/13 0:00 | 0 | ... | 13 |\n",
    "| 2013 | PeMS Detector 403255 | 249 | 0 | 3/5/13 0:00 | 0 | ... | 13 |\n",
    "| 2013 | PeMS Detector 403257 | 252 | 0 | 3/5/13 0:00 | 253 | ... | 13 |\n",
    "| 2013 | PeMS Detector 418387 | 255 | Did not exist in 2013 | X\t| . | ... | . |\n",
    "| 2013 | PeMS Detector 418388 | 258 | Did not exist in 2013 | X | . | ... | . |\n",
    "\n",
    "Year_info.csv should be like:\n",
    "\n",
    "| Name | City | ID | Main road | Cross road | Start lat | Start lng | End lat | End lng |\n",
    "|------|------|------|------|------|------|------|------|------|\n",
    "|Paseo Padre Pkwy Bet. Washington Blvd & Durham Rd | Fremont | 10 | Paseo Padre Pkwy | Washington Blvd & Durham Rd | 37.5316567 | -121.9342 | 37.5142565 | -121.9311615 |\n",
    "|Paseo Padre Pkwy Bet. Driscoll Rd & Washington Blvd | Fremont | 11 | Paseo Padre Pkwy | Driscoll Rd & Washington Blvd | 37.5412612 | -121.9482207 | 37.5316567 | -121.9342 |\n",
    "| Washington Blvd Bet. Paseo Padre Pkwy & Mission Blvd | Fremont | 12 | Washington Blvd | Paseo Padre Pkwy & Mission Blvd | 37.5316567 | -121.9342 | 37.5334051 | -121.9200877 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Matching detectors to Aimsun road section\n",
    "\n",
    "# To do:\n",
    "- re-explain that\n",
    "\n",
    "## 5. Comparing evolution of flow over years\n",
    "# To do:\n",
    "- re-explain that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FOLLOWING WILL BE REMOVED. Everything below that is still usefull should be at one time move up here.\n",
    "\n",
    "### B) Find the coordinates of the city detectors\n",
    "We obtained the coordinates of the detectors by calling the method get_geo_data(year) from the pre_process.py python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_geo_data in module pre_process_flow:\n",
      "\n",
      "get_geo_data(year, Input_dir, Processed_dir)\n",
      "    This function iterates over the ADT files and obtains the adresses of the detectors to then use with Google API to obtain latitude and longitude coordinates.\n",
      "    \n",
      "    This function has input:\n",
      "        - Year takes values 2013, 2015, 2017, 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    This function has output:\n",
      "        - CSV file \"year_info_coor.csv\" containing the coordinates of detectors and located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015 \n",
      "    \n",
      "    + add the doc files\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(pre_process.get_geo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_process.get_geo_data(2013, City_dir, Processed_dir)\n",
    "# pre_process.get_geo_data(2015, Kimley_Horn_flow_dir, Processed_dir)\n",
    "# pre_process.get_geo_data(2017, City_dir, Processed_dir)\n",
    "# pre_process.get_geo_data(2019, City_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f3b10b757f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pre_process.google_doc_generater(Processed_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpre_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_processed_generater1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mProcessed_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessed_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/research/Waze group/Fremont/Github/data-processing/pipeline/flow/pre_process_flow.py\u001b[0m in \u001b[0;36mflow_processed_generater1\u001b[0;34m(path, re_formated_Processed_dir)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m     \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_formated_Processed_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/processed_flow_tmp.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m     \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_formated_Processed_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/processed_flow_tmp.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_formated_Processed_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/processed_flow_tmp.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# summary files generater:\n",
    "\n",
    "# pre_process.google_doc_generater(Processed_dir)\n",
    "pre_process.flow_processed_generater1(Processed_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Theo: I am here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS <br>\n",
    "Done in the software manually in ArcGIS.\n",
    "1. Export Aimsun network as GIS file\n",
    "2. Import Aimsun network in ArcGIS\n",
    "3. Import detectors in ArcGIS as XY_points\n",
    "4. Move detectors to put them on corresponding road in Aimsun\n",
    "5. Associate to every detectors the External ID of the Aimsun road (to be done again)\n",
    "\n",
    "**TO DO THEO**: Add the process to create the detectors inside Aimsun.\n",
    "Add the process to match the detectors to road section (and create the file lines_to_detectors.xlsx\n",
    "\n",
    "\n",
    "### Later to do: do the spatial join in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed/ADT'\n",
    "PeMs_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMs'\n",
    "local_download =  str(os.path.join(Path.home(), \"Downloads\"))\n",
    "re_formated_Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Temporary exports to be copied to processed data/Flow_processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Process the csv files (city + caltrans) to one big file. <br>\n",
    "source code: processing flow to one CSV.ipynb <br>\n",
    "\n",
    "We combine all the flow traffic data into one big CSV file from both city and PeMS data files. This is done by calling the function process_data() from the process_flow.py python file. \n",
    "\n",
    "Description of function process_data()\n",
    "<br>\n",
    "The function has input:\n",
    "- \"Flow_processed_tmp.csv\" file that lists all the processed files from city and PeMS data\n",
    "- The processed files created from the Parsing Data section\n",
    "\n",
    "The function has output:\n",
    "- \"Flow_processed_city.csv\" containing combined city flow data for all year\n",
    "- \"Flow_processed_PeMS.csv\" containing combined PeMS flow data for all years\n",
    "\n",
    "For the function to work:\n",
    "- The processed files (input) must be located in City and PeMs folders\n",
    "- 2013 city processed files are located in \"City/2013 reformat/\"\n",
    "- 2017 and 2019 city processed files that originated from DOC (which originated from PDF) files are located in \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019\n",
    "- 2017 and 2019 city processed files that originated from Excel files are located in \"City/Year reformat/Format from xlsx\" folder where Year=2017 or 2019\n",
    "- 2013, 2017 and 2019 PeMS data files are located in \"PeMS_Year\" folder where Year=2013, 2017 or 2019\n",
    "\n",
    "For the pipeline to work:\n",
    "- The ouput files must remain in the working directory, no moving necessary.\n",
    "\n",
    "Structure of ouput files: \n",
    "- Flow_processed_city.csv\n",
    "    - contains city traffic data where the rows represent traffic flow. The first 5 columns give info about the traffic flow and are Year, Name, Id, Direction, Day 1 where Name refers to the file name from which the data originated, Id is the Id from the \"Flow_processed_tmp.csv\" file, Direction is the direction of flow and Day 1 is the start date of recording. The columns that follow are day-timesteps for flow data. There are 3 days total over which traffic flow is recorded and time progresses in 15 minute steps. Hence the data columns progress as \"Day 1 - 0:0\", \"Day 1 - 0:15\", \"Day 1 - 0:30\",...,\"Day 3 - 23:30\", \"Day 3 - 23:45\".\n",
    "- Flow_processed_PeMS.csv\n",
    "    - contains PeMS flow traffic data where the rows represent traffic flow. The first columns are Name, Id and Name PeMS where Name contains the PeMS detector Id, Id is the Id assigned from \"Flow_processed_tmp.csv\", Name PeMs is the road address. The next 6 columns give Observed Year and Day Year for the 3 years, 2013, 2017 and 2019. Observed Year is the percentage of the observed data and Day Year is the start date of recording. The columns that follow are Year-Day-timestep, there are 3 years, 3 days and time progresses in 15 minute steps. Hence the columns progress as \"2013-Day 1 - 0:0\", \"2013-Day 1 - 0:15\", \"2013-Day 1 - 0:30\",...,\"2019-Day 3 - 23:30\", \"2019-Day 3 - 23:45\".\n",
    "\n",
    "**(DONE) TO DO 8**: Explain the structure of the output files. Also, feel free to document the doc in the python file (or iPython file). Explain also the input (Flow_processed_tmp.csv) and how it was created (I think it was created from the google doc https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0).\n",
    "\n",
    "***Edson Question***: the Flow_processed_tmp.csv file and the google doc seem the same to me (except for the lat, lng info on the right side of the google doc). Beyond this, I don't know how the file was created. Who to ask for more info?\n",
    "\n",
    "***Theo Answer***: I guess I have created the file from the google doc. But I have also created the google doc during the parsing. Probably in 2) we can create Flow_processed_tmp.csv from year_info.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_flow as pf\n",
    "\n",
    "pf.process_data_City(Processed_dir, re_formated_Processed_dir)\n",
    "pf.proces_data_PeMS(Processed_dir, re_formated_Processed_dir, PeMS_dir)\n",
    "\n",
    "\n",
    "#pf.process_data(Processed_dir, re_formated_Processed_dir, PeMs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Create file that gives traffic flows for specific road sections for every year. <br>\n",
    "source note: put years together.ipynb\n",
    "\n",
    "- Use detectors (lines_to_detectors.csv) and flow processed city (flow_processed_city.csv) data to create all years flow data in \"flow_processed_section.csv\"\n",
    "\n",
    "\n",
    "# To do 9: the function pytogether should be written again. + add 2015\n",
    "- add the PeMS data\n",
    "- be more clever about missing data or road section associated with several detectors (take the average)\n",
    "\n",
    "### To do later, do the spatial join in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_years_together as pytogether\n",
    "\n",
    "line_to_detectors = re_formated_Processed_dir + '/lines_to_detectors.csv'\n",
    "flow_processed_city = re_formated_Processed_dir + '/Flow_processed_city.csv'\n",
    "pytogether.run(line_to_detectors, flow_processed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Analyse PCA results using heatmap inside ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_folder = dropbox_dir + \"/Private Structured data collection/Data processing/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plot\n",
    "from shapely import wkt\n",
    "# import keplergl as kp\n",
    "import spatial_join as sj\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# # directories to detectors and aimsum sections shape files\n",
    "# # start from current working directory (ie. same folder as notebook)\n",
    "data_process_folder = dropbox_dir + \"/Private Structured data collection/Data processing/\"\n",
    "detectors_folder = data_process_folder + \"Raw/Demand/Flow_speed/detectors/\"\n",
    "section_folder = data_process_folder + \"Raw/Network/Aimsun/\"\n",
    "output_folder = data_process_folder + \"Temporary exports to be copied to processed data/Network/Infrastructure/Detector/\"\n",
    "\n",
    "# find closest road segment for each detector\n",
    "# results written to a csv per year named: detetctors_to_road_segments_year.csv\n",
    "sj.detectors_to_road_segments(2013, sections_folder, detectors_folder, output_folder)\n",
    "sj.detectors_to_road_segments(2015, sections_folder, detectors_folder, output_folder)\n",
    "sj.detectors_to_road_segments(2017, sections_folder, detectors_folder, output_folder)\n",
    "sj.detectors_to_road_segments(2019, sections_folder, detectors_folder, output_folder)\n",
    "sj.detectors_to_road_segments('pems', sections_folder, detectors_folder, output_folder)\n",
    "\n",
    "# # Find duplicates. That is, multiple detectors are closest to one road, if any.\n",
    "# # results if any written to a csv per year named: duplicates_year.csv\n",
    "sj.find_duplicates(2013, sections_folder, detectors_folder, output_folder, show_plot=True)\n",
    "sj.find_duplicates(2015, sections_folder, detectors_folder, output_folder, show_plot=True)\n",
    "sj.find_duplicates(2017, sections_folder, detectors_folder, output_folder, show_plot=True)\n",
    "sj.find_duplicates(2019, sections_folder, detectors_folder, output_folder, show_plot=True)\n",
    "sj.find_duplicates('pems', sections_folder, detectors_folder, output_folder, show_plot=True)\n",
    "\n",
    "# kepler_map = sj.create_kepler_map(sections_folder, detectors_folder)\n",
    "# kepler_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edson Tasks Done (0-2)\n",
    "Additionally, library dependent files were added on top. They are:\n",
    "- Rtree (needs spatialindex, brew install spatialindex)\n",
    "- osmnx\n",
    "- keplergl\n",
    "- shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spatial_join as sj\n",
    "import process_years_together2 as pyt\n",
    "\n",
    "# Task 0\n",
    "# Formatted output files, detectors_to_road_segments_year.csv were sent to Su for all years and pems.\n",
    "\n",
    "# Task 1\n",
    "data_process_folder = dropbox_dir + \"/Private Structured data collection/Data processing/\"\n",
    "streetline_folder = data_process_folder + \"Raw/Demand/Flow_speed/Road section/\"\n",
    "detectors_folder = data_process_folder + \"Raw/Demand/Flow_speed/detectors/\"\n",
    "output_folder = data_process_folder + \"Temporary exports to be copied to processed data/Network/Infrastructure/Detector/\"\n",
    "\n",
    "# Create lines_to_detectors.csv in output_folder\n",
    "sj.streetlines_to_detectors(streetline_folder, detectors_folder, output_folder)    \n",
    "\n",
    "# Task 2\n",
    "flow_processed_all_folder = data_process_folder + \"Auxiliary files/Demand/Flow_speed/Flow_processed/\"\n",
    "\n",
    "# Create flow_processed_section.csv using lines_to_detectors.csv and Flow_processed_all.csv\n",
    "pyt.create_flow_processed_section(output_folder, flow_processed_all_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do Edson\n",
    "0. Make sure to have the output of the detectors spatial join on the road section of Aimsun in the correct format.\n",
    "See Aimsun team for that.\n",
    "\n",
    "1. Create line to detectors with Geopandas\n",
    "Join spatial the detectors to the closest road section in streetline shapefile in street_line_folder and create the doc line_to_detectors.csv where the headings is:\n",
    "OBJECTID, Shape_Length, Name, Direction, 2013, 2015, 2017, 2019, PeMS\n",
    "\n",
    "One row = one road section of the streetline shapefile with the corresponding id of the closest detectors for 2013, 2015, 2017, 2019 or PeMS detectors.\n",
    "One road section can be matched to several detectors (or no detectors).\n",
    "\n",
    "2. Create the flow_processed_section.csv \n",
    "This has been partially done in put years together.ipynb.\n",
    "One row = one road section\n",
    "One column = the data at one time step of 15 minutes for one specific year and one specific day and one specific time step.\n",
    "Create the file using both the line_to_detectors.csv file created in 1 and the process_flow_city.csv and process_flow_pems.csv in auxilary_folder.\n",
    "\n",
    "3. Ask Theo to do the PCA\n",
    "See the PCA notebook.\n",
    "Visualization with Kepler.gl\n",
    "\n",
    "4. Ask Theo to test the different work done so far\n",
    "Compare the resutls of the different spatial joins to manually made spatial join in ArcGIS. Both for detectors to Aimsun and detectors to street line.\n",
    "\n",
    "5. Learn how to modify a shapefile in order to change the IDs of the detectors in the shapefile\n",
    "Reorder the IDs of the detectors to have 2013, 2015, 2017 and 2019.\n",
    "\n",
    "6. List of PeMS detectors using project delimitation shapefile\n",
    "\n",
    "7. Internal centroids connections creation with geopandas in the demand pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
