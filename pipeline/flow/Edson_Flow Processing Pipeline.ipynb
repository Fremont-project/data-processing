{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the pipeline\n",
    "\n",
    "<font color=red> The goal of this notebook is to associate the raw ADT ground-data for 2013, 2015, 2017 and 2019, and the raw speed ground-data in 2015 to the Aimsun network for the calibration of Aimsun simulations. First the raw data is processed in a common format, then, every detector is associated with a network link inside Aimsun. Later, every detector is associated with a road section to create heatmap and understand the evolution of flows over years. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: write this in a better way\n",
    "### Outputs of the pipeline: \n",
    "- One file matching detectors location to Aimsun road section\n",
    "- One file with the processed flow data for 2013, 2015, 2017, 2019\n",
    "- One file with the processed speed data for 2015\n",
    "- One file with the flow data corresponding to road sections for 2013, 2015, 2017 and 2019\n",
    "- PCA on flow data\n",
    "\n",
    "### Inputs of the pipeline: \n",
    "**Raw data**\n",
    "- PeMS account [_publicly available_]. Can be created in http://pems.dot.ca.gov\n",
    "- PeMS detectors location [_publicly available_]. In the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/PeMS`\n",
    "- City average annual daily traffic (AADT) data for 2013, 2017 and 2019 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/City`\n",
    "- Kimley-Horn flow and speed data for 2015 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/Kimley\\ Horn\\ Data`\n",
    "\n",
    "**Manually made dataset**\n",
    "- Aimsun network\n",
    "- Detectors location\n",
    "- Road section layer\n",
    "- Doc files or city ADT data corresponding to the PDF files\n",
    "- Detectors ID to corresponding flow file name\n",
    "\n",
    "\n",
    "### Temporary files of the pipeline\n",
    "- CSV flow data\n",
    "    - City and Kimley Horn\n",
    "    - PeMS data\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent scripts: \n",
    "- [pems_download.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pems_download.py): the script automatically download PeMs data for chosen date.\n",
    "- [pre_process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pre_process_flow.py): the script (A). parse the ADT data from xlsx, doc and csv files to csv files (B). Find the coordinates of the city detectors (C). Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS \n",
    "- [process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/process_flow.py): this script processes all the flow traffic data into one big CSV file from both city and PeMS data files\n",
    "- [process_years_together.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/): this script combines the previous scripts and processes the data for 2013, 2015, 2017, 2019\n",
    "- fremontdropbox_flow\n",
    "\n",
    "### Dependent libraries:\n",
    "- os\n",
    "- sys\n",
    "- webbrowser\n",
    "- time\n",
    "- requests\n",
    "- pathlib\n",
    "- textract\n",
    "- [numpy](https://numpy.org)\n",
    "- datetime\n",
    "- pandas\n",
    "- To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work done by the script\n",
    "1. Obtaining Data\n",
    "    - Turn city pdf to doc\n",
    "    - PeMS data download\n",
    "2. Preprocessing city and Kimley-Horn data\n",
    "    - Parsing the flow data [We are here]\n",
    "    - Parsing the speed data\n",
    "    - Geocoding location of the detectors\n",
    "    - Manually update the location of the detectors\n",
    "3. Processing the data\n",
    "    - Creating one file with the flow for all detectors\n",
    "    - Creating a layer of road section\n",
    "    - Matching the detectors to road section\n",
    "    - Creating one file with the flow over time for every road section\n",
    "    - Matching the detectors with the corresponding road section in Aimsun\n",
    "4. Exporting the data\n",
    "    - Exporting a csv file where Aimsun road sections are matched with detectors\n",
    "    - Exporting a csv file with flow for the different road sections\n",
    "    - Exporting traffic flow heatmap\n",
    "    - Exporting speed heatmap\n",
    "    - PCA on the traffic flow heatmap over years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Jiayi, put the to dos in green when done </font>, <font color=blue> in blue if you need some help </font>, <font color=red>put them red if you have not done them yet</font>\n",
    "- should have exception handler in the process (and python function) if the files are not located where you are looking for.\n",
    "- run PCA and create heatmap with ArcGIS (heatmap will be done by Theo)\n",
    "- put the correct github link for the scripts in the Dependent scripts paragraph\n",
    "- Work done by the script\n",
    "- Dependent libraries\n",
    "- Put the scripts together in one main script\n",
    "- add link from work done by the script to section of the iPython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fremontdropbox import get_dropbox_location\n",
    "\n",
    "dropbox_dir = get_dropbox_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining Data\n",
    "\n",
    "### A) PeMS Data Download \n",
    "    \n",
    "Script: pems_download.py <br>\n",
    "Download traffic data from the PEMS website (pews.dot.ca.gov) for years 2013, 2015, 2017 and 2019 <br>\n",
    "\n",
    "We download the data calling the method download(detector_ids, year, PeMS_dir) from the pems_download.py file. The process takes about 20 minutes to run. \n",
    "Run `print(help(pems.download))` to get some help.\n",
    "\n",
    "### B) Turn city pdf to doc\n",
    "\n",
    "Some of the flow data that we got from the city were in pdf files. To be able to parse them, we convert them to doc files using online website. This should be done before running the code. The doc files are inputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download in module pems_download:\n",
      "\n",
      "download(year, detector_ids, PeMS_dir)\n",
      "    This function downloads traffic data from the PeMS website (pems.dot.ca.gov).\n",
      "    This function has for input:\n",
      "        - PeMS detectors ID: detector_ids (an array of detectors)\n",
      "        - Year for the desired data: year (one year as a integer, should be 2013, 2015, 2017 or 2019)\n",
      "    \n",
      "    This function has for output:\n",
      "        - All corresponding PeMS detectors data file for the given year (and the given days encoded in the url).\n",
      "        - Stored in the download folder as PeMS_dir/PeMS_year/PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
      "        One xlsx file has two sheets:\n",
      "            - PeMS Report Description\n",
      "            - Report Data\n",
      "                - Contains the traffic flow data\n",
      "                - Each row gives the number of vehicles observed in one time step (5 minutes) per lane number over the columns.\n",
      "                - The first column gives the date and time stamp, and the columns that follow are lanes (i.e. Lane 1 Flow, Lane 2 Flow). We care about the column 'Flow (Veh/5 Minutes)' which is the total flow for every lane every 5 minutes.\n",
      "                - The column \"% Observed\" correspond to how much of the flow is due to real vehicles sensed or due to estimation from other days due to a technical issue that make the sensor not sensing every cars.\n",
      "        - Flow data are download for three days\n",
      "            - From the first Tuesday of March at 00:00am until the first thursday of March at 23:59pm\n",
      "    For the function to work, you need to:\n",
      "        - Log in to PeMS in the same browser that runs this Jupyter notebook\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pems_download as pems\n",
    "print(help(pems.download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* IMPORTANT *************\n",
    "# --> For this cell to work, you need to log in to PeMS in the same browser that runs this Jupyter notebook\n",
    "\n",
    "## The IDs of the PeMS detectors where obtained using ArcGIS software and an input file, pems_detectors.csv, containing the locations of all the PeMS dectectors in California\n",
    "## this should be done in Python!\n",
    "detector_ids = [403250, 403256, 403255, 403257, 418387, 418388, 400376,\n",
    "               413981, 413980, 413982, 402794, 413983, 413984, 413985,\n",
    "               413987, 413986, 402796, 413988, 402799, 403251, 403710,\n",
    "               403254, 403719, 400566, 418420, 418419, 418422, 418423,\n",
    "               402793, 403226, 414015, 414016, 402795, 402797, 414011,\n",
    "               402798]\n",
    "PeMS_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMS'\n",
    "\n",
    "# pems.download(2013, detector_ids, PeMS_dir)\n",
    "# pems.download(2015, detector_ids, PeMS_dir)\n",
    "# pems.download(2017, detector_ids, PeMS_dir)\n",
    "# pems.download(2019, detector_ids, PeMS_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO Later:\n",
    "- Find the list of detectors ID using the project delimitation shapefile. (to be done later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeMS during 03/05/2013 00:00:00-03/07/2013 23:59:59\n",
      "PeMS during 03/03/2015 00:00:00-03/05/2015 23:59:59\n",
      "PeMS during 03/07/2017 00:00:00-03/09/2017 23:59:59\n",
      "PeMS during 03/05/2019 00:00:00-03/07/2019 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# ************* TO DO *************\n",
    "pems.PeMS_tester(2013, PeMS_dir, True)\n",
    "pems.PeMS_tester(2015, PeMS_dir, True)\n",
    "pems.PeMS_tester(2017, PeMS_dir, True)\n",
    "pems.PeMS_tester(2019, PeMS_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing city data\n",
    "\n",
    "\n",
    "### A) Parse city and Kimley Horn flow data from xlsx, doc and csv files to csv files\n",
    "\n",
    "Here, we process the Excel and PDF ADT data files (city data) into CSV files.\n",
    "\n",
    "- run `print(help(pre_process.parse_excel_2013)` to get some help for 2013 excel files\n",
    "- run `print(help(pre_process.parse_excel_2015)` to get some help for 2015 excel files\n",
    "- run `print(help(pre_process.parse_excel_2017)` to get some help for 2017 excel files\n",
    "- run `print(help(pre_process.parse_excel_2019)` to get some help for 2019 excel files\n",
    "- run `print(help(pre_process.parse_adt_as_file)` to get some help for the 2017 and 2019 doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function process_adt_data in module pre_process_flow:\n",
      "\n",
      "process_adt_data(year, Processed_dir, Input_dir)\n",
      "    This function processes the Excel and PDF ADT data files (city data) into CSV files. Note that one file corresponds to one main road and the traffic flow data recordings in it.\n",
      "    \n",
      "    This function has input:\n",
      "        - year which takes values 2013, 2015, 2017 or 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    The function has output:\n",
      "        - CSV files located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pre_process_flow as pre_process\n",
    "print(help(pre_process.process_adt_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Re do the parsing such that all the files look the same. The time format should be the same everywhere.\n",
    "- Remove the unnecessary functions in pre_process\n",
    "- <font color=green> Write test to check if the parsing of doc file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed'\n",
    "Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Flow_processed'\n",
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n",
    "local_testing = '/Users/LiJiayi/Documents'\n",
    "#pre_process.process_adt_data(2013, Processed_dir, City_dir)\n",
    "#pre_process.process_adt_data(2015, Processed_dir, Kimley_Horn_flow_dir)\n",
    "#pre_process.process_adt_data(2017, Processed_dir, City_dir)\n",
    "#pre_process.process_adt_data(2019, Processed_dir, City_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc files are all good!\n"
     ]
    }
   ],
   "source": [
    "#doc tests\n",
    "\n",
    "Input_dir = City_dir\n",
    "for year in [2017, 2019]:\n",
    "    input_folder_doc = Input_dir + \"/\" + \"%d doc/\" % year\n",
    "    input_file_docs = os.listdir(input_folder_doc)\n",
    "    for file_name in input_file_docs:\n",
    "        _, file_ext = os.path.splitext(file_name)\n",
    "        if file_ext == '.doc':\n",
    "            pre_process.doc_file_testing(input_folder_doc+file_name, year)\n",
    "print(\"Doc files are all good!\")\n",
    "#pre_process.doc_file_testing(file_path, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary files generater:\n",
    "pre_process.google_doc_generater(Processed_dir)\n",
    "pre_process.flow_processed_generater(Processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B) Find the coordinates of the city detectors\n",
    "We obtained the coordinates of the detectors by calling the method get_geo_data(year) from the pre_process.py python file. \n",
    "\n",
    "- run `print(help(pre_process.get_geo_data))` to get some help.\n",
    "\n",
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green>Put the different files year_info.csv together, and compare it to the [google doc] (Done)(https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0) </font>\n",
    "- <font color=green> Make sure to process doc files (Done) </font>\n",
    "- <font color=green> Make sure to process PeMS files too (for this ask Theo)</font>\n",
    "- <font color=blue> Then we want to match the detectors to the ID associated to them here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process.get_geo_data(2013, City_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2015, Kimley_Horn_flow_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2017, City_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2019, City_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 - Grimmer Blvd - Paseo Padre Pkwy to Osgood Rd.xls\n",
      "83 - Mission Blvd - Pine St to Durham Rd.xls\n",
      "135 - Warren Avenue - Curtner Road to Warm Springs Boulevard.xls\n",
      "44 - Durham Rd - I-680 to Mission Blvd.xls\n",
      "82 - Mission Blvd - St Joseph Terr to Pine St.xls\n",
      "140 - Washington Boulevard - Paseo Padre Pkwy to Mission Blvd.xls\n",
      "110 - Paseo Padre Pkwy - Mission Blvd to Curtner Rd.xls\n",
      "106 - Paseo Padre Pkwy - Driscoll Rd to Washington Blvd - Resurvey.xls\n",
      "84 - Mission Blvd - Durham Rd to Curtner Rd.xls\n",
      "107 - Paseo Padre Pkwy - Washington Blvd to Durham Rd - Resurvey.xls\n",
      "109 - Paseo Padre Pkwy - Onondaga Wy to Mission Blvd.xls\n",
      "108 - Paseo Padre Pkwy - Durham Rd to Onondaga Wy.xls\n",
      "81 - Mission Blvd - Mission Rd to St Joseph Terrace.xls\n",
      "139 - Washington Boulevard - Driscoll Road to Paseo Padre Pkwy.xls\n",
      "61 - South Grimmer Blvd - Osgood Rd to Fremont Blvd.xls\n",
      "Raw Data\n"
     ]
    }
   ],
   "source": [
    "# Preprocess 2015 speed data\n",
    "\n",
    "pre_process.Speed_data_parser(Kimley_Horn_flow_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Theo: I am here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS <br>\n",
    "Done in the software manually in ArcGIS.\n",
    "1. Export Aimsun network as GIS file\n",
    "2. Import Aimsun network in ArcGIS\n",
    "3. Import detectors in ArcGIS as XY_points\n",
    "4. Move detectors to put them on corresponding road in Aimsun\n",
    "5. Associate to every detectors the External ID of the Aimsun road (to be done again)\n",
    "\n",
    "**TO DO THEO**: Add the process to create the detectors inside Aimsun.\n",
    "Add the process to match the detectors to road section (and create the file lines_to_detectors.xlsx\n",
    "\n",
    "\n",
    "### Later to do: do the spatial join in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#from fremontdropbox_flow import get_dropbox_location\n",
    "dropbox_dir = get_dropbox_location()\n",
    "\n",
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed/ADT'\n",
    "PeMs_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMs'\n",
    "local_download =  str(os.path.join(Path.home(), \"Downloads\"))\n",
    "re_formated_Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Temporary exports to be copied to processed data/Flow_processed'\n",
    "#directory to year reformat\n",
    "City_dir = Processed_dir + \"/\" + \"City\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Create the file Flow_processed_tmp.csv from year_info.csv to check if everything is fine [Done]</font>\n",
    "- <font color=green> Rename the file Flow_processed_tmp.csv -> detectors_id.csv and put it in the manually made dataset [Partially done] </font>\n",
    "- <font color=blue> Merge flow_processed_city.csv and flow_processed_PeMS.csv together in the function </font> \n",
    "- <font color=red> Go over Zixuan code to make the code in process_flow better </font>\n",
    "- <font color=green>Processs speed data for 2015 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Process the csv files (city + caltrans) to one big file. <br>\n",
    "source code: processing flow to one CSV.ipynb <br>\n",
    "\n",
    "We combine all the flow traffic data into one big CSV file from both city and PeMS data files. This is done by calling the function process_data() from the process_flow.py python file. \n",
    "\n",
    "Description of function process_data()\n",
    "<br>\n",
    "The function has input:\n",
    "- \"Flow_processed_tmp.csv\" file that lists all the processed files from city and PeMS data\n",
    "- The processed files created from the Parsing Data section\n",
    "\n",
    "The function has output:\n",
    "- \"Flow_processed_city.csv\" containing combined city flow data for all year\n",
    "- \"Flow_processed_PeMS.csv\" containing combined PeMS flow data for all years\n",
    "\n",
    "For the function to work:\n",
    "- The processed files (input) must be located in City and PeMs folders\n",
    "- 2013 city processed files are located in \"City/2013 reformat/\"\n",
    "- 2017 and 2019 city processed files that originated from DOC (which originated from PDF) files are located in \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019\n",
    "- 2017 and 2019 city processed files that originated from Excel files are located in \"City/Year reformat/Format from xlsx\" folder where Year=2017 or 2019\n",
    "- 2013, 2017 and 2019 PeMS data files are located in \"PeMS_Year\" folder where Year=2013, 2017 or 2019\n",
    "\n",
    "For the pipeline to work:\n",
    "- The ouput files must remain in the working directory, no moving necessary.\n",
    "\n",
    "Structure of ouput files: \n",
    "- Flow_processed_city.csv\n",
    "    - contains city traffic data where the rows represent traffic flow. The first 5 columns give info about the traffic flow and are Year, Name, Id, Direction, Day 1 where Name refers to the file name from which the data originated, Id is the Id from the \"Flow_processed_tmp.csv\" file, Direction is the direction of flow and Day 1 is the start date of recording. The columns that follow are day-timesteps for flow data. There are 3 days total over which traffic flow is recorded and time progresses in 15 minute steps. Hence the data columns progress as \"Day 1 - 0:0\", \"Day 1 - 0:15\", \"Day 1 - 0:30\",...,\"Day 3 - 23:30\", \"Day 3 - 23:45\".\n",
    "- Flow_processed_PeMS.csv\n",
    "    - contains PeMS flow traffic data where the rows represent traffic flow. The first columns are Name, Id and Name PeMS where Name contains the PeMS detector Id, Id is the Id assigned from \"Flow_processed_tmp.csv\", Name PeMs is the road address. The next 6 columns give Observed Year and Day Year for the 3 years, 2013, 2017 and 2019. Observed Year is the percentage of the observed data and Day Year is the start date of recording. The columns that follow are Year-Day-timestep, there are 3 years, 3 days and time progresses in 15 minute steps. Hence the columns progress as \"2013-Day 1 - 0:0\", \"2013-Day 1 - 0:15\", \"2013-Day 1 - 0:30\",...,\"2019-Day 3 - 23:30\", \"2019-Day 3 - 23:45\".\n",
    "\n",
    "**(DONE) TO DO 8**: Explain the structure of the output files. Also, feel free to document the doc in the python file (or iPython file). Explain also the input (Flow_processed_tmp.csv) and how it was created (I think it was created from the google doc https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0).\n",
    "\n",
    "***Edson Question***: the Flow_processed_tmp.csv file and the google doc seem the same to me (except for the lat, lng info on the right side of the google doc). Beyond this, I don't know how the file was created. Who to ask for more info?\n",
    "\n",
    "***Theo Answer***: I guess I have created the file from the google doc. But I have also created the google doc during the parsing. Probably in 2) we can create Flow_processed_tmp.csv from year_info.csv\n",
    "\n",
    "# added 2015 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_flow as pf\n",
    "pf.process_data(Processed_dir, re_formated_Processed_dir, PeMs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Create file that gives traffic flows for specific road sections for every year. <br>\n",
    "source note: put years together.ipynb\n",
    "\n",
    "- Use detectors (lines_to_detectors.csv) and flow processed city (flow_processed_city.csv) data to create all years flow data in \"flow_processed_section.csv\"\n",
    "\n",
    "\n",
    "# To do 9: the function pytogether should be written again. + add 2015\n",
    "- add the PeMS data\n",
    "- be more clever about missing data or road section associated with several detectors (take the average)\n",
    "\n",
    "### To do later, do the spatial join in python\n",
    "### Edson Spatial Join: done with geopandas and visualization with Kepler\n",
    "Some notes\n",
    "- The spatial join was not done with geopandas.sjoin as its 3 options for joining operations (interset, within and contain) do not result in joining a detector to the closest road segment. In fact, the joinings give no results. This is due to the joining operations being a joined between some form of intersection of geometries. For example an are defined by a polygon containing a set of points. Our geometry however is that of Linestring (road sections) and Points (detectors) and hence there is no geometrical intersection between the two. https://geopandas.org/mergingdata.html\n",
    "- The spatial join was done by iterating through the detectors (points) and finding the closest road section (Linestring or Line) to it. The shortest distance between a point and a line is done using geopandas data structures, that is a point is a geoseries with a method called distance that takes as input another geoseries (ie. Linestring). So distance = point.distance(line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plot\n",
    "from shapely import wkt\n",
    "import keplergl as kp\n",
    "import spatial_join as sj\n",
    "\n",
    "# # directories to detectors and aimsum sections shape files\n",
    "# # start from current working directory (ie. same folder as notebook)\n",
    "dir_detectors = 'detectors'\n",
    "dir_sections = 'aimsum'\n",
    "\n",
    "# # find closest road segment for each detector\n",
    "# # results written to a csv per year named: detetctors_to_road_segments_year.csv\n",
    "sj.detectors_to_road_segments(2013, dir_sections, dir_detectors)\n",
    "sj.detectors_to_road_segments(2015, dir_sections, dir_detectors)\n",
    "sj.detectors_to_road_segments(2017, dir_sections, dir_detectors)\n",
    "sj.detectors_to_road_segments(2019, dir_sections, dir_detectors)\n",
    "\n",
    "# # Find duplicates. That is, multiple detectors are closest to one road, if any.\n",
    "# # results if any written to a csv per year named: duplicates_year.csv\n",
    "sj.find_duplicates(2013, dir_sections)\n",
    "sj.find_duplicates(2015, dir_sections)\n",
    "sj.find_duplicates(2017, dir_sections)\n",
    "sj.find_duplicates(2019, dir_sections)\n",
    "\n",
    "kepler_map = sj.create_kepler_map(dir_sections, dir_detectors)\n",
    "kepler_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_years_together as pytogether\n",
    "\n",
    "line_to_detectors = 'lines_to_detectors.csv'\n",
    "flow_processed_city = 'Flow_processed_city.csv'\n",
    "pytogether.run(line_to_detectors, flow_processed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Analysis\n",
    "\n",
    "# TO DO: To be done after the other to dos has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Analyse PCA results using heatmap inside ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
