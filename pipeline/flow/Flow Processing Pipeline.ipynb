{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the pipeline\n",
    "\n",
    "<font color=red> The goal of this notebook is to associate the raw ADT ground-data for 2013, 2015, 2017 and 2019, and the raw speed ground-data in 2015 to the Aimsun network for the calibration of Aimsun simulations. First the raw data is processed in a common format, then, every detector is associated with a network link inside Aimsun. Later, every detector is associated with a road section to create heatmap and understand the evolution of flows over years. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: write this in a better way\n",
    "### Outputs of the pipeline: \n",
    "- One file matching detectors location to Aimsun road section\n",
    "- One file with the processed flow data for 2013, 2015, 2017, 2019\n",
    "- One file with the processed speed data for 2015\n",
    "- One file with the flow data corresponding to road sections for 2013, 2015, 2017 and 2019\n",
    "- PCA on flow data\n",
    "\n",
    "### Inputs of the pipeline: \n",
    "**Raw data**\n",
    "- PeMS account [_publicly available_]. Can be created in http://pems.dot.ca.gov\n",
    "- PeMS detectors location [_publicly available_]. In the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/PeMS`\n",
    "- City average annual daily traffic (AADT) data for 2013, 2017 and 2019 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/City`\n",
    "- Kimley-Horn flow and speed data for 2015 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/Kimley\\ Horn\\ Data`\n",
    "\n",
    "**Manually made dataset**\n",
    "- Aimsun network\n",
    "- Detectors location\n",
    "- Road section layer\n",
    "- Doc files or city ADT data corresponding to the PDF files\n",
    "- Detectors ID to corresponding flow file name\n",
    "\n",
    "\n",
    "### Temporary files of the pipeline\n",
    "- CSV flow data\n",
    "    - City and Kimley Horn\n",
    "    - PeMS data\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent scripts: \n",
    "- [pems_download.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pems_download.py): the script automatically download PeMs data for chosen date.\n",
    "- [pre_process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pre_process_flow.py): the script (A). parse the ADT data from xlsx, doc and csv files to csv files (B). Find the coordinates of the city detectors (C). Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS \n",
    "- [process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/): this script processes all the flow traffic data into one big CSV file from both city and PeMS data files\n",
    "- [process_years_together.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/): this script combines the previous scripts and processes the data for 2013, 2015, 2017, 2019\n",
    "- fremontdropbox_flow\n",
    "\n",
    "### Dependent libraries:\n",
    "- os\n",
    "- sys\n",
    "- webbrowser\n",
    "- time\n",
    "- requests\n",
    "- pathlib\n",
    "- textract\n",
    "- [numpy](https://numpy.org)\n",
    "- datetime\n",
    "- pandas\n",
    "- To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work done by the script\n",
    "1. Obtaining Data\n",
    "    - Turn city pdf to doc\n",
    "    - PeMS data download\n",
    "2. Preprocessing city and Kimley-Horn data\n",
    "    - Parsing the flow data [We are here]\n",
    "    - Parsing the speed data\n",
    "    - Geocoding location of the detectors\n",
    "    - Manually update the location of the detectors\n",
    "3. Processing the data\n",
    "    - Creating one file with the flow for all detectors\n",
    "    - Creating a layer of road section\n",
    "    - Matching the detectors to road section\n",
    "    - Creating one file with the flow over time for every road section\n",
    "    - Matching the detectors with the corresponding road section in Aimsun\n",
    "4. Exporting the data\n",
    "    - Exporting a csv file where Aimsun road sections are matched with detectors\n",
    "    - Exporting a csv file with flow for the different road sections\n",
    "    - Exporting traffic flow heatmap\n",
    "    - Exporting speed heatmap\n",
    "    - PCA on the traffic flow heatmap over years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Jiayi, put the to dos in green when done </font>, <font color=blue> in blue if you need some help </font>, <font color=red>put them red if you have not done them yet</font>\n",
    "- should have exception handler in the process (and python function) if the files are not located where you are looking for.\n",
    "- run PCA and create heatmap with ArcGIS (heatmap will be done by Theo)\n",
    "- put the correct github link for the scripts in the Dependent scripts paragraph\n",
    "- Work done by the script\n",
    "- Dependent libraries\n",
    "- Put the scripts together in one main script\n",
    "- add link from work done by the script to section of the iPython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fremontdropbox import get_dropbox_location\n",
    "\n",
    "dropbox_dir = get_dropbox_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining Data\n",
    "\n",
    "### A) PeMS Data Download \n",
    "    \n",
    "Script: pems_download.py <br>\n",
    "Download traffic data from the PEMS website (pews.dot.ca.gov) for years 2013, 2015, 2017 and 2019 <br>\n",
    "\n",
    "We download the data calling the method download(detector_ids, year, PeMS_dir) from the pems_download.py file. The process takes about 20 minutes to run. \n",
    "Run `print(help(pems.download))` to get some help.\n",
    "\n",
    "### B) Turn city pdf to doc\n",
    "\n",
    "Some of the flow data that we got from the city were in pdf files. To be able to parse them, we convert them to doc files using online website. This should be done before running the code. The doc files are inputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download in module pems_download:\n",
      "\n",
      "download(year, detector_ids, PeMS_dir)\n",
      "    This function downloads traffic data from the PeMS website (pems.dot.ca.gov).\n",
      "    This function has for input:\n",
      "        - PeMS detectors ID: detector_ids (an array of detectors)\n",
      "        - Year for the desired data: year (one year as a integer, should be 2013, 2015, 2017 or 2019)\n",
      "    \n",
      "    This function has for output:\n",
      "        - All corresponding PeMS detectors data file for the given year (and the given days encoded in the url).\n",
      "        - Stored in the download folder as PeMS_dir/PeMS_year/PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
      "        One xlsx file has two sheets:\n",
      "            - PeMS Report Description\n",
      "            - Report Data\n",
      "                - Contains the traffic flow data\n",
      "                - Each row gives the number of vehicles observed in one time step (5 minutes) per lane number over the columns.\n",
      "                - The first column gives the date and time stamp, and the columns that follow are lanes (i.e. Lane 1 Flow, Lane 2 Flow). We care about the column 'Flow (Veh/5 Minutes)' which is the total flow for every lane every 5 minutes.\n",
      "                - The column \"% Observed\" correspond to how much of the flow is due to real vehicles sensed or due to estimation from other days due to a technical issue that make the sensor not sensing every cars.\n",
      "        - Flow data are download for three days\n",
      "            - From the first Tuesday of March at 00:00am until the first thursday of March at 23:59pm\n",
      "    For the function to work, you need to:\n",
      "        - Log in to PeMS in the same browser that runs this Jupyter notebook\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pems_download as pems\n",
    "print(help(pems.download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* IMPORTANT *************\n",
    "# --> For this cell to work, you need to log in to PeMS in the same browser that runs this Jupyter notebook\n",
    "\n",
    "## The IDs of the PeMS detectors where obtained using ArcGIS software and an input file, pems_detectors.csv, containing the locations of all the PeMS dectectors in California\n",
    "## this should be done in Python!\n",
    "detector_ids = [403250, 403256, 403255, 403257, 418387, 418388, 400376,\n",
    "               413981, 413980, 413982, 402794, 413983, 413984, 413985,\n",
    "               413987, 413986, 402796, 413988, 402799, 403251, 403710,\n",
    "               403254, 403719, 400566, 418420, 418419, 418422, 418423,\n",
    "               402793, 403226, 414015, 414016, 402795, 402797, 414011,\n",
    "               402798]\n",
    "PeMS_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMS'\n",
    "\n",
    "# pems.download(2013, detector_ids, PeMS_dir)\n",
    "# pems.download(2015, detector_ids, PeMS_dir)\n",
    "# pems.download(2017, detector_ids, PeMS_dir)\n",
    "# pems.download(2019, detector_ids, PeMS_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO Later:\n",
    "- Find the list of detectors ID using the project delimitation shapefile. (to be done later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeMS during 03/05/2013 00:00:00-03/07/2013 23:59:59\n",
      "PeMS during 03/03/2015 00:00:00-03/05/2015 23:59:59\n",
      "PeMS during 03/07/2017 00:00:00-03/09/2017 23:59:59\n",
      "PeMS during 03/05/2019 00:00:00-03/07/2019 23:59:59\n"
     ]
    }
   ],
   "source": [
    "# ************* TO DO *************\n",
    "pems.PeMS_tester(2013, PeMS_dir, True)\n",
    "pems.PeMS_tester(2015, PeMS_dir, True)\n",
    "pems.PeMS_tester(2017, PeMS_dir, True)\n",
    "pems.PeMS_tester(2019, PeMS_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing city data\n",
    "\n",
    "\n",
    "### A) Parse city and Kimley Horn flow data from xlsx, doc and csv files to csv files\n",
    "\n",
    "Here, we process the Excel and PDF ADT data files (city data) into CSV files.\n",
    "\n",
    "- run `print(help(pre_process.parse_excel_2013)` to get some help for 2013 excel files\n",
    "- run `print(help(pre_process.parse_excel_2015)` to get some help for 2015 excel files\n",
    "- run `print(help(pre_process.parse_excel_2017)` to get some help for 2017 excel files\n",
    "- run `print(help(pre_process.parse_excel_2019)` to get some help for 2019 excel files\n",
    "- run `print(help(pre_process.parse_adt_as_file)` to get some help for the 2017 and 2019 doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function process_adt_data in module pre_process_flow:\n",
      "\n",
      "process_adt_data(year, Processed_dir, Input_dir)\n",
      "    This function processes the Excel and PDF ADT data files (city data) into CSV files. Note that one file corresponds to one main road and the traffic flow data recordings in it.\n",
      "    \n",
      "    This function has input:\n",
      "        - year which takes values 2013, 2015, 2017 or 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    The function has output:\n",
      "        - CSV files located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pre_process_flow as pre_process\n",
    "print(help(pre_process.process_adt_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Re do the parsing such that all the files look the same. The time format should be the same everywhere.\n",
    "- Remove the unnecessary functions in pre_process\n",
    "- <font color=green> Write test to check if the parsing of doc file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed'\n",
    "Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Flow_processed'\n",
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n",
    "\n",
    "# pre_process.process_adt_data(2013, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2015, Processed_dir, Kimley_Horn_flow_dir)\n",
    "# pre_process.process_adt_data(2017, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2019, Processed_dir, City_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B) Find the coordinates of the city detectors\n",
    "We obtained the coordinates of the detectors by calling the method get_geo_data(year) from the pre_process.py python file. \n",
    "\n",
    "- run `print(help(pre_process.get_geo_data))` to get some help.\n",
    "\n",
    "# <font color=red> TO DO: </font>\n",
    "- Put the different files year_info.csv together, and compare it to the [google doc] (Done)(https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0)\n",
    "- Make sure to process doc files (Done)\n",
    "- Make sure to process PeMS files too (for this ask Theo)\n",
    "- Then we want to match the detectors to the ID associated to them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_process.get_geo_data(2013, City_dir, Processed_dir)\n",
    "#pre_process.get_geo_data(2015, Kimley_Horn_flow_dir, Processed_dir)\n",
    "# pre_process.get_geo_data(2017, City_dir, Processed_dir)\n",
    "# pre_process.get_geo_data(2019, City_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_info_coor.csv\n",
      "2017_info_coor.csv\n",
      "flow_out.csv\n",
      "2019_info_coor.csv\n",
      "2015_info_coor.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_filenames = []\n",
    "for filename in os.listdir(Processed_dir):\n",
    "    _, file_ext = os.path.splitext(filename)\n",
    "    if file_ext == '.csv':\n",
    "        print(filename)\n",
    "        all_filenames.append(filename)\n",
    "combined_csv = pd.concat([pd.read_csv(Processed_dir + \"/\" + str(f)) for f in all_filenames])\n",
    "combined_csv.to_csv(Processed_dir + \"/\" + \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't rerun the following cell!!! The rewriting is not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script go over year_info_coor.csv files and concatenate them into one large csv in consistent format\n",
    "#flow_out.csv is the same as the google doc \n",
    "debug = False\n",
    "fout=open(Processed_dir + \"/\" + \"flow_out.csv\", \"a\")\n",
    "id_counter = 1\n",
    "for filename in os.listdir(Processed_dir):\n",
    "    file_name, file_ext = os.path.splitext(filename)\n",
    "    if file_ext == '.csv' and file_name.split(\"_\")[0][0:2] == '20':\n",
    "        curr_year = filename.split(\"_\")[0]\n",
    "        curr_opening = \"./\" + curr_year + \" ADT Data\"\n",
    "        f=open(Processed_dir + \"/\" + filename)\n",
    "        for line in f:\n",
    "            first_ele = line.split(\",\")[0]\n",
    "            if first_ele == 'Name': #This is header\n",
    "                fout.write('\\n')\n",
    "                header = line[len(first_ele)+1:]\n",
    "                header = \"Id,\" + curr_opening + \",\" + header\n",
    "                if debug == True:\n",
    "                    print(header)\n",
    "                fout.write(header)\n",
    "                continue\n",
    "            line = str(id_counter) + \",\" + line + ''\n",
    "            id_counter += 1\n",
    "            if debug == True:\n",
    "                print(line)\n",
    "            fout.write(line)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,./2013 ADT Data\n",
      "1,Warm Springs Blvd betw. Warren & Kato.xlsx\n",
      "2,Durham Rd.xlsx\n",
      "3,EB Washngton Blvd at Olive.xls\n",
      "4,WB Washington Blvd at Gallegos.xls\n",
      "5,EB Washington Blvd at Palm.xls\n",
      "6,Washington Blvd at Olive.xlsx\n",
      "7,Mission Blvd betw. Durham & I680.xlsx\n",
      "8,Driscoll Rd betw. PPP & Washington.xlsx\n",
      "9,Driscoll Rd betw. Mission & PPP.xlsx\n",
      "10,Washington Blvd at Castillejo.xlsx\n",
      "11,Washington Blvd betw. PPP & Mission.xlsx\n",
      "12,S. Grimmer Blvd betw PPP & Osgood.xlsx\n",
      "13,Osgood Blvd betw. Auto Mall & Grimmer.xlsx\n",
      "14,Paseo Padre Pkwy betw. S. Grimmer & Mission.xlsx\n",
      "15,Paseo Padre Pkwy betw. Durham & S. Grimmer.xlsx\n",
      "16,Mission Blvd betw. Washington & Pine.xlsx\n",
      "17,Warm Springs Blvd betw. Mission & Warren.xlsx\n",
      "18,EB Washington Blvd at Gallegos.xls\n",
      "19,Warren Ave betw. Warm Springs & I880.xlsx\n",
      "20,Paseo Padre Pkwy betw. Mission & Curtner.xlsx\n",
      "21,Mission Blvd betw. Pine & Durham.xlsx\n",
      "22,Paseo Padre Pkwy betw. Washington & Durham.xlsx\n",
      "23,S. Grimmer Blvd betw. Osgood & Fremont.xlsx\n",
      "24,Osgood Rd betw. Washington & Auto Mall.xlsx\n",
      "25,Warren Ave betw. Curtner & Warm Springs.xlsx\n",
      "26,Warm Springs Blvd betw. S. Grmmer & Mission.xlsx\n",
      "27,Washington Blvd betw. Union & Driscoll.xlsx\n",
      "28,Mission Blvd betw. I680 & I880.xlsx\n",
      "29,Mission Blvd betw. I680 & Washington.xlsx\n",
      "30,Mission Blvd betw. Driscoll & I680.xlsx\n",
      "31,WB Washington Blvd at Ellsworth.xls\n",
      "32,Auto Mall Pkwy betw. Fremont & I680.xlsx\n",
      "Id,./2017 ADT Data\n",
      "33,MISSION BLVD BT DURHAM RD AND CURTNER RD.xlsx\n",
      "34,PASEO PADRE PKWY BT CAMINO DEL CAMPO AND PINE ST.xlsx\n",
      "35,GRIMMER BLVD BT OSGOOD RD AND FREMONT BLVD.xlsx\n",
      "36,DRISCOLL RD BT COMAC TERRACE AND DENISE ST.xlsx\n",
      "37,PAGE AVE BT KATO RD AND MILMONT DR.xlsx\n",
      "38,WARREN AVE BT CURTNER RD AND WARM SPRINGS BLVD.xlsx\n",
      "39,PASEO PADRE PKWY BT MISSION BLVD AND CURTNER RD.xlsx\n",
      "40,PINE ST E OF PASEO PADRE PKWY STOP SIGN.xlsx\n",
      "41,GRIMMER BLVD BT PASEO PADRE PKWY AND OSGOOD RD.xlsx\n",
      "42,MISSION BLVD S OF MISSION RD SIGNAL.xlsx\n",
      "43,FARWELL DR BT BROPHY DR AND FLAMINGO LN.xlsx\n",
      "44,WARM SPRINGS BLVD BT GRIMMER BLVD AND BROWN RD.xlsx\n",
      "45,WASHINGTON BLVD BT GALLEGOS AVE AND ADELINA CM.xlsx\n",
      "46,OSGOOD RD BT WASHINGTON BLVD AND AUTO MALL PKWY.xlsx\n",
      "47,PASEO PADRE PKWY BT ONONDAGA WAY AND MISSION BLVD.xlsx\n",
      "48,WASHINGTON BLVD W OF PASEO PADRE PKWY SIGNAL.xlsx\n",
      "49,OSGOOD RD BT AUTO MALL PKWY AND GRIMMER BLVD.xlsx\n",
      "50,PASEO PADRE PKWY BT PARKMEADOW DR AND BLACKFOOT DR.xlsx\n",
      "51,DRISCOLL RD BT HARRINGTON ST AND DURILLO DR.xlsx\n",
      "52,WARM SPRINGS BLVD BT BROWN RD AND WARREN AVE.xlsx\n",
      "53,AUTO MALL PKWY BT FREMONT BLVD AND I680.xlsx\n",
      "54,PASEO PADRE PKWY BT CHADBOURNE DR AND COVINGTON DR.xlsx\n",
      "55,PINE ST BT SOUTHERLAND WAY AND DUBAL CT.xlsx\n",
      "56,DURHAM RD BT I680 AND LAUREL CANYON WAY.xlsx\n",
      "57,MISSION BLVD S OF WASHINGTON BLVD SIGNAL.xlsx\n",
      "58,MISSION BLVD BT DRISCOLL RD AND I 680 NB.doc\n",
      "59,MISSION BLVD BT MOHAVE DR AND WARM SPRINGS BLVD EB.doc\n",
      "60,S GRIMMER BLVD BT PASEO PADRE PRKY AND MISSION BLVD EB.doc\n",
      "61,S GRIMMER BLVD BT PASEO PADRE PRKY AND MISSION BLVD WB.doc\n",
      "62,MISSION BLVD BT MOHAVE DR AND WARM SPRINGS BLVD WB.doc\n",
      "63,MISSION BLVD BT DRISCOLL RD AND I 680 SB.doc\n",
      "Id,./2019 ADT Data\n",
      "64,Paseo Padre Pkwy Bet. Washington Blvd & Durham Rd.xls\n",
      "65,Paseo Padre Pkwy Bet. Driscoll Rd & Washington Blvd.xls\n",
      "66,Washington Blvd Bet. Paseo Padre Pkwy & Mission Blvd.xls\n",
      "67,Washington Blvd Bet. Driscoll Rd & Paseo Padre Pkwy.xls\n",
      "68,Warm Springs Blvd Bet. Mission Blvd & Warren Ave.xls\n",
      "69,Paseo Padre Pkwy Bet. Durham Rd & S Grimmer Blvd.xls\n",
      "70,Paseo Padre Pkwy Bet. S Grimmer Blvd & Curtner Rd.xls\n",
      "71,Warrent Ave Bet. Navajo Rd & Curtner Rd.xls\n",
      "72,Driscoll Rd Bet. Paseo Padre Pkwy & Washington Blvd.xls\n",
      "73,Warm Springs Blvd Bet. S Grimmer Blvd & Warm Springs Ct.xls\n",
      "74,Osgood Rd Bet. Washtington & Auto Mall Pkwy.xls\n",
      "75,Warm Springs Blvd Bet. Warm Springs Ct & Mission Blvd.xls\n",
      "76,S Grimmer Blvd Bet. Osgood Rd & Fremont Blvd.xls\n",
      "77,Warre Ave Bet. Warm Springs Blvd & Navajo Rd.xls\n",
      "78,Driscoll Rd Bet. Mission Blvd & Paseo Padre Pkwy.xls\n",
      "79,S Grimmer Blvd Bet. Paseo Padre Pkwy & OSgood Rd.xls\n",
      "80,Osgood Rd Bet. Auto Mall Pkwy & S Grimmer Blvd.xls\n",
      "81,AUTO MALL PKWY BT FREMONT BLVD AND I-680 WB.doc\n",
      "82,MISSION BLVD BT WASHINGTON BLVD AND PINES ST NB.doc\n",
      "83,DURHAM RD BT I-680 AND MISSION BLVD EB.doc\n",
      "84,AUTO MALL PKWY BT FREMONT BLVD AND I-680 EB.doc\n",
      "85,MISSION BLVD BT PINE ST AND DURHAM RD SB.doc\n",
      "86,MISSION BLVD BT PASEO PADRE PKWY AND CURTNER RD SB.doc\n",
      "87,DURHAM RD BT I-680 AND MISSION BLVD WB.doc\n",
      "88,MISSION BLVD BT I-680 AND WASHINGTON BLVD NB.doc\n",
      "89,MISSION BLVD BT DURHAM RD AND PASEO PADRE PKWY SB.doc\n",
      "90,MISSION BLVD BT DRISCOLL RD AND I-680 NB.doc\n",
      "91,GRIMMER BLVD BT MISSION BLVD AND PASEO PADRE PKWY EB.doc\n",
      "92,MISSION BLVD BT WASHINGTON BLVD AND PINES ST SB.doc\n",
      "93,PINE ST BT PASEO PADRE PKWY AND SABERCAT RD EB.doc\n",
      "94,PINE ST BT MISSION BLVD AND PASEO PADRE PKWY WB.doc\n",
      "95,MISSION BLVD BT PINE ST AND DURHAM RD NB.doc\n",
      "96,MISSION BLVD BT PASEO PADRE PKWY AND CURTNER RD NB.doc\n",
      "97,PINE ST BT PASEO PADRE PKWY AND SABERCAT RD WB.doc\n",
      "98,PINE ST BT MISSION BLVD AND PASEO PADRE PKWY EB.doc\n",
      "99,GRIMMER BLVD BT MISSION BLVD AND PASEO PADRE PKWY WB.doc\n",
      "100,MISSION BLVD BT I-680 AND WASHINGTON BLVD SB.doc\n",
      "101,MISSION BLVD BT DURHAM RD AND PASEO PADRE PKWY NB.doc\n",
      "102,MISSION BLVD BT DRISCOLL RD AND I-680 SB.doc\n",
      "Id,./2015 ADT Data\n",
      "103,Washington Blvd betw. Driscoll and Paseo Padre.xls\n",
      "104,Paseo Padre Pkwy betw. Mission and Curtner.xls\n",
      "105,Warren Ave betw. Curtner and Warm Springs.xls\n",
      "106,Mission Blvd betw. Durham and Curtner.xls\n",
      "107,Mission Blvd betw. St. Josephs and Pine.xls\n",
      "108,Paseo Padre Pkwy betw. Quema and Durham.xls\n",
      "109,Grimmer Blvd (South) betw. Osgood and Fremont.xls\n",
      "110,Paseo Padre Pkwy betw. Onondaga and Mission.xls\n",
      "111,Mission Blvd betw. Mission Rd and St. Josephs.xls\n",
      "112,Paseo Padre Pkwy betw. Durham and Onondaga.xls\n",
      "113,Washington Blvd betw. Paseo Padre and Mission.xls\n",
      "114,Mission Blvd betw. Pine and Durham.xls\n",
      "115,Grimmer Blvd (South) betw. Paseo Padre and Osgood.xls\n",
      "116,Paseo Padre Pkwy betw. Driscoll and Quema.xls\n",
      "117,Durham Rd betw. I-680 and Mission.xls\n",
      "118,414011\n",
      "119,400566\n",
      "120,418388\n",
      "121,403255\n",
      "122,413983\n",
      "123,418419\n",
      "124,413988\n",
      "125,402797\n",
      "126,413985\n",
      "127,413982\n",
      "128,403710\n",
      "129,403254\n",
      "130,414016\n",
      "131,402796\n",
      "132,413984\n",
      "133,418420\n",
      "134,418387\n",
      "135,402793\n",
      "136,403257\n",
      "137,413981\n",
      "138,403226\n",
      "139,414015\n",
      "140,402798\n",
      "141,403251\n",
      "142,418423\n",
      "143,413987\n",
      "144,402795\n",
      "145,413980\n",
      "146,403256\n",
      "147,400376\n",
      "148,403719\n",
      "149,402799\n",
      "150,413986\n",
      "151,418422\n",
      "152,403250\n",
      "153,402794\n"
     ]
    }
   ],
   "source": [
    "#scripts to create an updated version of Flow_processed_tmp.csv\n",
    "debug = False\n",
    "fout=open(PeMS_dir + \"/\" + \"new_Flow_processed_tmp.csv\", \"w\")\n",
    "id_counter = 1\n",
    "for filename in os.listdir(Processed_dir):\n",
    "    file_name, file_ext = os.path.splitext(filename)\n",
    "    if file_ext == '.csv' and file_name.split(\"_\")[0][0:2] == '20':\n",
    "        curr_year = filename.split(\"_\")[0]\n",
    "        curr_opening = \"./\" + curr_year + \" ADT Data\"\n",
    "        f=open(Processed_dir + \"/\" + filename)\n",
    "        for line in f:\n",
    "            first_ele = line.split(\",\")[0]\n",
    "            if first_ele == 'Name': #This is header\n",
    "                fout.write('\\n')\n",
    "                header = \"Id,\" + curr_opening\n",
    "                if debug == True:\n",
    "                    print(header)\n",
    "                fout.write(header + '\\n')\n",
    "                continue\n",
    "            line = str(id_counter) + \",\" + first_ele\n",
    "            id_counter += 1\n",
    "            if debug == True:\n",
    "                print(line)\n",
    "            fout.write(line + '\\n') \n",
    "            \n",
    "            \n",
    "fout.write('\\n')   \n",
    "fout.write('PeMS')   \n",
    "for P_file in os.listdir(PeMS_dir + \"/PeMS_2013/\"):\n",
    "    detector_id, _ = P_file.split(\"_\")\n",
    "    line = str(id_counter) + \",\" + detector_id\n",
    "    id_counter += 1\n",
    "    if debug == True:\n",
    "        print(line)\n",
    "    fout.write(line +'\\n') \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Theo: I am here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS <br>\n",
    "Done in the software manually in ArcGIS.\n",
    "1. Export Aimsun network as GIS file\n",
    "2. Import Aimsun network in ArcGIS\n",
    "3. Import detectors in ArcGIS as XY_points\n",
    "4. Move detectors to put them on corresponding road in Aimsun\n",
    "5. Associate to every detectors the External ID of the Aimsun road (to be done again)\n",
    "\n",
    "**TO DO THEO**: Add the process to create the detectors inside Aimsun.\n",
    "Add the process to match the detectors to road section (and create the file lines_to_detectors.xlsx\n",
    "\n",
    "\n",
    "### Later to do: do the spatial join in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'process_flow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eb2290f64a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprocess_flow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'process_flow'"
     ]
    }
   ],
   "source": [
    "import process_flow as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- Create the file Flow_processed_tmp.csv from year_info.csv to check if everything is fine [Done]\n",
    "- Rename the file Flow_processed_tmp.csv -> detectors_id.csv and put it in the manually made dataset [Partially done]\n",
    "- Merge flow_processed_city.csv and flow_processed_PeMS.csv together in the function\n",
    "- Go over Zixuan code to make the code in process_flow better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Process the csv files (city + caltrans) to one big file. <br>\n",
    "source code: processing flow to one CSV.ipynb <br>\n",
    "\n",
    "We combine all the flow traffic data into one big CSV file from both city and PeMS data files. This is done by calling the function process_data() from the process_flow.py python file. \n",
    "\n",
    "Description of function process_data()\n",
    "<br>\n",
    "The function has input:\n",
    "- \"Flow_processed_tmp.csv\" file that lists all the processed files from city and PeMS data\n",
    "- The processed files created from the Parsing Data section\n",
    "\n",
    "The function has output:\n",
    "- \"Flow_processed_city.csv\" containing combined city flow data for all year\n",
    "- \"Flow_processed_PeMS.csv\" containing combined PeMS flow data for all years\n",
    "\n",
    "For the function to work:\n",
    "- The processed files (input) must be located in City and PeMs folders\n",
    "- 2013 city processed files are located in \"City/2013 reformat/\"\n",
    "- 2017 and 2019 city processed files that originated from DOC (which originated from PDF) files are located in \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019\n",
    "- 2017 and 2019 city processed files that originated from Excel files are located in \"City/Year reformat/Format from xlsx\" folder where Year=2017 or 2019\n",
    "- 2013, 2017 and 2019 PeMS data files are located in \"PeMS_Year\" folder where Year=2013, 2017 or 2019\n",
    "\n",
    "For the pipeline to work:\n",
    "- The ouput files must remain in the working directory, no moving necessary.\n",
    "\n",
    "Structure of ouput files: \n",
    "- Flow_processed_city.csv\n",
    "    - contains city traffic data where the rows represent traffic flow. The first 5 columns give info about the traffic flow and are Year, Name, Id, Direction, Day 1 where Name refers to the file name from which the data originated, Id is the Id from the \"Flow_processed_tmp.csv\" file, Direction is the direction of flow and Day 1 is the start date of recording. The columns that follow are day-timesteps for flow data. There are 3 days total over which traffic flow is recorded and time progresses in 15 minute steps. Hence the data columns progress as \"Day 1 - 0:0\", \"Day 1 - 0:15\", \"Day 1 - 0:30\",...,\"Day 3 - 23:30\", \"Day 3 - 23:45\".\n",
    "- Flow_processed_PeMS.csv\n",
    "    - contains PeMS flow traffic data where the rows represent traffic flow. The first columns are Name, Id and Name PeMS where Name contains the PeMS detector Id, Id is the Id assigned from \"Flow_processed_tmp.csv\", Name PeMs is the road address. The next 6 columns give Observed Year and Day Year for the 3 years, 2013, 2017 and 2019. Observed Year is the percentage of the observed data and Day Year is the start date of recording. The columns that follow are Year-Day-timestep, there are 3 years, 3 days and time progresses in 15 minute steps. Hence the columns progress as \"2013-Day 1 - 0:0\", \"2013-Day 1 - 0:15\", \"2013-Day 1 - 0:30\",...,\"2019-Day 3 - 23:30\", \"2019-Day 3 - 23:45\".\n",
    "\n",
    "**(DONE) TO DO 8**: Explain the structure of the output files. Also, feel free to document the doc in the python file (or iPython file). Explain also the input (Flow_processed_tmp.csv) and how it was created (I think it was created from the google doc https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0).\n",
    "\n",
    "***Edson Question***: the Flow_processed_tmp.csv file and the google doc seem the same to me (except for the lat, lng info on the right side of the google doc). Beyond this, I don't know how the file was created. Who to ask for more info?\n",
    "\n",
    "***Theo Answer***: I guess I have created the file from the google doc. But I have also created the google doc during the parsing. Probably in 2) we can create Flow_processed_tmp.csv from year_info.csv\n",
    "\n",
    "# added 2015 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_flow as pf\n",
    "pf.process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Create file that gives traffic flows for specific road sections for every year. <br>\n",
    "source note: put years together.ipynb\n",
    "\n",
    "- Use detectors (lines_to_detectors.csv) and flow processed city (flow_processed_city.csv) data to create all years flow data in \"flow_processed_section.csv\"\n",
    "- Note that the erroneous files are still being skipped, they are: ['DurhamRd I680 MissionBlv EB', 'Mission blvd Pine Washington SB']\n",
    "\n",
    "# To do 9: the function pytogether should be written again. + add 2015\n",
    "- add the PeMS data\n",
    "- be more clever about missing data or road section associated with several detectors (take the average)\n",
    "\n",
    "### To do later, do the spatial join in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_years_together as pytogether\n",
    "\n",
    "line_to_detectors = 'lines_to_detectors.csv'\n",
    "flow_processed_city = 'Flow_processed_city.csv'\n",
    "pytogether.run(line_to_detectors, flow_processed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Analysis\n",
    "\n",
    "# TO DO: To be done after the other to dos has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Analyse PCA results using heatmap inside ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fremont_proj",
   "language": "python",
   "name": "fremont_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
