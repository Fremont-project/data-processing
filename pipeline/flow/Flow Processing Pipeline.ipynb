{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Processing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the pipeline\n",
    "\n",
    "<font color=red> The goal of this notebook is to associate the raw ADT ground-data for 2013, 2015, 2017 and 2019, and the raw speed ground-data in 2015 to the Aimsun network for the calibration of Aimsun simulations. First the raw data is processed in a common format, then, every detector is associated with a network link inside Aimsun. Later, every detector is associated with a road section to create heatmap and understand the evolution of flows over years. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of the pipeline: \n",
    "- One file matching detectors location to Aimsun road section\n",
    "- One file with the processed flow data for 2013, 2015, 2017, 2019\n",
    "- One file with the processed speed data for 2015\n",
    "- One file with the flow data corresponding to road sections for 2013, 2015, 2017 and 2019\n",
    "- PCA on flow data\n",
    "\n",
    "### Inputs of the pipeline: \n",
    "**Raw data**\n",
    "- PeMS account [_publicly available_]. Can be created in http://pems.dot.ca.gov\n",
    "- PeMS detectors location [_publicly available_]. In the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/PeMS`\n",
    "- City average annual daily traffic (AADT) data for 2013, 2017 and 2019 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/City`\n",
    "- Kimley-Horn flow and speed data for 2015 [_given by the city of Fremont_]. Located in the dropbox under `Demand/Flow_speed/Traffic\\ flow\\ studies/Kimley\\ Horn\\ Data`\n",
    "\n",
    "**Manually made dataset**\n",
    "- Aimsun network\n",
    "- Detectors location\n",
    "- Road section layer\n",
    "- Doc files or city ADT data corresponding to the PDF files\n",
    "- Detectors ID to corresponding flow file name\n",
    "\n",
    "\n",
    "### Temporary files of the pipeline\n",
    "- CSV flow data\n",
    "    - City and Kimley Horn\n",
    "    - PeMS data\n",
    "- geographic information of road detectors for 2013, 2015, 2017, 2019\n",
    "- Processed speed data for Kimley Horn\n",
    "- Flow_processed_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent scripts: \n",
    "- [pems_download.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pems_download.py): the script automatically download PeMs data for chosen date.\n",
    "- [pre_process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/pre_process_flow.py): the script (A). parse the ADT data from xlsx, doc and csv files to csv files (B). Find the coordinates of the city detectors (C). Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS \n",
    "- [process_flow.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/process_flow.py): this script processes all the flow traffic data into one big CSV file from both city and PeMS data files\n",
    "- [process_years_together.py](https://github.com/Fremont-project/data-processing/blob/master/pipeline/flow/process_years_together.py): this script combines the previous scripts and processes the data for 2013, 2015, 2017, 2019\n",
    "- fremontdropbox_flow\n",
    "\n",
    "### Dependent libraries:\n",
    "- os\n",
    "- sys\n",
    "- webbrowser\n",
    "- time\n",
    "- requests\n",
    "- pathlib.Path\n",
    "- textract\n",
    "- [numpy](https://numpy.org)\n",
    "- datetime\n",
    "- pandas\n",
    "- math\n",
    "- requests\n",
    "- glob\n",
    "- csv\n",
    "- re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work done by the script\n",
    "1. Obtaining Data\n",
    "    - Turn city pdf to doc\n",
    "    - PeMS data download\n",
    "2. Preprocessing city and Kimley-Horn data\n",
    "    - Parsing the flow data [We are here]\n",
    "    - Parsing the speed data\n",
    "    - Geocoding location of the detectors\n",
    "    - Manually update the location of the detectors\n",
    "3. Processing the data\n",
    "    - Creating one file with the flow for all detectors\n",
    "    - Creating a layer of road section\n",
    "    - Matching the detectors to road section\n",
    "    - Creating one file with the flow over time for every road section\n",
    "    - Matching the detectors with the corresponding road section in Aimsun\n",
    "4. Exporting the data\n",
    "    - Exporting a csv file where Aimsun road sections are matched with detectors\n",
    "    - Exporting a csv file with flow for the different road sections\n",
    "    - Exporting traffic flow heatmap\n",
    "    - Exporting speed heatmap\n",
    "    - PCA on the traffic flow heatmap over years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Jiayi, put the to dos in green when done </font>, <font color=blue> in blue if you need some help </font>, <font color=red>put them red if you have not done them yet</font>\n",
    "- <font color=green> should have exception handler in the process (and python function) if the files are not located where you are looking for.</font>\n",
    "- run PCA and create heatmap with ArcGIS (heatmap will be done by Theo)\n",
    "- <font color=green> put the correct github link for the scripts in the Dependent scripts paragraph </font>\n",
    "- <font color=green> Work done by the script </font>\n",
    "- <font color=green> Dependent libraries </font>\n",
    "- <font color=red> Put the scripts together in one main script </font>\n",
    "- <font color=red> add link from work done by the script to section of the iPython notebook </font>\n",
    "- write tests for detectors shapefiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# We let this notebook to know where to look for fremontdropbox module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from fremontdropbox import get_dropbox_location\n",
    "\n",
    "dropbox_dir = get_dropbox_location()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining Data\n",
    "\n",
    "### A) PeMS Data Download \n",
    "    \n",
    "Script: pems_download.py <br>\n",
    "Download traffic data from the PEMS website (pews.dot.ca.gov) for years 2013, 2015, 2017 and 2019 <br>\n",
    "\n",
    "We download the data calling the method download(detector_ids, year, PeMS_dir) from the pems_download.py file. The process takes about 20 minutes to run. \n",
    "Run `print(help(pems.download))` to get some help.\n",
    "\n",
    "### B) Turn city pdf to doc\n",
    "\n",
    "Some of the flow data that we got from the city were in pdf files. To be able to parse them, we convert them to doc files using online website. This should be done before running the code. The doc files are inputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function download in module pems_download:\n",
      "\n",
      "download(year, detector_ids, PeMS_dir)\n",
      "    This function downloads traffic data from the PeMS website (pems.dot.ca.gov).\n",
      "    This function has for input:\n",
      "        - PeMS detectors ID: detector_ids (an array of detectors)\n",
      "        - Year for the desired data: year (one year as a integer, should be 2013, 2015, 2017 or 2019)\n",
      "    \n",
      "    This function has for output:\n",
      "        - All corresponding PeMS detectors data file for the given year (and the given days encoded in the url).\n",
      "        - Stored in the download folder as PeMS_dir/PeMS_year/PeMS-ID_YEAR.xlsx (where PeMS-ID is the detector ID given by PeMS).\n",
      "        One xlsx file has two sheets:\n",
      "            - PeMS Report Description\n",
      "            - Report Data\n",
      "                - Contains the traffic flow data\n",
      "                - Each row gives the number of vehicles observed in one time step (5 minutes) per lane number over the columns.\n",
      "                - The first column gives the date and time stamp, and the columns that follow are lanes (i.e. Lane 1 Flow, Lane 2 Flow). We care about the column 'Flow (Veh/5 Minutes)' which is the total flow for every lane every 5 minutes.\n",
      "                - The column \"% Observed\" correspond to how much of the flow is due to real vehicles sensed or due to estimation from other days due to a technical issue that make the sensor not sensing every cars.\n",
      "        - Flow data are download for three days\n",
      "            - From the first Tuesday of March at 00:00am until the first thursday of March at 23:59pm\n",
      "    For the function to work, you need to:\n",
      "        - Log in to PeMS in the same browser that runs this Jupyter notebook\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pems_download as pems\n",
    "print(help(pems.download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* IMPORTANT *************\n",
    "# --> For this cell to work, you need to log in to PeMS in the same browser that runs this Jupyter notebook\n",
    "\n",
    "## The IDs of the PeMS detectors where obtained using ArcGIS software and an input file, pems_detectors.csv, containing the locations of all the PeMS dectectors in California\n",
    "## this should be done in Python!\n",
    "detector_ids = [403250, 403256, 403255, 403257, 418387, 418388, 400376,\n",
    "               413981, 413980, 413982, 402794, 413983, 413984, 413985,\n",
    "               413987, 413986, 402796, 413988, 402799, 403251, 403710,\n",
    "               403254, 403719, 400566, 418420, 418419, 418422, 418423,\n",
    "               402793, 403226, 414015, 414016, 402795, 402797, 414011,\n",
    "               402798]\n",
    "PeMS_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMS'\n",
    "\n",
    "# pems.download(2013, detector_ids, PeMS_dir)\n",
    "# pems.download(2015, detector_ids, PeMS_dir)\n",
    "# pems.download(2017, detector_ids, PeMS_dir)\n",
    "# pems.download(2019, detector_ids, PeMS_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO Later:\n",
    "- Find the list of detectors ID using the project delimitation shapefile. (to be done later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeMS during 03/05/2013 00:00:00-03/07/2013 23:59:59\n",
      "PeMS during 03/03/2015 00:00:00-03/05/2015 23:59:59\n",
      "PeMS during 03/07/2017 00:00:00-03/09/2017 23:59:59\n",
      "PeMS during 03/05/2019 00:00:00-03/07/2019 23:59:59\n"
     ]
    }
   ],
   "source": [
    "pems.PeMS_tester(2013, PeMS_dir, True)\n",
    "pems.PeMS_tester(2015, PeMS_dir, True)\n",
    "pems.PeMS_tester(2017, PeMS_dir, True)\n",
    "pems.PeMS_tester(2019, PeMS_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing city data\n",
    "\n",
    "\n",
    "### A) Parse city and Kimley Horn flow data from xlsx, doc and csv files to csv files\n",
    "\n",
    "Here, we process the Excel and PDF ADT data files (city data) into CSV files.\n",
    "\n",
    "- run `print(help(pre_process.parse_excel_2013)` to get some help for 2013 excel files\n",
    "- run `print(help(pre_process.parse_excel_2015)` to get some help for 2015 excel files\n",
    "- run `print(help(pre_process.parse_excel_2017)` to get some help for 2017 excel files\n",
    "- run `print(help(pre_process.parse_excel_2019)` to get some help for 2019 excel files\n",
    "- run `print(help(pre_process.parse_adt_as_file)` to get some help for the 2017 and 2019 doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function process_adt_data in module pre_process_flow:\n",
      "\n",
      "process_adt_data(year, Processed_dir, Input_dir)\n",
      "    This function processes the Excel and PDF ADT data files (city data) into CSV files. Note that one file corresponds to one main road and the traffic flow data recordings in it.\n",
      "    \n",
      "    This function has input:\n",
      "        - year which takes values 2013, 2015, 2017 or 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    The function has output:\n",
      "        - CSV files located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pre_process_flow as pre_process\n",
    "print(help(pre_process.process_adt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function parse_excel_2013 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2013(dfs)\n",
      "    ***2013 Excel files*** are structured in data sheets. The first data sheet \"Summary\" contains the main road, cross streets, city information and the start date of the recording. It also summarizes the data contained in all other sheets into a bar plot of traffic flow vs time of day bins (i.e Tuesday AM, Wednesday PM) for different flow directions and into a line plot of traffic flow vs. hour of day for different days of the week. The sheets that follow are named \"D1\", \"D2\",...\"DN\" where N denotes the N'th day since the start date. These sheets are structured into two tables, AM counts and PM counts. Each table row gives the traffic flow per timestep of 15 minutes. The first column is the time of day in hh:mm format follow by direction columns of traffic flow (NB, SB, EB, WB).\n",
      "\n",
      "None\n",
      "Help on function parse_excel_2015 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2015(dfs)\n",
      "    ***2015 Excel files***. The files come from Kimley Horn. Every excel file has 7 relevant sheets including the hidden sheets. \n",
      "    They are ['Data', 'ns Day 1', 'ns Day 2', 'ns Day 3', 'ew Day 1', 'ew Day 2', 'ew Day 3']. Here, we are only going to process the 'Data' sheet.\n",
      "\n",
      "None\n",
      "Help on function parse_excel_2017 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2017(dfs)\n",
      "    ***2017 Excel files*** are structured in one data sheet giving a header and a table for traffic flow. The header gives the start date and time of the recording, \n",
      "    site code and sensor location, and the table gives traffic flow per a 15 minute timestep. The table's first two columns give the date and time and the following \n",
      "    columns give traffic flow per directions.\n",
      "\n",
      "None\n",
      "Help on function parse_excel_2019 in module pre_process_flow:\n",
      "\n",
      "parse_excel_2019(dfs)\n",
      "    ***2019 Excel files*** have similar structure as those of 2013. The data is organized in two types of sheet, \"Day N\" and \"GR N\" sheets. The \"Day N\" sheets give traffic flow data in the same fashion as the \"DN\" sheets of 2013 excel files. The day of recording can be found in the header of the two tables. The \"GR N\" sheets plot the corresponding flow data of the \"Day N\" sheets. A line plot of flow vs. hour of day for different flow directions is given.\n",
      "\n",
      "None\n",
      "Help on function parse_adt_as_file in module pre_process_flow:\n",
      "\n",
      "parse_adt_as_file(file_path, year, out_folder)\n",
      "    Process Doc files\n",
      "    \n",
      "    ***2017 and 2019 PDF files*** are structured with a header and 3 tables of traffic flow data (one table per day of subsequent days). \n",
      "    The header gives the site location and other miscellaneous meta data. Each table is titled by the date and timestep (15 minutes) of \n",
      "    the recording. A table is organized by columns each representing the hour of day (0 - 23). Hence for a given column, the first row \n",
      "    gives the hour of the day, the second gives the total flow for the hour, and the third to last row (4 rows total) gives traffic flow per\n",
      "     15 minute timestep for the hour.\n",
      "    \n",
      "    \n",
      "    Years 2017 and 2019 files have the same structure hence we can reuse this code.\n",
      "    Structure refers to data organized in 3 tables split by *\n",
      "    Note no doc files for 2013.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(pre_process.parse_excel_2013))\n",
    "print(help(pre_process.parse_excel_2015))\n",
    "print(help(pre_process.parse_excel_2017))\n",
    "print(help(pre_process.parse_excel_2019))\n",
    "print(help(pre_process.parse_adt_as_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Re do the parsing such that all the files look the same. The time format should be the same everywhere.\n",
    "- Remove the unnecessary functions in pre_process\n",
    "- <font color=green> Write test to check if the parsing of doc file is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed'\n",
    "Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/Flow_processed'\n",
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n",
    "local_testing = '/Users/LiJiayi/Documents'\n",
    "\n",
    "# pre_process.process_adt_data(2013, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2015, Processed_dir, Kimley_Horn_flow_dir)\n",
    "# pre_process.process_adt_data(2017, Processed_dir, City_dir)\n",
    "# pre_process.process_adt_data(2019, Processed_dir, City_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc files are all good for both 2017 and 2019!\n"
     ]
    }
   ],
   "source": [
    "#doc tests\n",
    "\n",
    "pre_process.doc_file_tester(City_dir, [2017, 2019])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary files generater:\n",
    "\n",
    "pre_process.google_doc_generater(Processed_dir)\n",
    "pre_process.flow_processed_generater(Processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### B) Find the coordinates of the city detectors\n",
    "We obtained the coordinates of the detectors by calling the method get_geo_data(year) from the pre_process.py python file. \n",
    "\n",
    "- run `print(help(pre_process.get_geo_data))` to get some help.\n",
    "\n",
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green>Put the different files year_info.csv together, and compare it to the [google doc] (Done)(https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0) </font>\n",
    "- <font color=green> Make sure to process doc files (Done) </font>\n",
    "- <font color=green> Make sure to process PeMS files too (for this ask Theo)</font>\n",
    "- <font color=blue> Then we want to match the detectors to the ID associated to them here.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_geo_data in module pre_process_flow:\n",
      "\n",
      "get_geo_data(year, Input_dir, Processed_dir)\n",
      "    This function iterates over the ADT files and obtains the adresses of the detectors to then use with Google API to obtain latitude and longitude coordinates.\n",
      "    \n",
      "    This function has input:\n",
      "        - Year takes values 2013, 2015, 2017, 2019\n",
      "        - Processed_dir: path to the output\n",
      "        - Input_dir: path to the inputs\n",
      "    \n",
      "    This function has output:\n",
      "        - CSV file \"year_info_coor.csv\" containing the coordinates of detectors and located in the Processed_dir/Year_processed/ folder where Year=2017 or 2019\n",
      "    \n",
      "    For the function to work:\n",
      "        - Files should be located in \n",
      "            1. Input_dir/Year\\ EXT/ folder if Year=2013, 2017 or 2019 where:\n",
      "                a. Year=2013, 2017 or 2019 if Ext=ADT Data \n",
      "                b. Year=2017 or 2019 if Ext=doc for 2017 and 2019\n",
      "            2. Input_dir/Raw\\ data/ folder if Year=2015 \n",
      "    \n",
      "    + add the doc files\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(pre_process.get_geo_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pre_process.get_geo_data(2013, City_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2015, Kimley_Horn_flow_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2017, City_dir, Processed_dir)\n",
    "pre_process.get_geo_data(2019, City_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "City_dir = ADT_dir + \"/City\"\n",
    "Kimley_Horn_flow_dir = ADT_dir + \"/Kimley Horn Data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 - Grimmer Blvd - Paseo Padre Pkwy to Osgood Rd.xls\n",
      "83 - Mission Blvd - Pine St to Durham Rd.xls\n",
      "135 - Warren Avenue - Curtner Road to Warm Springs Boulevard.xls\n",
      "44 - Durham Rd - I-680 to Mission Blvd.xls\n",
      "82 - Mission Blvd - St Joseph Terr to Pine St.xls\n",
      "140 - Washington Boulevard - Paseo Padre Pkwy to Mission Blvd.xls\n",
      "110 - Paseo Padre Pkwy - Mission Blvd to Curtner Rd.xls\n",
      "106 - Paseo Padre Pkwy - Driscoll Rd to Washington Blvd - Resurvey.xls\n",
      "84 - Mission Blvd - Durham Rd to Curtner Rd.xls\n",
      "107 - Paseo Padre Pkwy - Washington Blvd to Durham Rd - Resurvey.xls\n",
      "109 - Paseo Padre Pkwy - Onondaga Wy to Mission Blvd.xls\n",
      "108 - Paseo Padre Pkwy - Durham Rd to Onondaga Wy.xls\n",
      "81 - Mission Blvd - Mission Rd to St Joseph Terrace.xls\n",
      "139 - Washington Boulevard - Driscoll Road to Paseo Padre Pkwy.xls\n",
      "61 - South Grimmer Blvd - Osgood Rd to Fremont Blvd.xls\n",
      "Raw Data\n"
     ]
    }
   ],
   "source": [
    "# Preprocess 2015 speed data\n",
    "pre_process.Speed_data_parser(Kimley_Horn_flow_dir, Processed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Theo: I am here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Check and adjust the locations of the (City + PeMS) detectors to match them to our network using ArcGIS <br>\n",
    "Done in the software manually in ArcGIS.\n",
    "1. Export Aimsun network as GIS file\n",
    "2. Import Aimsun network in ArcGIS\n",
    "3. Import detectors in ArcGIS as XY_points\n",
    "4. Move detectors to put them on corresponding road in Aimsun\n",
    "5. Associate to every detectors the External ID of the Aimsun road (to be done again)\n",
    "\n",
    "**TO DO THEO**: Add the process to create the detectors inside Aimsun.\n",
    "Add the process to match the detectors to road section (and create the file lines_to_detectors.xlsx\n",
    "\n",
    "\n",
    "### Later to do: do the spatial join in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ADT_dir = dropbox_dir + '/Private Structured data collection/Data processing/Raw/Demand/Flow_speed/ADT'\n",
    "PeMs_dir = dropbox_dir + '/Private Structured data collection/Data processing/Auxiliary files/Demand/Flow_speed/PeMs'\n",
    "local_download =  str(os.path.join(Path.home(), \"Downloads\"))\n",
    "re_formated_Processed_dir = dropbox_dir + '/Private Structured data collection/Data processing/Temporary exports to be copied to processed data/Flow_processed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> TO DO: </font>\n",
    "- <font color=green> Create the file Flow_processed_tmp.csv from year_info.csv to check if everything is fine [Done]</font>\n",
    "- <font color=green> Rename the file Flow_processed_tmp.csv -> detectors_id.csv and put it in the manually made dataset [Partially done] </font>\n",
    "- <font color=blue> Merge flow_processed_city.csv and flow_processed_PeMS.csv together in the function </font> \n",
    "- <font color=red> Go over Zixuan code to make the code in process_flow better </font>\n",
    "- <font color=green>Processs speed data for 2015 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Process the csv files (city + caltrans) to one big file. <br>\n",
    "source code: processing flow to one CSV.ipynb <br>\n",
    "\n",
    "We combine all the flow traffic data into one big CSV file from both city and PeMS data files. This is done by calling the function process_data() from the process_flow.py python file. \n",
    "\n",
    "Description of function process_data()\n",
    "<br>\n",
    "The function has input:\n",
    "- \"Flow_processed_tmp.csv\" file that lists all the processed files from city and PeMS data\n",
    "- The processed files created from the Parsing Data section\n",
    "\n",
    "The function has output:\n",
    "- \"Flow_processed_city.csv\" containing combined city flow data for all year\n",
    "- \"Flow_processed_PeMS.csv\" containing combined PeMS flow data for all years\n",
    "\n",
    "For the function to work:\n",
    "- The processed files (input) must be located in City and PeMs folders\n",
    "- 2013 city processed files are located in \"City/2013 reformat/\"\n",
    "- 2017 and 2019 city processed files that originated from DOC (which originated from PDF) files are located in \"City/Year reformat/Format from pdf\" folder where Year=2017 or 2019\n",
    "- 2017 and 2019 city processed files that originated from Excel files are located in \"City/Year reformat/Format from xlsx\" folder where Year=2017 or 2019\n",
    "- 2013, 2017 and 2019 PeMS data files are located in \"PeMS_Year\" folder where Year=2013, 2017 or 2019\n",
    "\n",
    "For the pipeline to work:\n",
    "- The ouput files must remain in the working directory, no moving necessary.\n",
    "\n",
    "Structure of ouput files: \n",
    "- Flow_processed_city.csv\n",
    "    - contains city traffic data where the rows represent traffic flow. The first 5 columns give info about the traffic flow and are Year, Name, Id, Direction, Day 1 where Name refers to the file name from which the data originated, Id is the Id from the \"Flow_processed_tmp.csv\" file, Direction is the direction of flow and Day 1 is the start date of recording. The columns that follow are day-timesteps for flow data. There are 3 days total over which traffic flow is recorded and time progresses in 15 minute steps. Hence the data columns progress as \"Day 1 - 0:0\", \"Day 1 - 0:15\", \"Day 1 - 0:30\",...,\"Day 3 - 23:30\", \"Day 3 - 23:45\".\n",
    "- Flow_processed_PeMS.csv\n",
    "    - contains PeMS flow traffic data where the rows represent traffic flow. The first columns are Name, Id and Name PeMS where Name contains the PeMS detector Id, Id is the Id assigned from \"Flow_processed_tmp.csv\", Name PeMs is the road address. The next 6 columns give Observed Year and Day Year for the 3 years, 2013, 2017 and 2019. Observed Year is the percentage of the observed data and Day Year is the start date of recording. The columns that follow are Year-Day-timestep, there are 3 years, 3 days and time progresses in 15 minute steps. Hence the columns progress as \"2013-Day 1 - 0:0\", \"2013-Day 1 - 0:15\", \"2013-Day 1 - 0:30\",...,\"2019-Day 3 - 23:30\", \"2019-Day 3 - 23:45\".\n",
    "\n",
    "**(DONE) TO DO 8**: Explain the structure of the output files. Also, feel free to document the doc in the python file (or iPython file). Explain also the input (Flow_processed_tmp.csv) and how it was created (I think it was created from the google doc https://docs.google.com/spreadsheets/d/1tcps-8aorPZLY8nswnNCmjWSJi-7ey8Ps4twWFz2ls0/edit#gid=0).\n",
    "\n",
    "***Edson Question***: the Flow_processed_tmp.csv file and the google doc seem the same to me (except for the lat, lng info on the right side of the google doc). Beyond this, I don't know how the file was created. Who to ask for more info?\n",
    "\n",
    "***Theo Answer***: I guess I have created the file from the google doc. But I have also created the google doc during the parsing. Probably in 2) we can create Flow_processed_tmp.csv from year_info.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file not processed:  DURHAM RD BT I-680 AND MISSION BLVD EB\n",
      "file not processed:  MISSION BLVD BT WASHINGTON BLVD AND PINES ST SB\n"
     ]
    }
   ],
   "source": [
    "import process_flow as pf\n",
    "pf.process_data(Processed_dir, re_formated_Processed_dir, PeMs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Create file that gives traffic flows for specific road sections for every year. <br>\n",
    "source note: put years together.ipynb\n",
    "\n",
    "- Use detectors (lines_to_detectors.csv) and flow processed city (flow_processed_city.csv) data to create all years flow data in \"flow_processed_section.csv\"\n",
    "\n",
    "\n",
    "# To do 9: the function pytogether should be written again. + add 2015\n",
    "- add the PeMS data\n",
    "- be more clever about missing data or road section associated with several detectors (take the average)\n",
    "\n",
    "### To do later, do the spatial join in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x95 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7f29ed40123c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mline_to_detectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_formated_Processed_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/lines_to_detectors.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mflow_processed_city\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_formated_Processed_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Flow_processed_city.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpytogether\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_to_detectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_processed_city\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/data-processing/pipeline/flow/process_years_together.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(line_to_detectors, flow_processed_city)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_to_detectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_processed_city\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_to_detectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_processed_city\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msection_no_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Fremont_proj/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Fremont_proj/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Fremont_proj/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Fremont_proj/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Fremont_proj/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x95 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "import process_years_together as pytogether\n",
    "\n",
    "line_to_detectors = re_formated_Processed_dir + '/lines_to_detectors.csv'\n",
    "flow_processed_city = re_formated_Processed_dir + '/Flow_processed_city.csv'\n",
    "pytogether.run(line_to_detectors, flow_processed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Analysis\n",
    "\n",
    "# TO DO: To be done after the other to dos has been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Analyse PCA results using heatmap inside ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fremont_proj",
   "language": "python",
   "name": "fremont_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
